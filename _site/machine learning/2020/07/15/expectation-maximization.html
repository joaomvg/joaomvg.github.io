<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Expectation-Maximization | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Expectation-Maximization" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We explain the theory of the expectation-maximization algorithm." />
<meta property="og:description" content="We explain the theory of the expectation-maximization algorithm." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2020/07/15/expectation-maximization.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2020/07/15/expectation-maximization.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-15T00:00:00+02:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Expectation-Maximization" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2020/07/15/expectation-maximization.html","headline":"Expectation-Maximization","dateModified":"2020-07-15T00:00:00+02:00","datePublished":"2020-07-15T00:00:00+02:00","description":"We explain the theory of the expectation-maximization algorithm.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2020/07/15/expectation-maximization.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Expectation-Maximization</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-07-15T00:00:00+02:00" itemprop="datePublished">
        Jul 15, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3 id="the-algorithm">The algorithm</h3>

<p>Often we have to deal with hidden variables in machine learning problems. The maximum-likelihood algorithm requires “integrating” over these hidden variables if we want to compare with the observed distribution. However this can lead to a serious problem since we have to deal with sums inside the logarithms. That is, we are instructed to maximize the log-likelihood quantity</p>

\[\sum_i\ln p(x_i)=\sum_i\ln\Big( \sum_h p(x_i,h)\Big)\]

<p>where $h$ is the hidden variable and $x_i$ is the observed one. Except for simple problems, having two sums turns the problem computationally infeasible, especially if the hidden variable is continuous. To deal with this issue we use the concavity property of the logarithm to approximate</p>

\[\ln\Big( \sum_h p(x_i,h)\Big)=\ln\Big( \sum_h q(h)\frac{p(x_i,h)}{q(h)}\Big)\geq \sum_hq(h)\ln\Big(\frac{p(x_i,h)}{q(h)}\Big)\]

<p>where $q(h)$, that we will want to fix, obeys $\sum_h q(h)=1$. Further we write</p>

\[\ln \sum_h p(x_i,h)=\sum_hq(h)\ln\Big(\frac{p(x_i,h)}{q(h)}\Big)+R_i\]

<p>where the remaining $R_i$ is given by</p>

\[R_i=-\sum_h q(h)\ln\Big(\frac{p(h|x_i)}{q(h)}\Big)=KL(p(h|x_i)||q(h))\]

<p>which is the Kullback-Leibler divergence. Since $R_i\geq 0$ by definition, we have that</p>

\[\ln p(x_i|\theta)\geq \langle \ln p(x_i,h|\theta)\rangle_{q(h)}-\langle \ln q(h)\rangle_{q(h)}\]

<p>where we have introduced prior parameters $\theta$, without lack of generality. The lower bound, the KL divergence, is saturated provided we choose</p>

\[\text{E-step:}\quad q(h_i)=p(h_i|x_i,\theta_0)\]

<p>which is known as the expectation E-step. Note that we have a distribution $q(h_i)$ for each sample, and as a function of $x_i,\theta_0$. However, this step does not solve the maximum-likelihood problem because we still have to find the parameter $\theta$. What we do next is to maximize the lower bound by choosing $\theta$ keeping $q(h)$ fixed, that is,</p>

\[\text{M-step:}\quad \frac{\partial}{\partial \theta}\langle \ln p(x_i,h|\theta)\rangle_{q(h|\theta_0)}=0\]

<p>One can show that EM algorithm increases the log-likelihood. Consider the sequence $\theta_0,\ldots,\theta_j,\theta_{j+1},\ldots$. The log-likelihood at step $\theta_{j+1}$ is</p>

\[\ln p(x|\theta_{j+1})=\langle \ln p(x_i,h|\theta_{j+1})\rangle_{p(h|x,\theta_j)}-\langle \ln p(h|x,\theta_j)\rangle_{p(h|x,\theta_j)}+KL(p(h|x,\theta_{j+1})||p(h|x,\theta_j))\]

<p>Thus for $\theta_{j+1}\rightarrow \theta_j$ we have:</p>

\[\ln p(x|\theta_j)=\langle \ln p(x_i,h|\theta_j)\rangle_{p(h|x,\theta_j)}-\langle \ln p(h|x,\theta_j)\rangle_{p(h|x,\theta_j)}+KL(p(h|x,\theta_j)||p(h|x,\theta_j))\]

<p>since the last term (KL divergence) vanishes, the difference in log-likelihood is:</p>

\[\begin{equation*}\begin{split}&amp;\ln p(x|\theta_{j+1})-\ln p(x|\theta_j)=\\
&amp;=\langle \ln p(x_i,h|\theta_{j+1})\rangle_{p(h|x,\theta_j)}-\langle \ln p(x_i,h|\theta_j)\rangle_{p(h|x,\theta_j)}+KL(p(h|x,\theta_{j+1})||p(h|x,\theta_j))\end{split}\end{equation*}\]

<p>The KL term is always positive, while $\langle \ln p(x_i,h|\theta_{j+1})\rangle_{p(h|x,\theta_j)}-\langle \ln p(x_i,h|\theta_j)\rangle_{p(h|x,\theta_j)}$ is the quantity we maximize in the M-step. Therefore the term $\ln p(x|\theta)$ is increasing during the EM algorithm.</p>
<h3 id="gaussian-mixture-model">Gaussian Mixture Model</h3>

<p>Lets take an example that can help clarify some of these ideas. Consider the model which is a mixture of two normal distributions:</p>

\[p(x,c)=\phi(x|\mu_c,\sigma_c)\pi_c,\quad c=0,1\]

<p>where 
$\phi(x|\mu,\sigma)$
 is a normal distribution with mean $\mu$ and variance $\sigma$, and $\pi_c=p(c)$ with $\pi_0+\pi_1=1$. In this example $\theta\equiv \mu,\sigma$, and the hidden variable is $h\equiv c$.</p>

<p>In the E-step we calculate:</p>

\[\text{E-step:}\quad q(h)=p(h|x,\mu_h,\sigma_h)=\frac{\phi(x|\mu_h,\sigma_h)\pi_h}{\sum_c \phi(x|\mu_c,\sigma_c)\pi_c}\]

<p>We write $q(h_i=0)=\gamma_i(x_i)$ and $q(h_i=1)=1-\gamma_i(x_i)$ for each sample $x_i$, with $\gamma$ given by the ratio above. The initial parameters $\mu,\sigma$ are arbitrary.</p>

<p>The maximization step consists in maximizing the lower bound of the log-likelihood, hence</p>

\[\begin{equation*}\begin{split}\text{M-step:}\quad &amp;\gamma\ln p(x,h=0|\mu,\sigma)+(1-\gamma)\ln p(x,h=1|\mu,\sigma)\\
=&amp;\gamma \ln \phi(x|\mu_0,\sigma_0)+(1-\gamma)\ln \phi(x|\mu_1,\sigma_1)-\gamma\frac{1}{2}\ln\sigma_0-(1-\gamma)\frac{1}{2}\ln\sigma_1+\ldots\\
=&amp; -\gamma \frac{(x-\mu_0)^2}{2\sigma_0^2}-(1-\gamma) \frac{(x-\mu_1)^2}{2\sigma_1^2}-\gamma\frac{1}{2}\ln\sigma_0-(1-\gamma)\frac{1}{2}\ln\sigma_1+\ldots\end{split}\end{equation*}\]

<p>where $\ldots$ do not depend on $\mu,\sigma$. We need to sum over all samples, so the maximum is calculated</p>

\[\mu_0=\frac{\sum_i x_i\gamma_i}{\sum_i \gamma_i},\;\mu_1=\frac{\sum_i x_i(1-\gamma_i)}{\sum_i (1-\gamma_i)}\]

<p>and</p>

\[\sigma_0=\frac{\sum_i\gamma_i(x_i-\mu_0)^2}{\sum_i\gamma_i},\quad \sigma_1=\frac{\sum_i(1-\gamma_i)(x_i-\mu_1)^2}{\sum_i(1-\gamma_i)}\]

<p>Maximizing relatively to the probabilities $\pi$ gives</p>

\[\pi_0=\frac{1}{n}\sum_i\gamma_i,\;\pi_1=1-\pi_0\]

  </div><a class="u-url" href="/machine%20learning/2020/07/15/expectation-maximization.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




