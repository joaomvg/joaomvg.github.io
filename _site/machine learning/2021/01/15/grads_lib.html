<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Gradients Automation | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Gradients Automation" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We describe how to automate the calculation of gradients using tensors in python. This is very useful when building neural networks. We build a class that calculates the gradients without having to code them explicitly. This library is a toy-model that mimics professional libraries like the Pytorch or TensorFlow." />
<meta property="og:description" content="We describe how to automate the calculation of gradients using tensors in python. This is very useful when building neural networks. We build a class that calculates the gradients without having to code them explicitly. This library is a toy-model that mimics professional libraries like the Pytorch or TensorFlow." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2021/01/15/grads_lib.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2021/01/15/grads_lib.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/tensor_python.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-15T00:00:00+01:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/tensor_python.png" />
<meta property="twitter:title" content="Gradients Automation" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2021/01/15/grads_lib.html","image":"http://localhost:4000/tensor_python.png","headline":"Gradients Automation","dateModified":"2021-01-15T00:00:00+01:00","datePublished":"2021-01-15T00:00:00+01:00","description":"We describe how to automate the calculation of gradients using tensors in python. This is very useful when building neural networks. We build a class that calculates the gradients without having to code them explicitly. This library is a toy-model that mimics professional libraries like the Pytorch or TensorFlow.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2021/01/15/grads_lib.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Gradients Automation</h1>
    <p class="post-meta"><time class="dt-published" datetime="2021-01-15T00:00:00+01:00" itemprop="datePublished">
        Jan 15, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul>
  <li><a href="#1-gradients-and-tensors"><strong>1. Gradients and tensors</strong></a></li>
  <li><a href="#2-python-implementation"><strong>2. Python Implementation</strong></a></li>
</ul>

<p><a name="def1"></a></p>
<h3 id="1-gradients-and-tensors"><strong>1. Gradients and tensors</strong></h3>

<p>Neural networks can get very complicated very quickly. There are several types of architectures, and each can have multiple layers. Coding the gradients becomes a complicated task. Instead, libraries such as Pytorch or TensorFlow use a smart mechanism that allows to calculate gradients without having to code them. Essentially, these libraries keep track of all the mathematical operations on a tensor object and use the chain rule to determine the derivatives.</p>

<p>So how to calculate derivatives of tensors? Consider the scalar
\(\Phi=\exp\Big(\sum_{ij} T_{ij}w_iw_j\Big)\)
and its derivative relative to the tensor $w_i$. First, we calculate the derivative of $\Phi(u)$ with respect to $u$</p>

<p>\(\frac{\partial\Phi}{\partial u}=\exp(u)\)
and then use the chain rule together with the derivatives</p>

\[\frac{\partial u}{\partial w_k}=\sum_{ij}T_{ij}\frac{\partial w_i}{\partial w_k}w_j+\sum_{ij}T_{ij} w_i\frac{\partial w_j}{\partial w_k}+\sum_{ij}\frac{\partial T_{ij}}{\partial w_k}w_iw_j\]

<p>But we can do this calculation differently. Instead of starting from the function $\Phi(u)$ and propagate back the derivatives, we can keep track of the results at each step of the forward operation. That is, we perform the calculation in the following order</p>

<ol>
  <li>Calculate the derivatives $\frac{\partial w_i}{\partial w_k}$ and $\frac{\partial T_{ij}}{\partial w_k}$</li>
  <li>Determine $X_i\equiv\sum_{ij}T_{ij}w_j$ and $\frac{\partial X_i}{\partial w_k}=\sum_{ij}\frac{\partial T_{ij}}{\partial w_k}w_i + \sum_{ij}T_{ij}\frac{\partial w_i}{\partial w_k}$</li>
  <li>Determine $Y\equiv\sum_i X_iw_i$ and $\frac{\partial Y}{\partial w_k}=\sum_{i}\frac{\partial X_{i}}{\partial w_k}w_i + \sum_{i}X_{i}\frac{\partial w_i}{\partial w_k}$</li>
  <li>Finally calculate $\Phi=\exp(Y)$ and $\frac{\partial \Phi}{\partial w_k}=\Phi \frac{\partial Y}{\partial w_k}$</li>
</ol>

<p>At each step, we calculate the resulting tensor and the corresponding derivative. To accomplish this, we need an object (class) that implements various mathematical operations and keeps track of the gradients while the function takes place. In pseudo-code</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">array</span><span class="p">,</span><span class="n">gradient</span><span class="p">)</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="n">add_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="n">gradient</span><span class="o">=</span><span class="n">grad_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">gradient</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">):</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="n">mul_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="n">gradient</span><span class="o">=</span><span class="n">grad_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">gradient</span><span class="p">)</span>

</code></pre></div></div>
<p>We should also define non-linear functions, such as the sigmoid activation function</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
</code></pre></div></div>

<p>As an example, say we want to calculate the scalar
\(\phi=\sigma\Big(\sum_ix_i\omega_i\Big)\)
where $\sigma(z)$ is the sigmoid function, and we are interested in the derivative relative to $\omega_i$. First we create the Tensor objects,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w_tensor</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">w_array</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x_tensor</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x_array</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>
<p>where w_tensor and x_tensor have dimensions $(d,1)$ and $(1,d)$ respectively. The flag “requires_grad” specifies whether we want or not the derivative relative to that tensor. We instantiate the function Sigmoid</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sig</span><span class="o">=</span><span class="n">Sigmoid</span><span class="p">()</span>
</code></pre></div></div>
<p>and calculate</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">phi</span><span class="o">=</span><span class="n">sig</span><span class="p">(</span><span class="n">x_tensor</span><span class="o">*</span><span class="n">w_tensor</span><span class="p">)</span>
</code></pre></div></div>
<p>The operator * is overloaded in the class Tensor by the matrix multiplication operation.
The object “phi” is an instance of the class Tensor, which contains both the result of the mathematical operation and the gradient of “phi” with respect to $\omega_i$. Then we can access the gradient by the attribute “phi.grad”.</p>

<p><a name="python"></a></p>
<h3 id="2-python-implementation"><strong>2. Python Implementation</strong></h3>

<p>The Tensor class contains linear mathematical operations together with other methods such as transposing or squeezing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="n">calc_grad</span><span class="o">=</span><span class="bp">True</span> 

    <span class="s">"""calc_grad: bool, signaling whether to carry the gradients while artihmetic operations take place"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">array</span><span class="p">,</span>
                <span class="n">grad</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="s">"""
        array: numpy array
        grad: dic={id(object): numpy.array}
        requires_grad: bool, signaling whether to calculate or not the derivative relative to this tensor
        
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">=</span><span class="n">array</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span>
        
        <span class="k">if</span> <span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">name</span><span class="o">=</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> 
            <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_grad</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="s">'none'</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span>
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">ndim</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">transpose</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">index</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">][</span><span class="n">index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
        
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">Kron</span><span class="o">=</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
            <span class="n">ID</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="n">Kron</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">Kron</span><span class="p">,</span><span class="n">ID</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">new_shape</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">new_shape</span><span class="o">+=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">Kron</span><span class="o">=</span><span class="n">Kron</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Kron</span>

    <span class="k">def</span> <span class="nf">check_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">+</span><span class="n">x</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">+</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">+</span><span class="n">x</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    
    <span class="k">def</span> <span class="nf">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">+</span><span class="n">x</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>

                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">-</span><span class="n">x</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">x</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">grad1</span><span class="o">=</span><span class="mi">0</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">grad1</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        
                    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">grad2</span><span class="o">=</span><span class="mi">0</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">i</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
                        <span class="n">grad2</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">n1</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                        <span class="n">n2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">ndim</span>
                        <span class="n">n3</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">ndim</span>
                        <span class="n">r1</span><span class="o">=</span><span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n2</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n1</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n1</span><span class="o">+</span><span class="n">n3</span><span class="o">-</span><span class="mi">2</span><span class="p">)]</span>
                        <span class="n">r2</span><span class="o">=</span><span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n2</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n1</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
                        <span class="n">grad2</span><span class="o">=</span><span class="n">grad2</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">r1</span><span class="o">+</span><span class="n">r2</span><span class="p">)</span>
                    
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad1</span><span class="o">+</span><span class="n">grad2</span>

                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">axis</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s">'Tensor(</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="si">}</span><span class="s">,dtype </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">,requires_grad=</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">requires_grad</span><span class="si">}</span><span class="s">)'</span>
</code></pre></div></div>
<p>Non-linear functions (some examples) can be defined as:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
    <span class="s">"""
    returns: Tensor with gradients
    """</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="n">u</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">u</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                    <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                    <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad_func</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="n">den</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">u</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">u</span><span class="p">)</span>
        <span class="n">gd</span><span class="o">=</span><span class="n">u</span><span class="o">/</span><span class="n">den</span>

        <span class="k">return</span> <span class="n">gd</span>

<span class="k">class</span> <span class="nc">Log</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>

        <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                <span class="n">grad_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad_func</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">gd</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">x</span><span class="p">.</span><span class="n">array</span>

        <span class="k">return</span> <span class="n">gd</span>

<span class="k">class</span> <span class="nc">ReLU</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">sign</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">z</span><span class="p">[</span><span class="n">sign</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

        <span class="k">if</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                    <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                    <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">sign</span><span class="p">)</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad_func</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">sign</span><span class="p">):</span>
        <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">z</span><span class="p">[</span><span class="n">sign</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">z</span><span class="p">[</span><span class="o">~</span><span class="n">sign</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>

        <span class="k">return</span> <span class="n">z</span>

<span class="k">class</span> <span class="nc">Softmax</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""calculate grads after softmaz operation

        Args:
            x (Tensor): shape=(batch,num_classes)

        Returns:
            Tensor: contains gradients relative to softmax function
        """</span>
    
        <span class="n">prob</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">Z</span><span class="o">=</span><span class="n">prob</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">prob</span><span class="o">=</span><span class="n">prob</span><span class="o">/</span><span class="n">Z</span>

        <span class="k">if</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                    <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                    <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                    <span class="n">dp</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">dp</span><span class="o">-</span><span class="n">grad_func</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">dp</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
</code></pre></div></div>

<p>With these definitions it is now easy to build a simple feed forward neural network, without the trouble of coding the gradients explicitly.</p>

<p>Here we show an example. First we define a LinearLayer class:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">in_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_dim</span><span class="o">=</span><span class="n">in_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span>

        <span class="n">weight_</span><span class="p">,</span><span class="n">bias_</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">init_params</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">bias_</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">trainable</span><span class="o">=</span><span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                        <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">}</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        x: Tensor [batch,in_dim]
        """</span>
        <span class="n">out</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weight</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="bp">self</span><span class="p">.</span><span class="n">in_dim</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="p">))</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span>
</code></pre></div></div>
<p>and the Feed Forward model is obtained by superimposing linear layers,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">train</span><span class="p">()</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hid_layers</span><span class="o">=</span><span class="p">[</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hid_layers</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sig</span><span class="o">=</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        assume two class problem
        """</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">hid_layers</span><span class="p">:</span>
            <span class="n">out</span><span class="o">=</span><span class="n">layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sig</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        predict
        """</span>
        <span class="c1">#set model to eval mode so we dont need to calculate the derivatives
</span>        <span class="bp">self</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="n">pred</span><span class="o">=</span><span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pred</span><span class="o">=</span><span class="n">pred</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">array</span><span class="o">&gt;=</span><span class="mf">0.5</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'int8'</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">True</span>
    
    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">False</span> 
</code></pre></div></div>

<p>For a two-class problem we define the loss function and also the optimizer</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LogLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="o">=</span><span class="n">Log</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">prob</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="s">"""
        prob: Tensor
        y: Tensor
        """</span>
        <span class="n">not_y</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">.</span><span class="n">array</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="n">T</span>
        <span class="n">not_y</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">not_y</span><span class="p">)</span>
        <span class="n">y_</span><span class="o">=</span><span class="n">y</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="n">T</span>
        <span class="n">y_</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>

        <span class="n">not_prob</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">prob</span><span class="p">.</span><span class="n">array</span>
        <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">prob</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=-</span><span class="n">prob</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
        <span class="n">not_prob</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">not_prob</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>

        <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">prob</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">L</span><span class="o">=</span><span class="n">y_</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span><span class="o">+</span><span class="n">not_y</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">not_prob</span><span class="p">)</span>
        <span class="n">L</span><span class="o">=-</span><span class="n">L</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">L</span><span class="o">=</span><span class="n">size</span><span class="o">*</span><span class="n">L</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span><span class="o">=</span><span class="n">L</span><span class="p">.</span><span class="n">grad</span>

        <span class="k">return</span> <span class="n">L</span><span class="p">.</span><span class="n">array</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span>

<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">find_tensor</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">tensor</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">=</span><span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="s">'none'</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span> 

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">:</span>
                    <span class="n">tensor</span><span class="p">.</span><span class="n">array</span><span class="o">-=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'No grads!'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tensors</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">param1</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">__dict__</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param1</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">tensors</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">param1</span><span class="p">)]</span><span class="o">=</span><span class="n">param1</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param1</span><span class="p">,</span><span class="s">'__dict__'</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">param2</span> <span class="ow">in</span> <span class="n">param1</span><span class="p">.</span><span class="n">__dict__</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param2</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
                        <span class="n">tensors</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">param2</span><span class="p">)]</span><span class="o">=</span><span class="n">param2</span>
        <span class="k">return</span> <span class="n">tensors</span>

</code></pre></div></div>
<p>For mini-batch gradient descent we use the data loader</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DataSet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">28</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="o">=</span><span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_y</span><span class="o">=</span><span class="n">y</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bsz</span><span class="o">=</span><span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">L</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bsz</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">bsz</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">bsz</span><span class="p">):</span>
            <span class="n">batch_x</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">bsz</span><span class="p">])</span>
            <span class="n">batch_y</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">bsz</span><span class="p">])</span>
            <span class="k">yield</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span>
</code></pre></div></div>

<p>Now, we are ready to train a one-hidden layer model. As an example, we load the breast_cancer dataset from sklearn api.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span> 

<span class="n">data</span><span class="o">=</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>
<span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">/</span><span class="n">x</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>

<span class="n">data_loader</span><span class="o">=</span><span class="n">DataSet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="mi">128</span><span class="p">)</span>
</code></pre></div></div>

<p>Define model, loss function and optimizer</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">model</span><span class="o">=</span><span class="n">FeedForward</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">loss</span><span class="o">=</span><span class="n">LogLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>Perform training</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="n">epochs</span><span class="p">):</span>
        
    <span class="n">L</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">total_loss</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="o">=</span><span class="n">batch</span>
            <span class="n">bsz</span><span class="o">=</span><span class="n">x_batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">out</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>
            <span class="n">total_loss</span><span class="o">+=</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y_batch</span><span class="p">)</span><span class="o">*</span><span class="n">bsz</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">epoch</span><span class="o">%</span><span class="mi">10</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Loss: '</span><span class="p">,</span><span class="n">total_loss</span><span class="o">/</span><span class="n">L</span><span class="p">)</span>
    
<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>

</code></pre></div></div>

  </div><a class="u-url" href="/machine%20learning/2021/01/15/grads_lib.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




