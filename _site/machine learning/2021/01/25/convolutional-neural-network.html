<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Convolutional Neural Network | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Convolutional Neural Network" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A convolutional neural network can perform vision tasks such as image classification. The convolution operation creates features that carry information about the spatial distribution of the data. We implement a convolutional neural network from scratch using tensors in python." />
<meta property="og:description" content="A convolutional neural network can perform vision tasks such as image classification. The convolution operation creates features that carry information about the spatial distribution of the data. We implement a convolutional neural network from scratch using tensors in python." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2021/01/25/convolutional-neural-network.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2021/01/25/convolutional-neural-network.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/conv_layers.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-25T00:00:00+01:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/conv_layers.png" />
<meta property="twitter:title" content="Convolutional Neural Network" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2021/01/25/convolutional-neural-network.html","image":"http://localhost:4000/conv_layers.png","headline":"Convolutional Neural Network","dateModified":"2021-01-25T00:00:00+01:00","datePublished":"2021-01-25T00:00:00+01:00","description":"A convolutional neural network can perform vision tasks such as image classification. The convolution operation creates features that carry information about the spatial distribution of the data. We implement a convolutional neural network from scratch using tensors in python.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2021/01/25/convolutional-neural-network.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Convolutional Neural Network</h1>
    <p class="post-meta"><time class="dt-published" datetime="2021-01-25T00:00:00+01:00" itemprop="datePublished">
        Jan 25, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ol>
  <li><a href="#def1">Convolutional Neural Network</a></li>
  <li><a href="#python">Python implementation</a></li>
</ol>

<p><a name="def1"></a></p>
<h3 id="1-convolutional-neural-network"><strong>1. Convolutional neural network</strong></h3>

<p>A convolutional neural network incorporates geometrical features in the learning algorithm. The neural network uses the convolution operation to capture the spatial correlation between data at different positions. This type of architecture is most suitable for vision tasks like image classification. The convolution amounts to a linear operation over smaller regions of the image, much like a moving average, except that this is not normalized. More explicitly, for a matrix of pixels $m_{ij}$ with size $N\times N$, and a kernel $K$ with weights $w_{\mu\nu}$ and size $k\times k$ we calculate</p>

\[K(m_{ij})=\sum_{\mu,\nu=0}^{k-1} w_{\mu\nu}m_{i+\mu,j+\nu}+b\]

<p>where $b$ is a bias. The convolution operation is depicted in the picture below for a kernel of size $4\times4$.</p>

<div style="text-align: center"><img src="/images/conv_img.png" width="50%" /></div>

<p>The convolution runs over the pixels where the kernel is allowed to be inside the image. That is, we have $m_{i+\mu,j+\nu}=m_{i’j’}$. We can relax this condition and allow for a padding $P$ that we add at the beginning and end of the image. Then we ensure that all pixels living in the padding regions are zero, that is, $m_{ij}=0$ for $i,j\in [-P,0[\,\cup\, ]N-1,N-1+P]$. One can also define a stride $S$ that determines the pixels the kernel runs over, that is, $(i,j)=(-P\text{ mod}(S),-P \text{ mod(S)})$. Therefore, if the pixel matrix has size $N\times N$ then the output of the convolution has shape $N’\times N’$ with</p>

\[N'=(N+2P-k+1)//S+1\]

<p>After the convolution operation, we apply a non-linear activation function on each element of the matrix \(m'_{i'j'}\). The result is the output of a convolutional layer.  Besides, one can create channels whereby we apply multiple kernels to the same input. So if the kernel \(K^c\) has C channels, the convolutional layer’s output is $C$ matrices \(m^c_{i'j'}\). Similarly, for each matrix \(m^c_{i'j'}\) one can further apply a kernel with \(C'\) channels. The result of using first the kernel \(K^c\), and then \(K^{c'}\) is \(C\times C'\) matrices.</p>

<p>After the convolutional layers, the resulting matrix is flattened and added as an input to a feed-forward neural network. Below we show this series of operations.</p>

<div style="text-align: center"><img src="/images/conv_layers.png" width="50%" /></div>

<p><a name="python"></a></p>
<h3 id="2-python-implementation"><strong>2. Python implementation</strong></h3>

<p>Using the <strong>grad_lib</strong> library that we have built in the previous post, we can build neural networks more easily. First we define a convolutional neural network</p>

<h3 id="convolutional-neural-network-1-channel">Convolutional Neural Network (1 channel)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">grad_lib</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">,</span> <span class="n">Softmax</span><span class="p">,</span> <span class="n">LinearLayer</span><span class="p">,</span> <span class="n">Log</span><span class="p">,</span> <span class="n">DataSet</span> 

<span class="k">class</span> <span class="nc">ConvNet2D</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="s">"""Convolutional Layer with 2D kernel

        Args:
            kernel_size ([type]): [description]
            img_size (tuple, optional): [description]. Defaults to (8,8).
            stride (int, optional): [description]. Defaults to 1.
            padding (int, optional): [description]. Defaults to 0.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="o">=</span><span class="n">stride</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="o">=</span><span class="n">padding</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">init_param</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">get_out_dim</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="s">"""trainable Tensors
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">trainable</span><span class="o">=</span><span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">,</span>
                        <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
                        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""forward

        Args:
            x (Tensor): (batch,S,S)

        Returns:
            Tensor: (batch,num_neurons)
        """</span>
        <span class="n">x_tensor</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="o">*</span><span class="n">x_tensor</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">init_param</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weight</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">)</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> 

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""transform batch of images

        Args:
            x (numpy.array): (batch,S,S)

        Returns:
            Tensor: (kernel_size**2,batch,num_neurons)
        """</span>
        <span class="c1"># x: array
</span>        <span class="n">i_f</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">j_f</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">out_list</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">i_f</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">j_f</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">):</span>
                    <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">out</span><span class="p">,</span><span class="n">z</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">out_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        
        <span class="c1">#out_list: [batch,num_neurons,kernel_size**2]
</span>        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">out_list</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_out_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">test</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test</span><span class="p">).</span><span class="n">shape</span>

        <span class="k">return</span> <span class="n">size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="convolutional-neural-network-model-for-multi-class-problem">Convolutional neural network model for multi-class problem</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ConvClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">img_size</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="s">"""Convolutional Neural Network Classifier

        Args:
            img_size (tuple): (width,height)
            hidden_dim (int): number of hidden neurons
            out_dim (int, optional): number of class. Defaults to 1.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">convnet</span><span class="o">=</span><span class="n">ConvNet2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">)</span>
        <span class="n">in_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">convnet</span><span class="p">.</span><span class="n">out_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">softmax</span><span class="o">=</span><span class="n">Softmax</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""[summary]

        Args:
            x (numpy.array): input must be array, not Tensor

        Returns:
            probability: (batch,out_dim)
        """</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">prob</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prob</span>
        
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">True</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">eval</span><span class="p">():</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">False</span> 

</code></pre></div></div>

<h3 id="cross-entropy-loss-and-optimizer">Cross-Entropy loss and Optimizer</h3>
<p>We also need to write the cross-entropy loss function and optimizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="o">=</span><span class="n">Log</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">prob</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="s">"""loss function

        Args:
            prob (probability Tensor): (batch,num_classes)
            y (array): (batch,1)
        """</span>
        <span class="n">bsz</span><span class="o">=</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prob_</span><span class="o">=</span><span class="n">prob</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bsz</span><span class="p">),</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">bsz</span><span class="p">)</span><span class="o">*</span><span class="n">loss</span> 

        <span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span><span class="o">=</span><span class="n">loss</span><span class="p">.</span><span class="n">grad</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">array</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span>

<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">obj</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">__dict__</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="s">'trainable'</span><span class="p">):</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">obj</span><span class="p">.</span><span class="n">trainable</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">tensor</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">=</span><span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="s">'none'</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span> 

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">:</span>
                    <span class="n">tensor</span><span class="p">.</span><span class="n">array</span><span class="o">-=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'No grads!'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="training-function">Training function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">L</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">total_loss</span><span class="o">=</span><span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="p">,</span><span class="n">batch_y</span><span class="o">=</span><span class="n">batch</span>
            <span class="n">bsz</span><span class="o">=</span><span class="n">batch_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">pred</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>
            <span class="n">total_loss</span><span class="o">+=</span><span class="n">loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">batch_y</span><span class="p">.</span><span class="n">array</span><span class="p">)</span><span class="o">*</span><span class="n">bsz</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: '</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span><span class="s">" Loss: "</span><span class="p">,</span><span class="n">total_loss</span><span class="o">/</span><span class="n">L</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="example">Example</h2>

<p>Use the $8\times8$ version of the MNIST dataset as a toy-model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>

<span class="n">data</span><span class="o">=</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">imgs</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span>
<span class="n">target</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>

<span class="c1">#normalize the pixels for easier training
</span><span class="n">img_norm</span><span class="o">=</span><span class="n">imgs</span><span class="o">/</span><span class="mi">16</span>

<span class="c1">#create iterator
</span><span class="n">data_loader</span><span class="o">=</span><span class="n">DataSet</span><span class="p">(</span><span class="n">img_norm</span><span class="p">,</span><span class="n">target</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">model</span><span class="o">=</span><span class="n">ConvClassifier</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span><span class="n">hidden_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">loss</span><span class="o">=</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

  </div><a class="u-url" href="/machine%20learning/2021/01/25/convolutional-neural-network.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




