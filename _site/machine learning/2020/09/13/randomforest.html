<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Random Forest | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Random Forest" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A random forest is an ensemble of decision trees. The trees are fitted in random samples of the training set, preventing overfitting and reducing variance." />
<meta property="og:description" content="A random forest is an ensemble of decision trees. The trees are fitted in random samples of the training set, preventing overfitting and reducing variance." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2020/09/13/randomforest.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2020/09/13/randomforest.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/randomforest.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-13T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/randomforest.png" />
<meta property="twitter:title" content="Random Forest" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2020/09/13/randomforest.html","image":"http://localhost:4000/randomforest.png","headline":"Random Forest","dateModified":"2020-09-13T00:00:00+02:00","datePublished":"2020-09-13T00:00:00+02:00","description":"A random forest is an ensemble of decision trees. The trees are fitted in random samples of the training set, preventing overfitting and reducing variance.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2020/09/13/randomforest.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Random Forest</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-09-13T00:00:00+02:00" itemprop="datePublished">
        Sep 13, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul>
  <li><a href="#1-bagging-and-decision-trees"><strong>1. Bagging and Decision Trees</strong></a></li>
  <li><a href="#2-ensembles-and-random-forest"><strong>2. Ensembles and Random forest</strong></a></li>
  <li><a href="#3-python-implementation"><strong>3. Python Implementation</strong></a></li>
</ul>

<p><a name="def1"></a></p>
<h3 id="1-bagging-and-decision-trees"><strong>1. Bagging and Decision Trees</strong></h3>

<p>Bagging, short for bootstrap aggregating, is the process by which we train an ensemble of machine learning models using datasets sampled from the empirical distribution. This process helps reduce variance and overfitting. Given a dataset $S$, we generate $m$ samples $Sâ€™$ of size $n$, by drawing datapoints from $S$ uniformly and with replacement. We can then create an ensemble by fitting $m$ models on each sample and averaging (regression) or voting (classification) the result of each of the models. If each of these models is a decision tree, then this ensemble is a random forest.</p>

<p>To take advantage of the bootstrapping mechanism, each of the ensemble models must be independent of each other. This is not always the case because usually there are features the model learns more strongly than others, effectively making the different models depend on each other. To remediate this, we do not allow the decision tree to learn all the features. Instead, each of the models knows different subsets of features.  After fitting the models, the predicted class is determined by the majority vote. In the case of regression, we average each of the predictions.</p>

<p><a name="forest"></a></p>
<h3 id="2-ensembles-and-random-forest"><strong>2. Ensembles and Random forest</strong></h3>

<p>We analyze the effect of bootstrapping decision trees on the generalization error and bias/variance tradeoff.</p>

<p>Suppose we have $m$ models $V^{a}$, with $a=1\ldots m$. In the case of regression, consider the model average</p>

\[\bar{V}(x)=\sum_a \omega_a V^a(x)\]

<p>where $\omega_a$ are some weights. The ambiguity $A(x)^a$ for the model $a$ is defined as</p>

\[A^a(x)=(V^a(x)-\bar{V}(x))^2\]

<p>and the ensemble ambiguity $A(x)$ is obtained by taking the ensemble average</p>

\[A(x)=\sum_a \omega_aA^a(x)=\sum_a \omega_a(V^a(x)-\bar{V}(x))^2\]

<p>The error of a model and the ensemble, respectively $\epsilon^a$ and $\epsilon$, are</p>

\[\begin{equation*}\begin{split}&amp;\epsilon^a(x)=(y(x)-V^a(x))^2 \\
&amp;\epsilon= (y(x)-\bar{V}(x))^2
\end{split}\end{equation*}\]

<p>One can easily show that</p>

\[A(x)=\sum_a \omega_a\epsilon^a(x)-\epsilon(x)=\bar{\epsilon}(x)-\epsilon(x)\]

<p>where we defined the ensemble average $\bar{\epsilon}=\sum_a \omega_a\epsilon^a$. Averaging this quantities over the distribution of $x$, $D(x)$, we obtain an equation involving the generalization error of the ensemble and of the individual components, that is</p>

\[E=\bar{E}-A\]

<p>where $E=\int dx \epsilon(x) D(x)$ is the generalization error and $A=\int dx A(x) D(x)$ is the total ambiguity.</p>

<p>Note that the ambiguity $A$ only depends on the models $V^a$ and not on labeled data. It measures how the different models correlate with the average. Since $A$ is always positive, we can conclude that the generalization error is smaller than the average error.</p>

<p>If the models are highly biased, we expect similar predictions across the ensemble, making $A$ small. In this case, the generalization error will be essentially the same as the average of the generalization errors. However, if the predictions vary a lot from one model to another, the ambiguity will be higher,  making the generalization smaller than the average. So we want the models to disagree! Random forests implement this by letting each decision tree learn on a different subset of every split feature. This results in a set of trees with different split structure:</p>

<div style="text-align: center"><img src="/images/randomforest.png" width="80%" /></div>

<p>Another important aspect of ensemble methods is that they do not increase the bias of the model. For instance</p>

\[\begin{equation*}\begin{split}\text{Bias}=f(x)-\mathbb{E}\bar{V}(x)=\sum_a \omega_a (f(x)-\mathbb{E}V^a(x))=\sum_a \omega_a \text{Bias}^a=\text{bias}
\end{split}\end{equation*}\]

<p>where $\text{bias}$ is the bias of an individual model, assuming that each model has similar bias. On the other hand, the variance</p>

\[\mathbb{E}(\bar{V}(x)-\mathbb{E}\bar{V}(x))^2=\sum_a \omega_a^2(V^a-\mathbb{E}V^a)^2+\sum_{a\neq b}\omega_a\omega_b(V^a-\mathbb{E}V^a)(V^b-\mathbb{E}V^b)\]

<p>We do not expect the quantities $(V^a-\mathbb{E}V^a)^2$ and $(V^a-\mathbb{E}V^a)(V^b-\mathbb{E}V^b)$ to differ significantly across the models, and so defining</p>

\[\text{Var}\equiv (V^a-\mathbb{E}V^a)^2,\; \rho(x)\equiv\frac{(V^a-\mathbb{E}V^a)(V^b-\mathbb{E}V^b)}{\text{Var}(x)}\]

<p>we obtain</p>

\[\begin{equation*}\begin{split}\mathbb{E}(\bar{V}(x)-\mathbb{E}\bar{V}(x))^2&amp;=\text{Var}(x)\sum_a \omega_a^2 + \rho(x)\text{Var}(x) \sum_{a\neq b}\omega_a\omega_b\\
&amp;=\text{Var}(x)(1-\rho(x))\sum_a\omega_a^2+\rho(x)\text{Var}(x)&lt;\text{Var}(x)\end{split}\end{equation*}\]

<p>This quantity has a lower bound at $\omega_a=1/m$, the uniform distribution. This means that</p>

\[\text{Var}(x)\frac{(1-\rho(x))}{m}+\rho(x)\text{Var}(x)\leq \mathbb{E}(\bar{V}(x)-\mathbb{E}\bar{V}(x))^2\leq \text{Var}(x)\]

<p>If the models are averaged with constant weights, then $\sum_a \omega_a^2$ tends to zero as $m\rightarrow \infty$, and the variance is the product of the correlation $\rho(x)$ and the individual model variance.</p>

<p><a name="python"></a></p>
<h3 id="3-python-implementation"><strong>3. Python Implementation</strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RandomForest</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_estimators</span><span class="p">,</span><span class="n">params</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="o">=</span><span class="n">params</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">trees</span><span class="o">=</span><span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">n_instances</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="o">=</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">trees</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_estimators</span><span class="p">):</span>
            <span class="n">idx</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_instances</span><span class="p">)</span>
            <span class="n">idx_sample</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span><span class="n">n_instances</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="n">xsample</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">idx_sample</span><span class="p">]</span>
            <span class="n">ysample</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">idx_sample</span><span class="p">]</span>
            <span class="n">tree</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">params</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span><span class="s">'auto'</span><span class="p">)</span>
            <span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xsample</span><span class="p">,</span><span class="n">ysample</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">trees</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">classes</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">trees</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">classes_</span>
        <span class="n">dic</span><span class="o">=</span><span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">cl</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">cl</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">classes</span><span class="p">)}</span>
        <span class="n">ypred</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">trees</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">trees</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">ypred</span><span class="o">+=</span><span class="n">tree</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ypred</span><span class="o">=</span><span class="n">ypred</span>
        <span class="n">ypred</span><span class="o">=</span><span class="n">ypred</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">ypred</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">dic</span><span class="p">.</span><span class="n">get</span><span class="p">)(</span><span class="n">ypred</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">ypred</span>
</code></pre></div></div>

  </div><a class="u-url" href="/machine%20learning/2020/09/13/randomforest.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




