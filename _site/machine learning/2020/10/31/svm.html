<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Support Vector Machine (SVM) | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Support Vector Machine (SVM)" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="An SVM algorithm learns the data by engineering the optimal separating line/curve between the classes. Compare with the other algorithms which try to do this by first determining the Bayes predictor. The SVM has robust generalization properties. However, they are usually tough to train." />
<meta property="og:description" content="An SVM algorithm learns the data by engineering the optimal separating line/curve between the classes. Compare with the other algorithms which try to do this by first determining the Bayes predictor. The SVM has robust generalization properties. However, they are usually tough to train." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2020/10/31/svm.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2020/10/31/svm.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/svm.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-10-31T00:00:00+01:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/svm.png" />
<meta property="twitter:title" content="Support Vector Machine (SVM)" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2020/10/31/svm.html","image":"http://localhost:4000/svm.png","headline":"Support Vector Machine (SVM)","dateModified":"2020-10-31T00:00:00+01:00","datePublished":"2020-10-31T00:00:00+01:00","description":"An SVM algorithm learns the data by engineering the optimal separating line/curve between the classes. Compare with the other algorithms which try to do this by first determining the Bayes predictor. The SVM has robust generalization properties. However, they are usually tough to train.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2020/10/31/svm.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Support Vector Machine (SVM)</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-10-31T00:00:00+01:00" itemprop="datePublished">
        Oct 31, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul>
  <li><a href="#1-linear-svm"><strong>1. Linear SVM</strong></a></li>
  <li><a href="#2-generalization-properties"><strong>2. Generalization properties</strong></a></li>
  <li><a href="#3-non-linear-decision-boundary"><strong>3. Non-linear decision boundary</strong></a></li>
  <li><a href="#4-python-implementation"><strong>4. Python implementation</strong></a></li>
  <li><a href="#references"><strong>References</strong></a></li>
</ul>

<p><a name="def1"></a></p>
<h3 id="1-linear-svm"><strong>1. Linear SVM</strong></h3>

<p>The Support Vector Machine is a learning algorithm whose primary goal is to find the optimal decision boundary. In the separable case, when the decision boundary is a hyperplane, we can show that the solution only depends on a few data points, known as support vectors, and hence the name.</p>

<ul>
  <li><strong>Linearly separable case:</strong></li>
</ul>

<p>Let’s say we are in two dimensions, and we have a dataset with two labels. We want to find the line that achieves maximal separation between the two classes. That is, of all the separating lines, we want to find the one that maximizes the margin $\rho$, as depicted below.</p>

<div style="text-align: center"><img src="/images/svm.png" width="50%" /></div>

<p>A line has equation
\(\omega^T\cdot x+b=0\)
where $x=(x_1,x_2)$ are the 2d coordinates and $\omega$ is the normal vector.
The distance between a point with coordinates $x$ and the line is given by $\omega^T(x-x_0)/{\lVert\omega\rVert}$, where $x_0$ is a point on the plane. Since $\omega^T\cdot x_0=-b$, the signed distance is
\(d=\frac{\omega^T\cdot x+b}{\lVert\omega\rVert}\)</p>

<p>The margin is defined as a the minimum distance $C$ from the separating line, that is, $C=\text{Min }{y_i d_i}$. The optimal separating line maximizes this margin, that is,
\(y_i\frac{\omega^T\cdot x_i+b}{\lVert\omega\rVert}\geq \text{Max }C=\rho,\;\forall (x_i,y_i)\)
We have multiplied by the target $y_i\in{-1,1}$ to guarantee that each term is always positive on both sides of the separating line. Since the line equation is invariant under rescaling $(\omega,b)\rightarrow (\lambda \omega,\lambda b)$ we can choose $\lVert\omega\rVert=1/\rho$. This means that maximizing $\rho$ as above is equivalent to
\(\text{Min }\lVert\omega\rVert,\;\;y_i(\omega^T\cdot x_i+b)\geq 1,\;\forall (x_i,y_i)\)</p>

<p>We can translate this minization problem to finding the minima of the loss function
\(L=\frac{1}{2}\sum_{k=1}^d \omega_k^2 -\sum_{i=1}^N\alpha_i[ y_i(\omega^T\cdot x_i+b)-1],\;\alpha_i\geq 0\)
where $\alpha_i$ are Lagrange multipliers, $d$ is the number of dimensions and $N$ is the number of datapoints. The local minima solves the equations</p>

\[\begin{equation*}\begin{split}&amp;\frac{\partial L}{\partial \omega_k}=\omega_k -\sum_{i=1}^N\alpha_i y_ix^k_i=0\\
&amp;\frac{\partial L}{\partial b}=\sum_i\alpha_iy_i=0\\
&amp;\frac{\partial L}{\partial \alpha_i}= y_i(\omega^T\cdot x_i+b)-1=0
\end{split}\end{equation*}\]

<p>Provided $\alpha_i&gt;0$ for which the loss function is differentiable in $\alpha$. If $\alpha_i=0$ then the we only have the first two equations. This means that $\alpha_i&gt;0$ corresponds to points that sit exactly on the margin, and the remaining equations depend only on these points. These are known as support vectors.</p>

<p>If we replace $\omega$ with its equation in the loss function $L$, we obtain the dual problem:</p>

\[\hat{L}=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i \alpha_j y_iy_j x_i^T\cdot x_j,\;\alpha_i\geq 0\]

<p>where we have used that $\sum_i\alpha_iy_i=0$. The problem with $-\hat{L}$ is actually a convex minimization problem and can be solved using traditional methods. The solution of the dual problem must suplemented with the additional conditions:</p>

\[\begin{equation*}\begin{split}&amp;y_i(\omega^T\cdot x_i+b)=1,\;\alpha_i&gt;0\\
&amp;y_i(\omega^T\cdot x_i+b)&gt;1,\;\alpha_i=0\\
&amp;\sum_i\alpha_i y_i=0\end{split}\end{equation*}\]

<p>Support vectors live on the margin and thus $y_i(\omega^T\cdot x_i+b)=1$. Given a support vector $x_s,y_s$, we can use this equation to determine $b$
\(b=y_s-\sum_{i=1}^m\alpha_p y_px_p^T\cdot x_s\)
where $p=1\ldots m$ runs over the support vectors. $\hat{L}$ is maximized by the solution and as such
\(\frac{\partial\hat{L}}{\partial \alpha_s}=1-y_s\sum_{p=1}^m\alpha_py_px_p^T\cdot x_s=0\) 
Multiplying this equation by $\alpha_s$ and summing over $s$ we obtain</p>

\[\sum_{s=1}^m\alpha_s-\sum_{s=1}^m\sum_{p=1}^m\alpha_s\alpha_py_sy_px_p^T\cdot x_s=0\iff \lVert\omega\rVert^2=\sum_{s=1}^m\alpha_s=\lVert\alpha\rVert_1\]

<p>So the margin is inversely proportional to the linear norm of $\alpha$.</p>

<ul>
  <li><strong>Non-separable case:</strong></li>
</ul>

<div style="text-align: center"><img src="/images/svm3.png" width="50%" /></div>

<p>In the non-separable case, one cannot find a hyperplane that separates the classes. That is, for any hyperplane, there exists $x_i,y_i$ such that</p>

\[y_i(\omega^T\cdot x_i +b)\ngtr 1\]

<p>See picture above.</p>

<p>However, one can formulate a relaxed version of the constraints using <em>slack variables</em> $\xi_i\geq 0$</p>

\[y_i(\omega^T\cdot x_i +b)\geq 1-\xi_i\]

<p>If we remove the points for which $0 &lt; y_i(\omega^T\cdot x_i +b)&lt;1$ then the data is linearly separable. With the remaining data, we can define a margin called a soft-margin instead of a hard-margin as in the separable case. The points for which $\xi_i$ is non-zero are the outliers.</p>

<p>As before, we want to minimize $\lVert\omega\rVert$, but at the same time we want to use the smallest possible number of $\xi$ with the smallest values possible. This can be written as</p>

\[\frac{1}{2}\lVert\omega\rVert^2+\lambda \sum_i \xi_i^p-\sum_i\alpha_i[y_i(\omega^T\cdot x_i +b)- 1+\xi_i],\;\alpha_i\geq 0,\xi_i\geq 0\]

<p>The term $\lambda \sum_i \xi_i^p$ with $\lambda&gt;0$ works as a regulator, which prevents $\xi_i$ from taking large values as well as having a large number of non-zero $\xi_i$. The exponent $p$ defines different types of regularization. For $p=1$ the loss function becomes the <em>Hinge loss function</em></p>

\[\frac{1}{2}\lVert\omega\rVert^2+\lambda\sum_i \text{max}(0,1-y_i(\omega^T\cdot x_i +b))\]

<p>The next steps are very similar to the separable case. One can build a dual problem and determine the support vectors and outliers.</p>

<p><a name="gen"></a></p>
<h3 id="2-generalization-properties"><strong>2. Generalization properties</strong></h3>

<p>Given the separating hyperplane we can build the predictor</p>

\[h(x)=\text{sign}(\omega\cdot x+b)\]

<p>We want to bound the generalization error</p>

\[R(h_S)=\sum_{x,y} 1_{h(x)\neq y}D(x)\]

<p>To do this we can explore the leave-one-out error $R_{LOO}$. The $R_{LOO}(x)$ is the error on a point $x$ provided we train the algorithm on the remaining $S\setminus{x}$ dataset, that is, with the point $x$ excluded. The empirical $\hat{R}_{LLO}$ is obtained by averaging over all points of the dataset $S$, that is,</p>

\[\hat{R}_{LLO}=\frac{1}{m}\sum_{x} R_{LLO}(x)\]

<p>One can show that the average of $\hat{R}_{LLO}$ is an unbiased estimate of the generalization error. That is,</p>

\[\mathbb{E}_{S\sim D^m} \hat{R}_{LLO}=\mathbb{E}_{S'\sim D^{m-1}}(R(h_{S'}))\]

<p>In more detail,</p>

\[\begin{aligned}\mathbb{E}_{S\sim D^m} \hat{R}_{LLO}&amp;=\mathbb{E}_{S\sim D^m}\frac{1}{m}\sum_{x} R_{LLO}(x)\\
&amp;=\mathbb{E}_{S\sim D^m}1_{h_{S'}(x)\neq y}\\
&amp;=\mathbb{E}_{S'\sim D^{m-1},x\sim D}1_{h_{S'}(x)\neq y}\\
&amp;=\mathbb{E}_{S'\sim D^{m-1}}R(h_{S'})
\end{aligned}\]

<p>Let’s estimate $R_{LLO}(x)$ for the SVM in the separable case. If $x$ is above the margin, then the error is zero because if we remove this point, the predictor will not change as it depends only on the support vectors. However, if $x$ is exactly on the margin, then the new predictor’s support vectors will change. This point $x$ may or may not be correctly classified, and so the maximum number of points that this procedure can misclassify is the same as the number of support vectors. This means that,
\(\hat{R}_{LLO}\leq \frac{NV(S)}{m}\)
where $NV$ is the number of support vectors for the dataset $S$. We may therefore conclude that the average generalization error is bounded by the average number of support vectors, that is,
\(\mathbb{E}_{S'\sim D^{m-1}}R(h_{S'})\leq \frac{\mathbb{E}_{S\sim D^m}NV(S)}{m}\)</p>

<p>Assuming that $NV$ remains small for different datasets, the above result implies that the average error remains small.
<a name="non-linear"></a></p>
<h3 id="3-non-linear-decision-boundary"><strong>3. Non-linear decision boundary</strong></h3>

<p>The dataset may be separable, but the decision boundary is not a hyperplane.
In this case, there should exist a map $\phi: x\rightarrow x’$ that makes the problem linearly separable.
All the previous steps follow except that we use $x’$ instead of $x$. The problem is in determining the map $\phi$, which is usually a difficult problem.</p>

<div style="text-align: center"><img src="/images/svm2.png" width="50%" /></div>

<p>Kernel methods are used to address solutions of this type. Suppose we find such a map. Then we have a new set of features $\phi(x_1),\phi(x_2),\ldots \phi(x_n)$. The dual problem becomes</p>

\[L_D=\sum_{i=1}^N\alpha_i-\frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i \alpha_j y_iy_j \phi(x_i)^T\cdot \phi(x_j),\;\alpha_i\geq 0\]

<p>and the predictor</p>

\[\begin{equation*}\begin{split}G(x)&amp;=\text{sign} \big(\omega^T\phi(x)+b\big)\\
&amp;=\text{sign} \big(\sum_{i=1}^N\alpha_i y_i\phi(x)^T\cdot\phi(x_i)+b\big)
\end{split}\end{equation*}\]

<p>The constant $b$ can be determined from the location of a support vector. So we see that the solution only depends on the scalar $K(x,x’)=\phi(x)^T\cdot \phi(x’)$. The function $K(x,x’)$ is a Kernel function and obeys the following conditions</p>

<ul>
  <li>It is symmetric: $K(x,x’)=K(x’,x)$</li>
  <li>It is positive semi-definite: $\sum_{i,j}K(x_i,x_j)\lambda_i\lambda_j\geq 0,\; \forall \lambda_i$</li>
</ul>

<p>Instead of looking for the map $\phi$, we can search for the Kernel, a scalar function. 
For example, let’s consider the gaussian Kernel:
\(K(x,y)=e^{-\frac{\lVert x-y\rVert^2}{2\sigma^2}}\)
Since this is a positive and symmetric function, it is easy to see that the above conditions are satisfied. But can we find the map $\phi$ such that $K(x,x’)=\phi(x)^T\phi(x’)$? Note that</p>

\[e^{-\frac{\lVert x-y\rVert^2}{2\sigma^2}}=e^{-\frac{x^2+y^2}{2\sigma^2}}\sum_{n=1}^{\infty}\frac{(xy)^n}{(2\sigma^2)^n n!}\]

<p>For each polynomial term in the expansion</p>

\[\begin{equation*}\begin{split}(x y)^n=(\sum_{i=1}^d x_iy_i)^n&amp;=\sum_{\sum_i k_i=n}\frac{n!}{k_1!k_2!\ldots k_d!}(x_1y_1)^{k_1}(x_2y_2)^{k_2}\ldots (x_dy_d)^{k_d}\\
&amp;=\sum_{\sum_i k_i=n}\frac{n!}{k_1!k_2!\ldots k_d!} x_1^{k_1}x_2^{k_2}\ldots x_d^{k_d} y_1^{k_1}y_2^{k_2}\ldots y_d^{k_d}\\
&amp;=h_n(x)^T\cdot h_n(y)
\end{split}\end{equation*}\]

<p>where</p>

\[h_n(x)=(x_1^n,\sqrt{n} x_1^{n-1}x_2,\sqrt{n} x_1^{n-1}x_3,\ldots,\sqrt{\frac{n!}{k_1!k_2!\ldots k_d!}} x_1^{k_1}x_2^{k_2}\ldots x_d^{k_d},\ldots x_d^n)\]

<p>This means that for the gaussian Kernel the map $\phi$ is infinite dimensional. This shows how powerful Kernel methods can be.</p>

<p><a name="python"></a></p>
<h3 id="4-python-implementation"><strong>4. Python implementation</strong></h3>

<ul>
  <li>The CVXOPT library can be used to solve quadratic optimization problems with constraints.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cvxopt</span>

<span class="k">class</span> <span class="nc">LinearSVM</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">num_iter</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">coef_</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">intercept_</span><span class="o">=</span><span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight_</span><span class="o">=</span><span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">support_vectors_</span><span class="o">=</span><span class="p">{}</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span><span class="o">=</span><span class="n">num_iter</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">y_aux</span><span class="o">=</span><span class="n">y</span><span class="p">.</span><span class="n">copy</span><span class="p">().</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_aux</span><span class="o">=</span><span class="n">y_aux</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'float64'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="o">=</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">cvxopt</span><span class="p">.</span><span class="n">solvers</span><span class="p">.</span><span class="n">options</span><span class="p">[</span><span class="s">'show_progress'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>
        
        <span class="n">pairs</span><span class="o">=</span><span class="p">[[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">classes</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">classes</span> <span class="k">if</span> <span class="n">j</span><span class="o">&gt;</span><span class="n">i</span><span class="p">]]</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
            <span class="n">idx</span><span class="o">=</span><span class="p">(</span><span class="n">y_aux</span><span class="o">==</span><span class="n">i</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">y_aux</span><span class="o">==</span><span class="n">j</span><span class="p">)</span>
            <span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">x_temp</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">y_temp</span><span class="o">=</span><span class="n">y_aux</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">y_temp</span><span class="p">[</span><span class="n">y_temp</span><span class="o">==</span><span class="n">j</span><span class="p">]</span><span class="o">=-</span><span class="mf">1.0</span>
            <span class="n">y_temp</span><span class="p">[</span><span class="n">y_temp</span><span class="o">==</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mf">1.0</span>
            
            <span class="n">z</span><span class="o">=</span><span class="n">y_temp</span><span class="o">*</span><span class="n">x_temp</span>
            <span class="n">Q</span><span class="o">=</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">z</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">[(</span><span class="mi">1</span><span class="p">),(</span><span class="mi">1</span><span class="p">)])</span>
            <span class="n">Q</span><span class="o">=</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">Q</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="n">p</span><span class="o">=-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x_temp</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">p</span><span class="o">=</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="n">G</span><span class="o">=-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x_temp</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">G</span><span class="o">=</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">G</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="n">h</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x_temp</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">h</span><span class="o">=</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">h</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="n">A</span><span class="o">=</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">y_temp</span><span class="p">.</span><span class="n">tolist</span><span class="p">())</span>
            <span class="n">b</span><span class="o">=</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

            <span class="n">sol</span><span class="o">=</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">solvers</span><span class="p">.</span><span class="n">qp</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
            <span class="n">sol</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="s">'x'</span><span class="p">])</span>

            <span class="c1">#support vectors
</span>            <span class="n">sup_vec_loc</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">sol</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,)</span>
            <span class="n">sol</span><span class="p">[</span><span class="n">sup_vec_loc</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
            <span class="n">sup_vec_loc</span><span class="o">=</span><span class="n">sup_vec_loc</span><span class="o">!=</span><span class="mi">0</span>
            <span class="n">sup_vec</span><span class="o">=</span><span class="n">x_temp</span><span class="p">[</span><span class="n">sup_vec_loc</span><span class="p">]</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">support_vectors_</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span><span class="o">=</span><span class="n">sup_vec</span>

            <span class="c1">#margin
</span>            <span class="n">w</span><span class="o">=</span><span class="p">((</span><span class="n">sol</span><span class="o">*</span><span class="n">y_temp</span><span class="p">)</span><span class="o">*</span><span class="n">x_temp</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">y_s</span><span class="o">=</span><span class="n">y_temp</span><span class="p">[</span><span class="n">sup_vec_loc</span><span class="p">]</span>
            <span class="n">v</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">sup_vec</span><span class="p">,</span><span class="n">w</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">coef</span><span class="o">=</span><span class="p">(</span><span class="n">y_s</span><span class="o">*</span><span class="n">v</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span><span class="o">-</span><span class="n">y_s</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="n">v</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
            <span class="n">coef</span><span class="o">=</span><span class="n">coef</span><span class="o">/</span><span class="p">(</span><span class="n">v</span><span class="p">.</span><span class="n">var</span><span class="p">())</span>
            <span class="n">intercept</span><span class="o">=</span><span class="n">y_s</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span><span class="o">-</span><span class="n">coef</span><span class="o">*</span><span class="n">v</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>

            <span class="bp">self</span><span class="p">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span><span class="o">=</span><span class="n">intercept</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">weight_</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span><span class="o">=</span><span class="n">coef</span><span class="o">*</span><span class="n">w</span>
            
</code></pre></div></div>
<ul>
  <li>SMO algorithm</li>
</ul>

<p>In the SMO algorithm, or sequential minimal optimization, we solve the dual minimization problem iteratively. First we randomly initialize all $\alpha_i$. Then we choose a random pair of $\alpha$’s, say $\alpha_0,\alpha_1$ and solve for the minimum of $L(\alpha_0,\alpha_1)$ with the other $\alpha$ fixed. This is easy to do because the function $L(\alpha_0,\alpha_1)$ is actually one dimensional after using the constraint $\sum_i \alpha_i y_i=0$. Then we proceed with a different pair of $\alpha$’s  and repeat until the solution converges.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="s">"""Ensure that the target y has values -1,1
    Step 1): generate all (i,j) to get access to alpha pairs
    Step 2):
"""</span>
<span class="n">alpha</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,(</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">))</span>
<span class="n">alpha</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>

<span class="n">l</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>
<span class="n">samples</span><span class="o">=</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">l</span> <span class="k">if</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">j</span><span class="p">]</span>
<span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">num_iter</span><span class="o">=</span><span class="mi">0</span>
<span class="n">threshold</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>
<span class="n">alpha_prev</span><span class="o">=</span><span class="n">alpha</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">T</span><span class="o">=</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">y</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">Z</span><span class="o">=</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">y</span><span class="o">*</span><span class="n">x</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">not_converged</span><span class="o">=</span><span class="bp">True</span>
<span class="k">while</span> <span class="n">not_converged</span> <span class="ow">and</span> <span class="n">num_iter</span><span class="o">&lt;</span><span class="mi">5</span><span class="o">*</span><span class="n">threshold</span><span class="p">:</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="n">samples</span><span class="p">:</span>
        
        <span class="n">alpha0</span><span class="o">=</span><span class="n">alpha</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">alpha1</span><span class="o">=</span><span class="n">alpha</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">y0</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">y1</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1">#If constraint is possible to be solved continue
</span>        <span class="n">k</span><span class="o">=-</span><span class="n">T</span><span class="o">+</span><span class="n">alpha0</span><span class="o">*</span><span class="n">y0</span><span class="o">+</span><span class="n">alpha1</span><span class="o">*</span><span class="n">y1</span>
        <span class="k">if</span> <span class="n">y1</span><span class="o">*</span><span class="n">k</span><span class="o">&lt;</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">y0</span><span class="o">*</span><span class="n">y1</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        
        <span class="c1"># Solve 2-dimensional optimization problem with constraint
</span>        <span class="n">A</span><span class="o">=</span><span class="n">Z</span><span class="o">-</span><span class="n">alpha0</span><span class="o">*</span><span class="n">y0</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">alpha1</span><span class="o">*</span><span class="n">y1</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">B</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">y0</span><span class="o">*</span><span class="n">y1</span><span class="o">-</span><span class="n">y0</span><span class="o">*</span><span class="n">k</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">])</span><span class="o">-</span><span class="n">y0</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">A</span><span class="p">)</span>
        <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">B</span><span class="p">,</span><span class="n">B</span><span class="p">))</span>

        <span class="n">alpha0</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">a</span><span class="p">)</span>
        <span class="n">alpha1</span><span class="o">=</span><span class="n">y1</span><span class="o">*</span><span class="n">k</span><span class="o">-</span><span class="n">alpha0</span><span class="o">*</span><span class="n">y0</span><span class="o">*</span><span class="n">y1</span>
        <span class="k">if</span> <span class="n">alpha1</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">alpha1</span><span class="o">=</span><span class="mi">0</span>
            <span class="n">alpha0</span><span class="o">=</span><span class="n">k</span><span class="o">*</span><span class="n">y0</span>

        <span class="c1">#update alpha
</span>        <span class="n">alpha</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="n">alpha0</span>
        <span class="n">alpha</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="n">alpha1</span>
        
        <span class="n">T</span><span class="o">=-</span><span class="n">k</span><span class="o">+</span><span class="n">alpha0</span><span class="o">*</span><span class="n">y0</span><span class="o">+</span><span class="n">alpha1</span><span class="o">*</span><span class="n">y1</span>
        <span class="n">Z</span><span class="o">=</span><span class="n">A</span><span class="o">+</span><span class="n">alpha0</span><span class="o">*</span><span class="n">y0</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">alpha1</span><span class="o">*</span><span class="n">y1</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        
        <span class="c1"># verify if alpha converges
</span>        <span class="n">error</span><span class="o">=</span><span class="n">alpha</span><span class="o">-</span><span class="n">alpha_prev</span>    
        <span class="n">error</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">error</span><span class="p">).</span><span class="nb">max</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">error</span><span class="o">&lt;</span><span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">)</span> <span class="ow">and</span> <span class="n">num_iter</span><span class="o">&gt;</span><span class="mi">3</span><span class="o">*</span><span class="n">threshold</span><span class="p">:</span>
            <span class="n">not_converged</span><span class="o">=</span><span class="bp">False</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'converged'</span><span class="p">)</span>
            <span class="k">break</span>
        <span class="n">alpha_prev</span><span class="o">=</span><span class="n">alpha</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">num_iter</span><span class="o">+=</span><span class="mi">1</span>
</code></pre></div></div>

<h3 id="references"><strong>References</strong></h3>
<p><br /></p>

<p>[1] <em>Understanding Machine Learning: from Theory to Algorithms</em>, Shai Ben-David and Shai Shalev-Shwartz</p>

<p>[2] <em>The elements of statistical learning</em>, T. Hastie, R. Tibshirani, J. Friedman</p>

<p>[3] <em>Foundations of machine learning</em>, M. Mohri, A. Rostamizadeh, A. Talwalkar</p>


  </div><a class="u-url" href="/machine%20learning/2020/10/31/svm.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




