<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-01-08T18:33:50+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Data Science and Machine Learning</title><subtitle>Machine Learning algorithms in Python, statistics and cloud computing.</subtitle><author><name>Joao Gomes</name></author><entry><title type="html">Model Deployment with AWS Sagemaker</title><link href="http://localhost:4000/cloud-computing/2021/12/01/model-deployment.html" rel="alternate" type="text/html" title="Model Deployment with AWS Sagemaker" /><published>2021-12-01T00:00:00+01:00</published><updated>2021-12-01T00:00:00+01:00</updated><id>http://localhost:4000/cloud-computing/2021/12/01/model-deployment</id><content type="html" xml:base="http://localhost:4000/cloud-computing/2021/12/01/model-deployment.html"><![CDATA[<h3 id="write-custom-container">Write custom Container</h3>

<p>We can deploy our model in AWS Sagemaker using a custom container. We create a folder ‘/opt/program’ inside the container where we store the files:</p>
<ul>
  <li>serve: starts the server API</li>
  <li>predictor.py: defines Flask REST API</li>
</ul>

<p>When Sagemaker runs the container it starts the CMD “serve”, which deploys the REST API. The file</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>predictor.py
</code></pre></div></div>
<p>loads the pickled model and implements a Flask API with two methods that Sagemaker expects:</p>
<ul>
  <li>[GET] /ping</li>
  <li>[POST] /invocations</li>
</ul>

<p>The pickled model can be copied directly to the container to a folder of choice. Or it can be stored in a S3 bucket and passed on to Sagemaker as an artifact. Sagemaker then extracts the tar.gz file from S3 and copies it to the folder ‘/opt/ml/model’. Therefore, if we pass the model as an artifact, the predictor module needs to unpickle the file at ‘/opt/ml/model’.</p>

<p>The Dockerfile has the basic structure:</p>
<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> ubuntu:latest</span>

<span class="k">RUN </span>apt-get <span class="nt">-y</span> update <span class="o">&amp;&amp;</span> apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">--no-install-recommends</span> <span class="se">\
</span>         wget <span class="se">\
</span>         python3 <span class="se">\
</span>         python3-pip<span class="se">\
</span>         nginx <span class="se">\
</span>         ca-certificates <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>

<span class="c">#Install python libraries</span>
<span class="k">COPY</span><span class="s"> requirements.txt /opt/program/</span>
<span class="k">RUN </span>python3 <span class="nt">-m</span> pip <span class="nb">install</span> /opt/prorgam/requirements.txt <span class="o">&amp;&amp;</span> <span class="se">\
</span>        <span class="nb">rm</span> <span class="nt">-rf</span> /root/.cache

<span class="k">ENV</span><span class="s"> PYTHONUNBUFFERED=TRUE</span>
<span class="k">ENV</span><span class="s"> PYTHONDONTWRITEBYTECODE=TRUE</span>
<span class="k">ENV</span><span class="s"> PATH="/opt/program:${PATH}"</span>

<span class="c">#copy model to /opt/ml/model or other folder</span>
<span class="k">COPY</span><span class="s"> model.pkl /opt/ml/model/</span>
<span class="c"># Set up the program in the image</span>
<span class="k">COPY</span><span class="s"> model-files /opt/program</span>
<span class="k">WORKDIR</span><span class="s"> /opt/program</span>
<span class="k">RUN </span><span class="nb">chmod</span> +x serve

<span class="k">CMD</span><span class="s"> [ "serve" ]</span>
</code></pre></div></div>

<p>We can run the container locally and test the API:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#build model</span>
docker build <span class="nt">-t</span> sagemaker-model <span class="nb">.</span>
<span class="c">#run the container</span>
docker run <span class="nt">-p</span> 8080:8080 sagemaker-model:latest 
</code></pre></div></div>

<p>Now we can access the API at 127.0.0.1:8080:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--location</span> <span class="nt">--request</span> POST <span class="s1">'http://localhost:8080/invocations'</span> <span class="se">\</span>
<span class="nt">--header</span> <span class="s1">'Content-Type: application/json'</span> <span class="se">\</span>
<span class="nt">--data-raw</span> <span class="s1">'{"data": [[1,2],[3,4],[3,3],[10,1],[7,8]]}'</span>
</code></pre></div></div>

<h3 id="sagemaker-deployment">Sagemaker Deployment</h3>

<p>First we need to push our docker image to our AWS ECR repository. Assuming that we have already created a repository with URI: “aws_account_id”.dkr.ecr.”region”.amazonaws.com/”name-model”, we tag the docker image using the same repository URI, that is,</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker tag sagemaker-model:latest <span class="s2">"aws_account_id"</span>.dkr.ecr.<span class="s2">"region"</span>.amazonaws.com/sagemaker-model:latest
</code></pre></div></div>
<p>and then push to the ECR repository (it presupposes that one has logged in)</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker push <span class="s2">"aws_account_id"</span>.dkr.ecr.<span class="s2">"region"</span>.amazonaws.com/model-sagemaker:latest
</code></pre></div></div>

<p>Now that we have uploaded the docker image we can go to Sagemaker section and create a Model, an Endpoint Configuration and finaly deploy the model to an Endpoint.</p>

<h4 id="create-model"><strong>Create Model</strong></h4>
<p>We give it a name</p>
<div style="text-align: center"><img src="/blog-data-science/images/sagemaker_deployment_model_step.png" width="80%" /></div>
<p>then we choose to “Provide model artifacts and image location” since we want to use our container</p>
<div style="text-align: center"><img src="/blog-data-science/images/sagemaker_deployment_model_step2.png" width="80%" /></div>
<p>and last we choose “single model” and then write the URI of the docker image. Since our container already has the pickled model we do not need to write anything in the box “Location of model artifacts”</p>
<div style="text-align: center"><img src="/blog-data-science/images/sagemaker_deployment_model_step3.png" width="80%" /></div>

<h4 id="endpoint-configuration"><strong>Endpoint-Configuration</strong></h4>

<p>We give it a name and then choose the model that we have created in previous step. At this point we need to choose the EC2 instance that will run the container.</p>
<div style="text-align: center"><img src="/blog-data-science/images/sagemaker_deployement_endpntconfig.png" width="80%" /></div>

<h4 id="endpoint"><strong>Endpoint</strong></h4>

<p>Give a name to the endpoint and then choose an existing endpoint-configuration, the one we have previously created:</p>
<div style="text-align: center"><img src="/blog-data-science/images/sagemaker_deployment_endpoint.png" width="80%" /></div>
<p>Then choose “Create Endpoint”.</p>

<h3 id="access-endpoint">Access Endpoint</h3>

<p>Now that the model is deployed and the endpoint is in “Service”, we build an API to call the container endpoint. There are essentially two ways of doing this:</p>

<p>1) We can invoke the Sagemaker endpoint directly. For this we need to create a role with permission to invoke the sagemaker endpoint.</p>

<p>2) Create a REST API Gateway with a Lambda to call the Sagemaker Endpoint.</p>

<h4 id="1-invoke-sagemaker-directly">1. Invoke Sagemaker directly</h4>

<p>In this case the AWS user must have the permission to invoke the sagemaker endpoint. Then we need the credentials <strong>Access_Key_id</strong> and <strong>Secret_access_key</strong> of this user. In Postman the request looks like</p>
<div style="text-align: center"><img src="/blog-data-science/images/postman_access_endpoint.png" width="100%" /></div>]]></content><author><name>Joao Gomes</name></author><category term="Cloud-Computing" /><summary type="html"><![CDATA[Deploy custom model in AWS Sagemaker using containers.]]></summary></entry><entry><title type="html">Power Test</title><link href="http://localhost:4000/statistics/2021/10/09/powertest.html" rel="alternate" type="text/html" title="Power Test" /><published>2021-10-09T00:00:00+02:00</published><updated>2021-10-09T00:00:00+02:00</updated><id>http://localhost:4000/statistics/2021/10/09/powertest</id><content type="html" xml:base="http://localhost:4000/statistics/2021/10/09/powertest.html"><![CDATA[<h3 id="statistical-power">Statistical Power</h3>

<p>The power statistic is defined as the probability</p>

\[\text{power}=P(\text{reject }H_0|H_1\text{ True})\]

<p>where $H_0$ is the null hypothesis and $H_1$ is the alternative hypothesis.</p>

<p>We can model the t-statistics of both hypothesis using the Student’s t-distribution.</p>

<div style="text-align: center"><img src="/blog-data-science/images/powertest.png" width="80%" /></div>

<p>On the right is the distribution for the $H_1$ hypothesis while on the left we have the $H_0$ or null hypothesis. The area in red is the probability of rejecting the null hypothesis given that $H_0$ is true. This is the significance level that is usually is set to 5%. The area in blue is the probability of rejecting the null given that $H_1$ is true. If the distributions are far apart then the power approaches 1, while if they are close to each other the power is small.</p>

<p>Consider a statistical test for the difference of means of two samples with equal sizes $n_1=n_2=n$ and variance. The t-statistic is</p>

\[t=\frac{\bar{X}_1-\bar{X}_2}{s_p\sqrt{\frac{2}{n}}}\]

<p>where $s_p$ is the pooled variance:</p>

\[s_p^2=\frac{s_1^2+s_2^2}{2}\]

<p>and $\text{df}=2n-2$ are the number of degrees of freedom.</p>

<p>For large $n$ the Student t-distribution approaches a standard normal distribution. So we can calculate the power as</p>

\[\text{power}=\int_{t_{\alpha}}^{\infty}dt\frac{e^{-(t-t^*)^2/2}}{\sqrt{2\pi}}=1-\Phi(t_{\alpha}-t^*)\]

<p>Here $t_{\alpha}$ is the value for which the null hypothesis is rejected, and $t^*$ is the expected value when $H_1$ is true.</p>

<p>The value of power is usually set at 80%, which means that $\Phi(t_{\alpha}-t^*)=0.2$ or:</p>

\[t^*-t_{\alpha}\simeq 0.842\]

<p>while $t_{\alpha}\simeq 1.96$, which is the value for which $\Phi(t_{\alpha})=0.975$. Definining the effect size as:</p>

\[d=\frac{\bar{X}_1-\bar{X}_2}{s_p}\]

<p>we can calculate the sample size with</p>

\[n=2(1.96+0.842)^2/d^2\]]]></content><author><name>Joao Gomes</name></author><category term="Statistics" /><summary type="html"><![CDATA[The power statistics calculates the probability of rejecting the null hypothesis assuming that the alternative is true. This is used to estimate sample sizes for trial experiments.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/powertest.png" /><media:content medium="image" url="http://localhost:4000/powertest.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Kernel Methods</title><link href="http://localhost:4000/machine%20learning/2021/09/29/kernels.html" rel="alternate" type="text/html" title="Kernel Methods" /><published>2021-09-29T00:00:00+02:00</published><updated>2021-09-29T00:00:00+02:00</updated><id>http://localhost:4000/machine%20learning/2021/09/29/kernels</id><content type="html" xml:base="http://localhost:4000/machine%20learning/2021/09/29/kernels.html"><![CDATA[<h3 id="definition">Definition</h3>

<p>A kernel $K(x,x’)$ is a function that obeys the property</p>

\[K(x,x')=\langle\Phi(x)\cdot\Phi(x')\rangle\]

<p>where $\langle\cdot\rangle$ denotes inner product in some vector space $\mathbb{V}$ and $\Phi(x)$ is a mapping from $x\in \mathbb{R}^d$ to $\mathbb{V}$, known as feature map. Examples:</p>

<ul>
  <li>Any polynomial function of $\langle x\cdot x’\rangle$ is a kernel. This is because of the property that</li>
</ul>

\[\begin{equation*}\begin{split}
&amp;\langle x\cdot x'\rangle^p = \left(\sum_{i=1}^d x_i x'_i\right)^p =\sum_k C(k) x_i^k {x'}_i^k\\
&amp;=(C(1)^{1/2}x_1,C(2)^{1/2}x_1^2,\ldots, C(1)^{1/2}x_2,\ldots)^{T}(C(1)^{1/2}{x'}_1,C(2)^{1/2}{x'}_1^2,\ldots,C(1)^{1/2} {x'}_2,\ldots)
\end{split}\end{equation*}\]

<ul>
  <li>Using this, we can also show that the gaussian function is a kernel:</li>
</ul>

\[\begin{equation*}\begin{split}&amp;\exp{\left(-\gamma |x-x'|^2\right)}=\exp{\left(-\gamma x^2-\gamma {x'}^2-2\gamma \langle x\cdot x'\rangle\right)}=\\
&amp;=\exp{\left(-\gamma x^2-\gamma {x'}^2\right)} \sum_{n=1}^{\infty}\frac{(-2\langle x\cdot x'\rangle)^n}{n!}\end{split}\end{equation*}\]

<h3 id="regression">Regression</h3>

<p>In the KNN algorithm we take the K nearest neighbors of point $x$ and average their values. That is,</p>

\[\hat{y}|_x=\frac{1}{K}\sum_{i\in \text{K-neighbors(x)}} y_i|_x\]

<p>We can put it differently by considering probabilities $p(y_i|x)=\frac{1}{K}$ and attach them to the $K$ neighbors of point $x$. 
Then the above average becomes</p>

\[\hat{y}|_x=E(y|x)\]

<p>Rather than giving equal weights to the neighbors, we can give weights that decay with distance. This allows us to include contributions from very far without introducing additional bias. For example, using the gaussian function kernel we can write</p>

\[p(y_i,x)=\frac{1}{N}\exp{\left(-\frac{|x-x_i|^2}{d^2}\right)}\]

<p>where $N$ is the number of datapoints in the training set and $d$ is the Kernel width. It follows that</p>

\[p(y_i|x)=\frac{p(y_i,x)}{\sum_i p(y_i,x)}\]

<p>and</p>

\[E(y|x)=\frac{\sum_i y_ip(y_i,x)}{\sum_i p(y_i,x)}\]

<p>Note that for $d\rightarrow \infty$ all the data-points contribute equally and $p(y_i|x)\rightarrow\frac{1}{N}$. 
This is the limiting case of KNN algorithm when we include all the neighbors. We have seen that when the number of neighbors is large variance decreases but bias increases. However, for $d$ small, only the closest neighbors contribute. In this case we expecte variance to increase, but bias to be small.</p>

<p>As an example, we generate an artificial training dataset for the line:</p>

\[f(x)=x^2 e^{-0.05x}\]

<p>and fit a gaussian kernel.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="n">xtrain</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">eps</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y</span><span class="o">=</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ytrain</span><span class="o">=</span><span class="n">y</span><span class="o">+</span><span class="n">eps</span>

<span class="k">class</span> <span class="nc">KernelRg</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="o">=</span><span class="n">d</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="o">=</span><span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="o">=</span><span class="n">y</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mu</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">r</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">f</span><span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">r_</span><span class="o">=</span><span class="n">r</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">y</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">r_</span><span class="o">=</span><span class="n">r_</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">N</span><span class="o">=</span><span class="n">r</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">r_</span><span class="o">/</span><span class="n">N</span>
    
    <span class="k">def</span> <span class="nf">prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mu</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">r</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">f</span><span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span>
        <span class="n">p</span><span class="o">=</span><span class="n">r</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">p</span>
    
    <span class="k">def</span> <span class="nf">le</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">prob</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">L</span><span class="o">=-</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">L</span>
        
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">p</span>

<span class="n">K</span><span class="o">=</span><span class="n">KernelRg</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">K</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">)</span>

<span class="n">sample</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">ysample</span><span class="o">=</span><span class="n">K</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</code></pre></div></div>

<p>We plot a prediction for various widths $d$ including the log-likelihood defined as</p>

\[\text{Log-Likelihood}=-\sum_x \ln(\sum_i p(y_i,x))\]

<div style="text-align: center"><img src="/blog-data-science/images/kernel_reg_d.png" width="100%" /></div>

<p>For small $d$ we see a lot of variance and for larger values of $d$ the function changes very slowly. In fact, when $d$ is small the contributions from the nearest neighbors contribute more strongly and as such variance increases. But for large $d$ all the data-points start contributing equally which increases bias.</p>

<h3 id="density-estimation">Density Estimation</h3>

<p>One way to estimate a density distribution is to build a histogram. In a histogram we partition the feature space in buckets and then count how many data-points fall in those buckets. However, the histogram depends on a partition choice, so it is not unique, and does not provide a smooth, continuous distribution function.</p>

<p>A way to resolve these issues is by considering the empirical distribution. It can be represented as a sum of Dirac delta functions:</p>

\[f(x)=\frac{1}{N}\sum_{x_i} \delta(x-x_i)\]

<p>where $N$ is the number of datapoints $x_i$. In contrast with the histogram, this distribution is unique and can be used to calculate the probability for any $x$. For example, the probability of finding $x$ in the interval $[a,b]$ is given by</p>

\[\int_a^bf(x)dx=\frac{1}{N}\sum_{x_i} \int_a^b\delta(x-x_i)dx=\frac{\# x_i\in[a,b]}{N}\]

<p>Despite providing an accurate representation of the sample distribution, $f(x)$ is highly singular and cannot be used in practice. Instead we can consider an approximation, which is smooth, by using the identity:</p>

\[\delta(x-x')=\lim_{d\rightarrow 0}\frac{1}{\sqrt{2\pi}d}e^{-(x-x')^2/2d^2}\]

<p>We can approximate the sum of delta functions using a finite $d$, that is,</p>

\[f(x)\simeq \frac{1}{N}\sum_{x_i}\frac{1}{\sqrt{2\pi}d}e^{-(x-x_i)^2/2d^2}\]

<p>which reproduces exactly $f(x)$ in the limit $d\rightarrow 0$. Here $d$, the width, acts as a regulator of the singular behaviour of the Dirac delta function.</p>

<p>As an example, we sample a set of $x_i$ from the distribution:</p>

\[f(x)=\frac{1}{3\sqrt{2\pi 2}}e^{-x^2/4}+\frac{1}{3\sqrt{2\pi 6}}e^{-(x-7)^2/6}+\frac{1}{3\sqrt{2\pi 3}}e^{-(x-12)^2/3}\]

<div style="text-align: center"><img src="/blog-data-science/images/kernel_dens_1.png" width="70%" /></div>

<p>Then we fit the training set with a gaussian Kernel:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KernelDensity</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="o">=</span><span class="n">d</span>
    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">xtrain</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">xtrain</span><span class="o">=</span><span class="n">xtrain</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n</span><span class="o">=</span><span class="n">xtrain</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">mu</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">xtrain</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">r</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">f</span><span class="p">(</span><span class="n">z</span><span class="o">-</span><span class="n">mu</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">r</span><span class="o">/</span><span class="bp">self</span><span class="p">.</span><span class="n">n</span>
    
    <span class="k">def</span> <span class="nf">le</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">r</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">r</span><span class="o">=-</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">r</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">r</span>
    
    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">p</span>

<span class="n">K</span><span class="o">=</span><span class="n">KernelDensity</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">K</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">)</span>
</code></pre></div></div>

<p>This is the estimated density for various values of $d$ including the corresponding log-likelihood:</p>

<div style="text-align: center"><img src="/blog-data-science/images/kernel_dens_2.png" width="100%" /></div>

<h3 id="classification">Classification</h3>

<p>In classification, we have a set $(x_i,c_i)$ with $c_i=0,1,\ldots$ the labels. We are interested in estimating</p>

\[P(c|x)\]

<p>from the data. This can be written as</p>

\[P(c|x)=\frac{P(x|c)P(c)}{\sum_{c'} P(x|c')P(c')}\]

<p>So using the previous results on density estimation, we can calculate</p>

\[P(x|c)\]

<p>for each class using a kernel. The probability $P(c)$ is easily estimated using the maximum-likelihood principle, giving</p>

\[P(c)=\frac{\# (c_i=c)}{N}\]

<p>with $N$ the number of data-points.</p>

<p>Example in Python</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">data</span><span class="o">=</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span>
<span class="n">Y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>

</code></pre></div></div>
<p>We use StandardScaler to standardize the dataset:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ss</span><span class="o">=</span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_s</span><span class="o">=</span><span class="n">ss</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">xtrain</span><span class="p">,</span><span class="n">xtest</span><span class="p">,</span><span class="n">ytrain</span><span class="p">,</span><span class="n">ytest</span><span class="o">=</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X_s</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div>

<p>Then we define the Kernel model:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">KernelClf</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">d</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="o">=</span><span class="n">d</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">labels</span><span class="p">,</span> <span class="n">counts</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">N</span><span class="o">=</span><span class="n">counts</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pc</span><span class="o">=</span><span class="p">{</span><span class="n">l</span><span class="p">:</span> <span class="n">c</span><span class="o">/</span><span class="n">N</span> <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">counts</span><span class="p">)}</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">kernels</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
            <span class="n">id_c</span><span class="o">=</span> <span class="n">y</span><span class="o">==</span><span class="n">l</span>
            <span class="n">x_c</span><span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">id_c</span><span class="p">]</span>
            <span class="n">K</span><span class="o">=</span><span class="n">Kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">d</span><span class="p">)</span>
            <span class="n">K</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_c</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">kernels</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">=</span><span class="n">K</span>
    
    <span class="k">def</span> <span class="nf">predict_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">pv</span><span class="o">=</span><span class="p">[</span><span class="n">K</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">pc</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">K</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernels</span><span class="p">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="n">P</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">pv</span><span class="p">)</span>
        <span class="n">prob</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">/</span><span class="n">P</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pv</span><span class="p">]</span>
        <span class="n">prob</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">p</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prob</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">prob</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">pv</span><span class="o">=</span><span class="p">[</span><span class="n">K</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">pc</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">K</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernels</span><span class="p">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="n">P</span><span class="o">=</span><span class="nb">sum</span><span class="p">(</span><span class="n">pv</span><span class="p">)</span>
        <span class="n">prob</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="o">/</span><span class="n">P</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pv</span><span class="p">]</span>
        <span class="n">prob</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">p</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">prob</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">prob</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Train:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Kclf</span><span class="o">=</span><span class="n">KernelClf</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">Kclf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">)</span>

<span class="n">ypred</span><span class="o">=</span><span class="n">Kclf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>
<span class="n">acc</span><span class="o">=</span><span class="p">(</span><span class="n">ypred</span><span class="o">==</span><span class="n">ytest</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
<span class="mf">97.37</span><span class="o">%</span>
</code></pre></div></div>]]></content><author><name>Joao Gomes</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[We explore the use of Kernels in classification, regression and density estimation.]]></summary></entry><entry><title type="html">Clustering K-Means</title><link href="http://localhost:4000/machine%20learning/2021/09/24/clustering.html" rel="alternate" type="text/html" title="Clustering K-Means" /><published>2021-09-24T00:00:00+02:00</published><updated>2021-09-24T00:00:00+02:00</updated><id>http://localhost:4000/machine%20learning/2021/09/24/clustering</id><content type="html" xml:base="http://localhost:4000/machine%20learning/2021/09/24/clustering.html"><![CDATA[<h3 id="kmeans-algorithm">Kmeans algorithm</h3>

<p>Kmeans clustering is an unsupervised machine learning algorithm. Given a set of data-points and the number of clusters the algorithm assigns each point to a particular cluster. The algorithm works iteratively starting from N randomly assigned cluster positions, and subsequently changing the positions until convergence is achieved.</p>

<p>The algorithm follows the steps:</p>

<ol>
  <li>Choose random positions for the clusters;</li>
  <li>For each data-point determine the closest cluster;</li>
  <li>Calculate the center-of-mass for each group, which is now the new cluster center position;</li>
  <li>Loop through 2,3 until a certain degree of convergence is achieved.</li>
</ol>

<div style="text-align: center"><img src="/blog-data-science/images/kmeans.png" width="40%" /></div>

<p>In the picture above, $p1$ represents the initial cluster position and $p2$ is center-of-mass. The algorithm continues until the change in the cluster positions is within a certain margin of error, indicating that it has converged.</p>

<p>The problem can be formulated as follows:</p>
<ul>
  <li>Find cluster positions $c_1,c_2\ldots,c_N$ and labels $l$ such that we minimize</li>
</ul>

\[D=\sum_{l=1}^N \sum_{\substack{i=1\\ x\in \text{cluster}:l}}^{N_l}|x^i_l-c_l|^2\]

<p>Here, $N_l$ is the number of data-points in cluster $l$. The algorithm depends strongly on the initial positions and it is not guaranteed that it will achieve a global optimum. Step 2 of the algorithm consists in atributing labels $l$ for the data-points $x$ such that $D$ is minimized given the centers of the clusters $c_l$. In step 3, we minimize with respect to the center positions $c_l$, that is,</p>

\[\begin{equation}\begin{split}
&amp;\frac{\partial D}{\partial c_l}=\sum_{x\in \text{cluster: }l}(x_l-c_l)=0\\
&amp;\Leftrightarrow c_l=\frac{1}{N_l}\sum_{x\in \text{cluster: }l} x_l
\end{split}\end{equation}\]

<h3 id="statistical-point-of-view">Statistical point of view</h3>

<p>Consider the mixture gaussian model:</p>

\[\begin{equation}\begin{split}
&amp;P(x|c)=\frac{1}{\sigma_c\sqrt{2\pi}}\exp{-\frac{(x-x_c)^2}{2\sigma_c^2}} \\
&amp;P(c)=\frac{1}{N}
\end{split}\end{equation}\]

<p>The probability $P(x)$ is</p>

\[P(x)=\sum_c P(x|c)P(c)=\sum_c\frac{1}{N\sigma_c\sqrt{2\pi}}\exp{-\frac{|x-x_c|^2}{2\sigma_c^2}}\]

<p>We want to use maximum-likelihood estimation to determine the centers $x_c$. Therefore, we want to maximize the likelihood:</p>

\[L=\sum_{x^i}\ln P(x^i)\]

<p>This is can be hard to solve because $P(x)$ contains a sum over multiple terms. However, we can approximate $P(x^i)$ by the cluster $c(i)$ that is closer to $x^i$, that is,</p>

\[P(x^i)\simeq \frac{1}{N\sigma_c\sqrt{2\pi}}\exp{-\frac{|x^i-x_{c(i)}|^2}{2\sigma_c^2}}\]

<p>The approximation is valid provided there is a clear separation between the clusters, so the clusters different from $c(i)$ have exponentially suppressed contributions. That is, we need</p>

\[\frac{|x^i-x_{c(i)}|^2}{\sigma_{c(i)}}\ll \frac{|x^i-x_{c'}|^2}{\sigma_{c'}},\;c(i)\neq c'\]

<p>then the likelihood function is:</p>

\[L=\sum_{x^i}\ln P(x^i)\simeq -\frac{1}{2\sigma_{c(i)}^2}\sum_{x^i} |x^i-x_{c(i)}|^2\]

<p>Maximizing $L$ is equivalent to minimizing:</p>

\[\sum_{x^i} \frac{1}{\sigma_{c(i)}^2}|x^i-x_{c(i)}|^2=\sum_{c} \frac{1}{\sigma_{c(i)}^2}\sum_{x\in \text{cluster}}|x^i-x_{c(i)}|^2\]

<p>Provided all the clusters have the same variance $\sigma_c=\sigma$, we recover the kmeans algorithm.</p>
<h3 id="python-implementation">Python Implementation</h3>

<p>The Python code is:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Kmeans</span><span class="p">:</span>
    <span class="s">"""
    KMeans algorithm:
    * initialization: assigns random positions to clusters given mean
    and standard deviation of data
    """</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">n_clusters</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span>
        <span class="c1"># position centers for each iteration
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">centers_pos</span><span class="o">=</span><span class="p">[]</span>
        <span class="c1"># centers positions: clusters_centers[i]=(x,y) for center i
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">clusters_centers</span><span class="o">=</span><span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">std_dev</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1">#pick N random data-points
</span>        <span class="n">idx</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="bp">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="c1">#initialize center positions
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">clusters_centers</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">+</span><span class="n">std_dev</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">clusters</span><span class="o">=</span><span class="p">[]</span>
        
        <span class="n">not_converged</span><span class="o">=</span><span class="bp">True</span>
        <span class="k">while</span> <span class="n">not_converged</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">centers_pos</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">clusters_centers</span><span class="p">)</span>
            <span class="c1"># calculate new
</span>            <span class="n">new_centers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">newpos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="n">dev</span><span class="o">=</span><span class="n">new_centers</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">clusters_centers</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">clusters_centers</span><span class="o">=</span><span class="n">new_centers</span>
            <span class="n">dev</span><span class="o">=</span><span class="p">(</span><span class="n">dev</span><span class="o">*</span><span class="n">dev</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">dev</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">dev</span><span class="o">&gt;=</span><span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">not_converged</span><span class="o">=</span><span class="bp">False</span>
                <span class="k">print</span><span class="p">(</span><span class="s">'Converged'</span><span class="p">)</span>
    
    <span class="c1">#determine new means given clusters
</span>    <span class="k">def</span> <span class="nf">newpos</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">distances</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">x_cl</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">clusters_centers</span><span class="p">:</span>
            <span class="n">d</span><span class="o">=</span><span class="n">x</span><span class="o">-</span><span class="n">x_cl</span>
            <span class="n">d</span><span class="o">=</span><span class="p">(</span><span class="n">d</span><span class="o">*</span><span class="n">d</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">d</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="n">distances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">d</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">distances</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">clusters</span><span class="o">=</span><span class="n">distances</span><span class="p">.</span><span class="n">argmin</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="c1">#re-evaluate cluster centers
</span>        <span class="n">centers</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">clusters_centers</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">):</span>
            <span class="n">idx</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">clusters</span><span class="o">==</span><span class="n">i</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">idx</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">!=</span><span class="mi">0</span><span class="p">:</span>
                <span class="n">new_center</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">centers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">new_center</span>
        
        <span class="k">return</span> <span class="n">centers</span>
</code></pre></div></div>

<p>Then we generate data as:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span><span class="o">=</span><span class="mi">4</span> <span class="c1">#number of clusters
</span><span class="n">centers</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="p">,(</span><span class="n">N</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">xs</span><span class="o">=</span><span class="p">[]</span>
<span class="n">xcenters</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">100</span><span class="o">*</span><span class="n">N</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="k">for</span> <span class="n">c</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">centers</span><span class="p">,</span><span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)):</span>
    <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="n">c</span>
    <span class="n">x</span><span class="o">=</span><span class="n">c</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">100</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">xcenters</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="mi">100</span><span class="p">:</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),:]</span><span class="o">=</span><span class="n">c</span>
    <span class="n">xs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">xs_all</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</code></pre></div></div>
<p>that is,</p>

<div style="text-align: center"><img src="/blog-data-science/images/clusters.png" width="70%" /></div>

<p>To solve the problem instantiate the object and run fit method:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">km</span><span class="o">=</span><span class="n">Kmeans</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">km</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xs_all</span><span class="p">)</span>
</code></pre></div></div>

<div style="text-align: center"><img src="/blog-data-science/images/clusters_pred.png" width="70%" /></div>

<p>The dark crosses represent the solution of the k-means algorithm. We can keep track of the iterations:</p>

<div style="text-align: center"><img src="/blog-data-science/images/kmeans_iterations.png" width="70%" /></div>

<p>The larger circle represents the initial position, and subsequent smaller circles are the intermediate positions until convergence.</p>]]></content><author><name>Joao Gomes</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[Kmeans clustering is a learning algorithm whereby datapoints are clustered in an unsupervised manner.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/clusters_pred.png" /><media:content medium="image" url="http://localhost:4000/clusters_pred.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Bayesian Network</title><link href="http://localhost:4000/machine%20learning/2021/09/16/bayesian-net.html" rel="alternate" type="text/html" title="Bayesian Network" /><published>2021-09-16T00:00:00+02:00</published><updated>2021-09-16T00:00:00+02:00</updated><id>http://localhost:4000/machine%20learning/2021/09/16/bayesian-net</id><content type="html" xml:base="http://localhost:4000/machine%20learning/2021/09/16/bayesian-net.html"><![CDATA[<h3 id="dag">DAG</h3>

<p>A Bayesian network encodes a probabilistic model in a directed acyclic graph or DAG. For example, for a model with three variables A, B and C, whereby A and B are independent, we have</p>

\[P(C,A,B)=P(C|A,B)P(A)P(B)\]

<p>This can be represented with the graph:</p>

<div style="text-align: center"><img src="/blog-data-science/images/bn_dag.png" width="40%" /></div>

<p>Since A and B are independent variables, there is no arrow in between them. On the other hand, if we introduce an arrow in between A and B,</p>
<div style="text-align: center"><img src="/blog-data-science/images/bn_dag_2.png" width="40%" /></div>

<p>then the probabilistic model becomes</p>

\[P(C,A,B)=P(C|A,B)P(A|B)P(B)\]

<p>The rule is that the conditional probabilty at a node only depends on the parents, that is, the nodes which have arrows pointing towards that node. That is,</p>

\[P(x_1,x_2,\ldots,x_n)=\prod_i^n P(x_i|\text{pa}(x_i))\]

<p>where $\text{pa}(x_i)$ denotes the parents of $x_i$.</p>

<p>The network can be used to describe causality relationships because of the directionality of the graph edges. The idea of causality, however, can be confusing because the probability model is defined on a set, which obviously has no prefered direction. What this means is, for example, that we can write 
$P(A,B)$ as $P(A|B)P(B)$ or $P(B|A)P(A)$. Causality on the other hand pressuposes a prefered direction. For example, to model a fire ignited by a spark we write</p>

\[P(\text{fire}| \text{spark})\]

<p>that is the chance of starting a fire given that there has been a spark. This gives a very intuitive way of understanding the chain of events that lead to a fire. However, if we equivalently write the model using the reverse probability 
$P(\text{spark}|\text{fire})$, it is more difficult to make sense of the order of events. If we require that $P(\text{spark=True}|\text{fire=True})&lt;1$ then we need to ensure that $P(\text{fire=True}|\text{spark=False})&gt;0$, that is, that there can be a fire without a spark. In other words, we need to extend the space of events to include fires started by other reasons.</p>

<h3 id="structure-learning">Structure Learning</h3>

<p>An important question is to determine the graph that better explains the observed data. This requires exploring the space of possible graphs and therefore the name “structure learning”.</p>

<p>We motivate this by considering a simple problem. We take the same DAG as above with A and B independent and probabilities:</p>

\[\begin{equation}\begin{split}&amp;P(a=1)=0.2, \\
&amp;P(b=1)=0.5,\\
&amp;P(c=1,a=1,b=1)=0.7,\\
&amp;P(c=1,a=1,b=0)=0.8,\\
&amp;P(c=1,a=0,b=1)=0.4,\\
&amp;P(c=1,a=0,b=0)=0.6\end{split}\end{equation}\]

<p>and generate a dataset by random sampling:</p>
<div style="text-align: center"><img src="/blog-data-science/images/bn_data.png" width="30%" /></div>

<p>Now we can re-determine the various parameteres using maximum likelihood estimation. For each sample we calculate the corresponding probability and its logarithm. The total log-likelihood is the sum over all samples. That is,</p>

\[\begin{equation}\begin{split}&amp;\sum_{i,j,k}\log(P(a=i,b=j,c=k))\\
&amp;=\sum_i\log(P(a=i))+\sum_j\log(P(b=j))+\sum_{k|(i,j)}\log(P(c=k|a=i,b=j))\\
&amp;=N_{a=1}\log(p(a=1))+N_{a=0}\log(1-p(a=1))\\
&amp;+N_{b=1}\log(p(b=1))+N_{b=0}\log(1-p(b=1))\\
&amp;+N_{c=1|(1,1)}\log(p(c=1|1,1))+N_{c=0|(1,1)}\log(1-p(c=1|1,1))\\
&amp;+N_{c=1|(1,0)}\log(p(c=1|1,0))+N_{c=0|(1,0)}\log(1-p(c=1|1,0))\\
&amp;+N_{c=1|(0,1)}\log(p(c=1|0,1))+N_{c=0|(0,1)}\log(1-p(c=1|0,1))\\
&amp;+N_{c=1|(0,0)}\log(p(c=1|0,0))+N_{c=0|(0,0)}\log(1-p(c=1|0,0))\\
\end{split}\end{equation}\]

<p>Differentiating with respect to 
$p(a=1),p(b=1)$ and $p(c=1|i,j)$ we obtain</p>

\[\begin{equation}\begin{split}&amp;p(a=1)=\frac{N_{a=1}}{N_{a=0}+N_{a=1}}\\
&amp;p(b=1)=\frac{N_{b=1}}{N_{b=0}+N_{b=1}}\\
&amp;p(c=1|i,j)=\frac{N_{c=1|(i,j)}}{N_{c=0|(i,j)}+N_{c=1|(i,j)}}\end{split}\end{equation}\]

<p>The Python code calculates the probabilities and the Log-likelihood:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">L</span><span class="o">=</span><span class="mi">0</span> <span class="c1">#Log-likelihood
</span><span class="n">N</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Na</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">Nb</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'b'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">pa</span><span class="o">=</span><span class="n">Na</span><span class="o">/</span><span class="n">N</span>
<span class="n">pb</span><span class="o">=</span><span class="n">Nb</span><span class="o">/</span><span class="n">N</span>

<span class="n">L</span><span class="o">+=</span><span class="n">Na</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">pa</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">Na</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pa</span><span class="p">)</span>
<span class="n">L</span><span class="o">+=</span><span class="n">Nb</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">pa</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">Nb</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pb</span><span class="p">)</span>

<span class="n">pc</span><span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">Nc</span><span class="o">=</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'b'</span><span class="p">]</span><span class="o">==</span><span class="n">j</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'c'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">Nij</span><span class="o">=</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'b'</span><span class="p">]</span><span class="o">==</span><span class="n">j</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">p</span><span class="o">=</span><span class="n">Nc</span><span class="o">/</span><span class="n">Nij</span>
        <span class="n">pc</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)]</span><span class="o">=</span><span class="n">p</span>
        <span class="n">L</span><span class="o">+=</span><span class="n">Nc</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">Nij</span><span class="o">-</span><span class="n">Nc</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
<span class="n">L</span><span class="o">=-</span><span class="n">L</span><span class="o">/</span><span class="n">N</span>
</code></pre></div></div>

<p>from which we obtain</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pc</span><span class="p">:</span> <span class="p">{(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mf">0.6072338257768721</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="mf">0.3985257985257985</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span> <span class="mf">0.7852216748768472</span><span class="p">,</span>
    <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span> <span class="mf">0.7007077856420627</span><span class="p">}</span>

<span class="n">pa</span><span class="p">:</span> <span class="mf">0.2004</span>
<span class="n">pb</span><span class="p">:</span> <span class="mf">0.5059</span>
</code></pre></div></div>
<p>validating the initial model. The total Log-likelihood is calculated</p>

\[\begin{equation}\begin{split}L=&amp;-\frac{1}{N}\sum_{i,j,k}\log(P(a=i,b=j,c=k))\\
=&amp;2.31237\end{split}\end{equation}\]

<p>We consider in addition three different models with the following free parameters:</p>
<ul>
  <li>Model 2: $P(A),\,P(B)$ and $P(C)$</li>
  <li>Model 3: 
$P(A),\,P(B)$, and $P(C|B)$</li>
  <li>Model 4: 
$P(A),\,P(B),\,P(B|A),\,P(C|A,B)$</li>
</ul>

<p>For model 2, where A, B and C are all independent, the Log-likelihood is</p>

\[L=2.35073\]

<p>which is larger. 
For model 3, the free parameters are $P(a=1)$, $P(c=1|b=0,1)$ and $P(b=1)$. The log-likelihood is still larger:</p>

\[L=2.33311\]

<p>The model 4 is the most general graph which contains 7 parameters. In this case the log-likelihood is smaller:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">L</span><span class="o">=</span><span class="mi">0</span> <span class="c1">#Log-likelihood
</span><span class="n">N</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Na</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
<span class="n">pa</span><span class="o">=</span><span class="n">Na</span><span class="o">/</span><span class="n">N</span>

<span class="n">L</span><span class="o">+=</span><span class="n">Na</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">pa</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">Na</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">pa</span><span class="p">)</span>

<span class="n">pc</span><span class="o">=</span><span class="p">{}</span>
<span class="n">pb</span><span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="n">Nb</span><span class="o">=</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'b'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">Ni</span><span class="o">=</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">).</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">p</span><span class="o">=</span><span class="n">Nb</span><span class="o">/</span><span class="n">Ni</span>
    <span class="n">pb</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">p</span>
    <span class="n">L</span><span class="o">+=</span><span class="n">Nb</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">Ni</span><span class="o">-</span><span class="n">Nb</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">Nc</span><span class="o">=</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'b'</span><span class="p">]</span><span class="o">==</span><span class="n">j</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'c'</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">Nij</span><span class="o">=</span><span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="s">'a'</span><span class="p">]</span><span class="o">==</span><span class="n">i</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'b'</span><span class="p">]</span><span class="o">==</span><span class="n">j</span><span class="p">)).</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">p</span><span class="o">=</span><span class="n">Nc</span><span class="o">/</span><span class="n">Nij</span>
        <span class="n">pc</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">)]</span><span class="o">=</span><span class="n">p</span>
        <span class="n">L</span><span class="o">+=</span><span class="n">Nc</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">Nij</span><span class="o">-</span><span class="n">Nc</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
<span class="n">L</span><span class="o">=-</span><span class="n">L</span><span class="o">/</span><span class="n">N</span>
</code></pre></div></div>

\[L=1.84381\]

<p>However, when we inspect the probabilities 
$P(b=1|a)$ we find:</p>

\[\begin{equation}\begin{split}&amp;p(b=1|a=0)=0.49804\\
&amp;p(b=1|a=1)=0.49985
\end{split}\end{equation}\]

<p>which have almost the same value. In fact, we can check that the difference is not statistically significant, but only due to finite sample size. To do this, we generate permutation samples for the values ‘b’ and calculate
 $p(b=1|a=0)$ and $p(b=1|a=1)$. Then we determine the distribution of the difference $p(b=1|a=1)-p(b=1|a=0)$. The 95% probability interval is:</p>

\[[-0.00773, 0.00770]\]

<p>while the observed difference is $0.00181$, which is well inside that interval. So effectively, model 4 is statistically the same as the model 1.</p>

<h3 id="bnlearn">BNLearn</h3>

<p>BNLearn is a Python library for bayesian learning. We can perform structure learning very easily:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">bnlearn</span> <span class="k">as</span> <span class="n">bn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">bn</span><span class="p">.</span><span class="n">structure_learning</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">G</span> <span class="o">=</span> <span class="n">bn</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>
<div style="text-align: center"><img src="/blog-data-science/images/bnlearn_fit.png" width="75%" /></div>

<p>which is precisely the model that we have designed.</p>]]></content><author><name>Joao Gomes</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[Bayesian networks encode probabilistic models in directed acyclic graphs. A node represents a covariate and the edges encode the conditional probabilities. We describe bayesian networks, give examples and explain how to determine the graph given observed data- structure learning.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/bn_cover.png" /><media:content medium="image" url="http://localhost:4000/bn_cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deploying with Docker Containers</title><link href="http://localhost:4000/cloud-computing/2021/08/30/deploying-with-docker.html" rel="alternate" type="text/html" title="Deploying with Docker Containers" /><published>2021-08-30T00:00:00+02:00</published><updated>2021-08-30T00:00:00+02:00</updated><id>http://localhost:4000/cloud-computing/2021/08/30/deploying-with-docker</id><content type="html" xml:base="http://localhost:4000/cloud-computing/2021/08/30/deploying-with-docker.html"><![CDATA[<h3 id="dockerfile">Dockerfile</h3>

<p>The Dockerfile contains a set of instructions which are used to build a new Docker image. The starting point of these instructions is a base image. Then other commands follow like copying files, installing dependencies or running programs.</p>

<p>The purpose of a docker image is to run a self contained and lightweight version of an operating system- the Docker container. This makes it very useful to deploy machine learning models because we do not need to worry about the operating system of the host nor we need to setup the environment (like Python) every time we need to deploy a new model.</p>

<p>For example, the following Dockerfile deploys a machine learning model via a Flask application that we wrote in the file app.py:</p>
<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> python:3.8.2-slim</span>

<span class="c"># Copy function code</span>
<span class="k">COPY</span><span class="s"> app.py rf_model.pkl /app/</span>

<span class="k">COPY</span><span class="s"> requirements.txt  .</span>
<span class="k">RUN  </span>pip3 <span class="nb">install</span> <span class="nt">-r</span> requirements.txt <span class="nt">--target</span> /app/

<span class="c">#run api</span>
<span class="k">CMD</span><span class="s"> python3 /app/app.py</span>
</code></pre></div></div>
<p>The build starts from the base image “python:3.8.2-slim”, copies the app.py program and the saved machine learning model (random forest object) into the folder “/app/”, and installs necessary dependencies. When the image is run as a container the line with the “CMD” keyword is run by default- this is our machine learning API.</p>

<p>First we build this image with name “app”, and then run the container</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> app <span class="nb">.</span>
docker run <span class="nt">-p</span> 5001:5000 app
</code></pre></div></div>
<p>The flag “-p 5001:5000” forwards container port 5000 to port 5001 on the localhost. We can now call the API at localhost:5001:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">--location</span> <span class="nt">--request</span> POST <span class="s1">'http://127.0.0.1:5001/model/invoke'</span> 
<span class="nt">--header</span> <span class="s1">'Content-Type: application/json'</span> 
<span class="nt">--data-raw</span> <span class="s1">'{"data":[[6.49517312e-01, -1.63477913e+00,  1.02223807e+00, -2.90998418e-01,
        4.08584955e-01, -2.51346205e-01, -1.19300836e+00, -7.79194513e-02,
        1.89090598e-04,  1.43111208e+00, -1.58314852e+00,  1.67256137e+00,
       -2.12077154e+00]]}'</span>
</code></pre></div></div>
<p>From which we get the answer:</p>
<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>
    <span class="s2">"prediction"</span>: <span class="o">[</span>
        41.089999999999996
    <span class="o">]</span>
<span class="o">}</span>
</code></pre></div></div>]]></content><author><name>Joao Gomes</name></author><category term="Cloud-Computing" /><summary type="html"><![CDATA[I explain how to use Docker containers to deploy a machine learning application.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/container_cover.png" /><media:content medium="image" url="http://localhost:4000/container_cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deploy Machine Learning using Flask</title><link href="http://localhost:4000/cloud-computing/2021/08/20/deploy-machine-learning-using-flask.html" rel="alternate" type="text/html" title="Deploy Machine Learning using Flask" /><published>2021-08-20T09:20:00+02:00</published><updated>2021-08-20T09:20:00+02:00</updated><id>http://localhost:4000/cloud-computing/2021/08/20/deploy-machine-learning-using-flask</id><content type="html" xml:base="http://localhost:4000/cloud-computing/2021/08/20/deploy-machine-learning-using-flask.html"><![CDATA[<p>In this post I explain how to run a machine learning model on a cloud computer and access its predictions via an API.</p>

<p>To do this goal I write a simple API interface using Flask in Python. I have trained a linear regression model on artificial data and saved the model object using Pickle. The code below loads the model and runs an API that receives POST requests for the model predictions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">flask</span> <span class="kn">import</span> <span class="n">Flask</span><span class="p">,</span> <span class="n">jsonify</span><span class="p">,</span> <span class="n">abort</span><span class="p">,</span><span class="n">request</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">Flask</span><span class="p">(</span><span class="n">__name__</span><span class="p">)</span>

<span class="n">lr</span><span class="o">=</span><span class="n">pickle</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'lr_model.pkl'</span><span class="p">,</span><span class="s">'rb'</span><span class="p">))</span>

<span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">route</span><span class="p">(</span><span class="s">'/'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">index</span><span class="p">():</span>
    <span class="k">return</span> <span class="s">"Linear Regression model API."</span>

<span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">route</span><span class="p">(</span><span class="s">'/model/invoke'</span><span class="p">,</span><span class="n">methods</span><span class="o">=</span><span class="p">[</span><span class="s">'POST'</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">():</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span> <span class="ow">or</span> <span class="ow">not</span> <span class="s">'data'</span> <span class="ow">in</span> <span class="n">request</span><span class="p">.</span><span class="n">json</span><span class="p">:</span>
        <span class="n">abort</span><span class="p">(</span><span class="mi">400</span><span class="p">)</span>
    <span class="n">data</span><span class="o">=</span><span class="n">request</span><span class="p">.</span><span class="n">json</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span>
    <span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">pred</span><span class="o">=</span><span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">jsonify</span><span class="p">({</span><span class="s">'prediction'</span><span class="p">:</span> <span class="n">pred</span><span class="p">.</span><span class="n">tolist</span><span class="p">()}),</span> <span class="mi">201</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">app</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">debug</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">port</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span><span class="n">host</span><span class="o">=</span><span class="s">'0.0.0.0'</span><span class="p">)</span>

</code></pre></div></div>
<p>To test the api, run this script on a remote cloud instance (Linode offers up to 100 dollars to experiment their services). Then allow for incoming connections on port 5000, as set in the script (or any other port of your choice).  Send a POST request to the api using the IP address of the instance, for example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">curl</span> <span class="o">--</span><span class="n">location</span> <span class="o">--</span><span class="n">request</span> <span class="n">POST</span> <span class="s">'xxx.xxx.xxx.xxx:5000/model/invoke'</span> \
<span class="o">--</span><span class="n">header</span> <span class="s">'Content-Type: application/json'</span> \
<span class="o">--</span><span class="n">data</span><span class="o">-</span><span class="n">raw</span> <span class="s">'{"data":[2,3,4,5]}'</span>
</code></pre></div></div>
<p>On the remote, you can see a POST request from the Ip address of your local computer:
<img src="/blog-data-science/images/consoleapi.jpg" alt="" /></p>

<p>And this is the response from the api call:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span>
  <span class="s">"prediction"</span><span class="p">:</span> <span class="p">[</span>
    <span class="mf">4.354176603044118</span><span class="p">,</span> 
    <span class="mf">6.384373814367889</span><span class="p">,</span> 
    <span class="mf">8.414571025691659</span><span class="p">,</span> 
    <span class="mf">10.44476823701543</span>
  <span class="p">]</span>
<span class="p">}</span>

</code></pre></div></div>]]></content><author><name>Joao Gomes</name></author><category term="Cloud-Computing" /><summary type="html"><![CDATA[In this post I explain how to run a machine learning model on a cloud computer and access its predictions via an API.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/consoleapi.jpg" /><media:content medium="image" url="http://localhost:4000/consoleapi.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Convolutional Neural Network</title><link href="http://localhost:4000/machine%20learning/2021/01/25/convolutional-neural-network.html" rel="alternate" type="text/html" title="Convolutional Neural Network" /><published>2021-01-25T00:00:00+01:00</published><updated>2021-01-25T00:00:00+01:00</updated><id>http://localhost:4000/machine%20learning/2021/01/25/convolutional-neural-network</id><content type="html" xml:base="http://localhost:4000/machine%20learning/2021/01/25/convolutional-neural-network.html"><![CDATA[<ol>
  <li><a href="#def1">Convolutional Neural Network</a></li>
  <li><a href="#python">Python implementation</a></li>
</ol>

<p><a name="def1"></a></p>
<h3 id="1-convolutional-neural-network"><strong>1. Convolutional neural network</strong></h3>

<p>A convolutional neural network incorporates geometrical features in the learning algorithm. The neural network uses the convolution operation to capture the spatial correlation between data at different positions. This type of architecture is most suitable for vision tasks like image classification. The convolution amounts to a linear operation over smaller regions of the image, much like a moving average, except that this is not normalized. More explicitly, for a matrix of pixels $m_{ij}$ with size $N\times N$, and a kernel $K$ with weights $w_{\mu\nu}$ and size $k\times k$ we calculate</p>

\[K(m_{ij})=\sum_{\mu,\nu=0}^{k-1} w_{\mu\nu}m_{i+\mu,j+\nu}+b\]

<p>where $b$ is a bias. The convolution operation is depicted in the picture below for a kernel of size $4\times4$.</p>

<div style="text-align: center"><img src="/images/conv_img.png" width="50%" /></div>

<p>The convolution runs over the pixels where the kernel is allowed to be inside the image. That is, we have $m_{i+\mu,j+\nu}=m_{i’j’}$. We can relax this condition and allow for a padding $P$ that we add at the beginning and end of the image. Then we ensure that all pixels living in the padding regions are zero, that is, $m_{ij}=0$ for $i,j\in [-P,0[\,\cup\, ]N-1,N-1+P]$. One can also define a stride $S$ that determines the pixels the kernel runs over, that is, $(i,j)=(-P\text{ mod}(S),-P \text{ mod(S)})$. Therefore, if the pixel matrix has size $N\times N$ then the output of the convolution has shape $N’\times N’$ with</p>

\[N'=(N+2P-k+1)//S+1\]

<p>After the convolution operation, we apply a non-linear activation function on each element of the matrix \(m'_{i'j'}\). The result is the output of a convolutional layer.  Besides, one can create channels whereby we apply multiple kernels to the same input. So if the kernel \(K^c\) has C channels, the convolutional layer’s output is $C$ matrices \(m^c_{i'j'}\). Similarly, for each matrix \(m^c_{i'j'}\) one can further apply a kernel with \(C'\) channels. The result of using first the kernel \(K^c\), and then \(K^{c'}\) is \(C\times C'\) matrices.</p>

<p>After the convolutional layers, the resulting matrix is flattened and added as an input to a feed-forward neural network. Below we show this series of operations.</p>

<div style="text-align: center"><img src="/images/conv_layers.png" width="50%" /></div>

<p><a name="python"></a></p>
<h3 id="2-python-implementation"><strong>2. Python implementation</strong></h3>

<p>Using the <strong>grad_lib</strong> library that we have built in the previous post, we can build neural networks more easily. First we define a convolutional neural network</p>

<h3 id="convolutional-neural-network-1-channel">Convolutional Neural Network (1 channel)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">grad_lib</span> <span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">FeedForward</span><span class="p">,</span> <span class="n">Softmax</span><span class="p">,</span> <span class="n">LinearLayer</span><span class="p">,</span> <span class="n">Log</span><span class="p">,</span> <span class="n">DataSet</span> 

<span class="k">class</span> <span class="nc">ConvNet2D</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span><span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="s">"""Convolutional Layer with 2D kernel

        Args:
            kernel_size ([type]): [description]
            img_size (tuple, optional): [description]. Defaults to (8,8).
            stride (int, optional): [description]. Defaults to 1.
            padding (int, optional): [description]. Defaults to 0.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="o">=</span><span class="n">stride</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="o">=</span><span class="n">padding</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span><span class="n">kernel_size</span><span class="p">)</span>

        <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">init_param</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">get_out_dim</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="s">"""trainable Tensors
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">trainable</span><span class="o">=</span><span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="p">,</span>
                        <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
                        <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""forward

        Args:
            x (Tensor): (batch,S,S)

        Returns:
            Tensor: (batch,num_neurons)
        """</span>
        <span class="n">x_tensor</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel</span><span class="o">*</span><span class="n">x_tensor</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 

    <span class="k">def</span> <span class="nf">init_param</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weight</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">)</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> 

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""transform batch of images

        Args:
            x (numpy.array): (batch,S,S)

        Returns:
            Tensor: (kernel_size**2,batch,num_neurons)
        """</span>
        <span class="c1"># x: array
</span>        <span class="n">i_f</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">j_f</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">pad</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">out_list</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">i_f</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">j_f</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">stride</span><span class="p">):</span>
                    <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">kernel_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]].</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">out</span><span class="p">,</span><span class="n">z</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">out_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
        
        <span class="c1">#out_list: [batch,num_neurons,kernel_size**2]
</span>        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">out_list</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_out_dim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">test</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="p">.</span><span class="n">img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test</span><span class="p">).</span><span class="n">shape</span>

        <span class="k">return</span> <span class="n">size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div>

<h3 id="convolutional-neural-network-model-for-multi-class-problem">Convolutional neural network model for multi-class problem</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ConvClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">img_size</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="s">"""Convolutional Neural Network Classifier

        Args:
            img_size (tuple): (width,height)
            hidden_dim (int): number of hidden neurons
            out_dim (int, optional): number of class. Defaults to 1.
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">convnet</span><span class="o">=</span><span class="n">ConvNet2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">img_size</span><span class="o">=</span><span class="n">img_size</span><span class="p">)</span>
        <span class="n">in_dim</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">convnet</span><span class="p">.</span><span class="n">out_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">softmax</span><span class="o">=</span><span class="n">Softmax</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""[summary]

        Args:
            x (numpy.array): input must be array, not Tensor

        Returns:
            probability: (batch,out_dim)
        """</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">prob</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">prob</span>
        
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">True</span>
    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">eval</span><span class="p">():</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">False</span> 

</code></pre></div></div>

<h3 id="cross-entropy-loss-and-optimizer">Cross-Entropy loss and Optimizer</h3>
<p>We also need to write the cross-entropy loss function and optimizer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="o">=</span><span class="n">Log</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">prob</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="s">"""loss function

        Args:
            prob (probability Tensor): (batch,num_classes)
            y (array): (batch,1)
        """</span>
        <span class="n">bsz</span><span class="o">=</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">prob_</span><span class="o">=</span><span class="n">prob</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">bsz</span><span class="p">),</span><span class="n">y</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]]</span>
        <span class="n">loss</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob_</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">bsz</span><span class="p">)</span><span class="o">*</span><span class="n">loss</span> 

        <span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span><span class="o">=</span><span class="n">loss</span><span class="p">.</span><span class="n">grad</span>

        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">array</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span>

<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">obj</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">__dict__</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span><span class="s">'trainable'</span><span class="p">):</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">obj</span><span class="p">.</span><span class="n">trainable</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">tensor</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">=</span><span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="s">'none'</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span> 

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">:</span>
                    <span class="n">tensor</span><span class="p">.</span><span class="n">array</span><span class="o">-=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'No grads!'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="training-function">Training function</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">L</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">total_loss</span><span class="o">=</span><span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="n">batch_x</span><span class="p">,</span><span class="n">batch_y</span><span class="o">=</span><span class="n">batch</span>
            <span class="n">bsz</span><span class="o">=</span><span class="n">batch_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">pred</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">batch_x</span><span class="p">)</span>
            <span class="n">total_loss</span><span class="o">+=</span><span class="n">loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="n">batch_y</span><span class="p">.</span><span class="n">array</span><span class="p">)</span><span class="o">*</span><span class="n">bsz</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">print</span><span class="p">(</span><span class="s">'Epoch: '</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span><span class="s">" Loss: "</span><span class="p">,</span><span class="n">total_loss</span><span class="o">/</span><span class="n">L</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="example">Example</h2>

<p>Use the $8\times8$ version of the MNIST dataset as a toy-model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>

<span class="n">data</span><span class="o">=</span><span class="n">load_digits</span><span class="p">()</span>
<span class="n">imgs</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span>
<span class="n">target</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>

<span class="c1">#normalize the pixels for easier training
</span><span class="n">img_norm</span><span class="o">=</span><span class="n">imgs</span><span class="o">/</span><span class="mi">16</span>

<span class="c1">#create iterator
</span><span class="n">data_loader</span><span class="o">=</span><span class="n">DataSet</span><span class="p">(</span><span class="n">img_norm</span><span class="p">,</span><span class="n">target</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">model</span><span class="o">=</span><span class="n">ConvClassifier</span><span class="p">(</span><span class="n">img_size</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">8</span><span class="p">),</span><span class="n">hidden_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">loss</span><span class="o">=</span><span class="n">CrossEntropyLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name>Joao Gomes</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[A convolutional neural network can perform vision tasks such as image classification. The convolution operation creates features that carry information about the spatial distribution of the data. We implement a convolutional neural network from scratch using tensors in python.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/conv_layers.png" /><media:content medium="image" url="http://localhost:4000/conv_layers.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Gradients Automation</title><link href="http://localhost:4000/machine%20learning/2021/01/15/grads_lib.html" rel="alternate" type="text/html" title="Gradients Automation" /><published>2021-01-15T00:00:00+01:00</published><updated>2021-01-15T00:00:00+01:00</updated><id>http://localhost:4000/machine%20learning/2021/01/15/grads_lib</id><content type="html" xml:base="http://localhost:4000/machine%20learning/2021/01/15/grads_lib.html"><![CDATA[<ul>
  <li><a href="#1-gradients-and-tensors"><strong>1. Gradients and tensors</strong></a></li>
  <li><a href="#2-python-implementation"><strong>2. Python Implementation</strong></a></li>
</ul>

<p><a name="def1"></a></p>
<h3 id="1-gradients-and-tensors"><strong>1. Gradients and tensors</strong></h3>

<p>Neural networks can get very complicated very quickly. There are several types of architectures, and each can have multiple layers. Coding the gradients becomes a complicated task. Instead, libraries such as Pytorch or TensorFlow use a smart mechanism that allows to calculate gradients without having to code them. Essentially, these libraries keep track of all the mathematical operations on a tensor object and use the chain rule to determine the derivatives.</p>

<p>So how to calculate derivatives of tensors? Consider the scalar
\(\Phi=\exp\Big(\sum_{ij} T_{ij}w_iw_j\Big)\)
and its derivative relative to the tensor $w_i$. First, we calculate the derivative of $\Phi(u)$ with respect to $u$</p>

<p>\(\frac{\partial\Phi}{\partial u}=\exp(u)\)
and then use the chain rule together with the derivatives</p>

\[\frac{\partial u}{\partial w_k}=\sum_{ij}T_{ij}\frac{\partial w_i}{\partial w_k}w_j+\sum_{ij}T_{ij} w_i\frac{\partial w_j}{\partial w_k}+\sum_{ij}\frac{\partial T_{ij}}{\partial w_k}w_iw_j\]

<p>But we can do this calculation differently. Instead of starting from the function $\Phi(u)$ and propagate back the derivatives, we can keep track of the results at each step of the forward operation. That is, we perform the calculation in the following order</p>

<ol>
  <li>Calculate the derivatives $\frac{\partial w_i}{\partial w_k}$ and $\frac{\partial T_{ij}}{\partial w_k}$</li>
  <li>Determine $X_i\equiv\sum_{ij}T_{ij}w_j$ and $\frac{\partial X_i}{\partial w_k}=\sum_{ij}\frac{\partial T_{ij}}{\partial w_k}w_i + \sum_{ij}T_{ij}\frac{\partial w_i}{\partial w_k}$</li>
  <li>Determine $Y\equiv\sum_i X_iw_i$ and $\frac{\partial Y}{\partial w_k}=\sum_{i}\frac{\partial X_{i}}{\partial w_k}w_i + \sum_{i}X_{i}\frac{\partial w_i}{\partial w_k}$</li>
  <li>Finally calculate $\Phi=\exp(Y)$ and $\frac{\partial \Phi}{\partial w_k}=\Phi \frac{\partial Y}{\partial w_k}$</li>
</ol>

<p>At each step, we calculate the resulting tensor and the corresponding derivative. To accomplish this, we need an object (class) that implements various mathematical operations and keeps track of the gradients while the function takes place. In pseudo-code</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">array</span><span class="p">,</span><span class="n">gradient</span><span class="p">)</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="n">add_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="n">gradient</span><span class="o">=</span><span class="n">grad_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">gradient</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">):</span>
        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="n">mul_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="n">gradient</span><span class="o">=</span><span class="n">grad_operation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">tensor</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">array</span>
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">gradient</span><span class="p">)</span>

</code></pre></div></div>
<p>We should also define non-linear functions, such as the sigmoid activation function</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
</code></pre></div></div>

<p>As an example, say we want to calculate the scalar
\(\phi=\sigma\Big(\sum_ix_i\omega_i\Big)\)
where $\sigma(z)$ is the sigmoid function, and we are interested in the derivative relative to $\omega_i$. First we create the Tensor objects,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w_tensor</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">w_array</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">x_tensor</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">x_array</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>
<p>where w_tensor and x_tensor have dimensions $(d,1)$ and $(1,d)$ respectively. The flag “requires_grad” specifies whether we want or not the derivative relative to that tensor. We instantiate the function Sigmoid</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sig</span><span class="o">=</span><span class="n">Sigmoid</span><span class="p">()</span>
</code></pre></div></div>
<p>and calculate</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">phi</span><span class="o">=</span><span class="n">sig</span><span class="p">(</span><span class="n">x_tensor</span><span class="o">*</span><span class="n">w_tensor</span><span class="p">)</span>
</code></pre></div></div>
<p>The operator * is overloaded in the class Tensor by the matrix multiplication operation.
The object “phi” is an instance of the class Tensor, which contains both the result of the mathematical operation and the gradient of “phi” with respect to $\omega_i$. Then we can access the gradient by the attribute “phi.grad”.</p>

<p><a name="python"></a></p>
<h3 id="2-python-implementation"><strong>2. Python Implementation</strong></h3>

<p>The Tensor class contains linear mathematical operations together with other methods such as transposing or squeezing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Tensor</span><span class="p">:</span>
    <span class="n">calc_grad</span><span class="o">=</span><span class="bp">True</span> 

    <span class="s">"""calc_grad: bool, signaling whether to carry the gradients while artihmetic operations take place"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">array</span><span class="p">,</span>
                <span class="n">grad</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="s">"""
        array: numpy array
        grad: dic={id(object): numpy.array}
        requires_grad: bool, signaling whether to calculate or not the derivative relative to this tensor
        
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">=</span><span class="n">array</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span>
        
        <span class="k">if</span> <span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">name</span><span class="o">=</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> 
            <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">make_grad</span><span class="p">()}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="s">'none'</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
        <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span>
    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">ndim</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">T</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">T</span>

    <span class="k">def</span> <span class="nf">transpose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">shape</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">transpose</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">squeeze</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">squeeze</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">index</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">][</span><span class="n">index</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
        
        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">make_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">Kron</span><span class="o">=</span><span class="mi">1</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">shape</span><span class="p">:</span>
            <span class="n">ID</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
            <span class="n">Kron</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">Kron</span><span class="p">,</span><span class="n">ID</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">new_shape</span><span class="o">=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">new_shape</span><span class="o">+=</span><span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">Kron</span><span class="o">=</span><span class="n">Kron</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">new_shape</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">Kron</span>

    <span class="k">def</span> <span class="nf">check_grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__add__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">+</span><span class="n">x</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">+</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">+</span><span class="n">x</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    
    <span class="k">def</span> <span class="nf">__radd__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">+</span><span class="n">x</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__sub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>

                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="o">-</span><span class="n">x</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">x</span><span class="o">-</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__mul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="bp">self</span><span class="p">.</span><span class="n">check_grads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">grad1</span><span class="o">=</span><span class="mi">0</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">grad1</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        
                    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">grad2</span><span class="o">=</span><span class="mi">0</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">i</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
                        <span class="n">grad2</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">],</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">n1</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                        <span class="n">n2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">ndim</span>
                        <span class="n">n3</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">ndim</span>
                        <span class="n">r1</span><span class="o">=</span><span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n2</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span><span class="o">+</span><span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n1</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n1</span><span class="o">+</span><span class="n">n3</span><span class="o">-</span><span class="mi">2</span><span class="p">)]</span>
                        <span class="n">r2</span><span class="o">=</span><span class="p">[</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n2</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n1</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
                        <span class="n">grad2</span><span class="o">=</span><span class="n">grad2</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">r1</span><span class="o">+</span><span class="n">r2</span><span class="p">)</span>
                    
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad1</span><span class="o">+</span><span class="n">grad2</span>

                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__rmul__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="nb">float</span><span class="p">):</span>
            <span class="n">result</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__neg__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=-</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">axis</span><span class="p">):</span>
        <span class="n">result</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s">'Tensor(</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="si">}</span><span class="s">,dtype </span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">,requires_grad=</span><span class="si">{</span><span class="bp">self</span><span class="p">.</span><span class="n">requires_grad</span><span class="si">}</span><span class="s">)'</span>
</code></pre></div></div>
<p>Non-linear functions (some examples) can be defined as:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
    <span class="s">"""
    returns: Tensor with gradients
    """</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>

        <span class="n">u</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">u</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                    <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                    <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad_func</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
        <span class="n">den</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">u</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">u</span><span class="p">)</span>
        <span class="n">gd</span><span class="o">=</span><span class="n">u</span><span class="o">/</span><span class="n">den</span>

        <span class="k">return</span> <span class="n">gd</span>

<span class="k">class</span> <span class="nc">Log</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>

        <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                <span class="n">grad_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad_func</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

        <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">gd</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">x</span><span class="p">.</span><span class="n">array</span>

        <span class="k">return</span> <span class="n">gd</span>

<span class="k">class</span> <span class="nc">ReLU</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">sign</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">z</span><span class="p">[</span><span class="n">sign</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

        <span class="k">if</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                    <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                    <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">sign</span><span class="p">)</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">grad_func</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">sign</span><span class="p">):</span>
        <span class="n">z</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">z</span><span class="p">[</span><span class="n">sign</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
        <span class="n">z</span><span class="p">[</span><span class="o">~</span><span class="n">sign</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>

        <span class="k">return</span> <span class="n">z</span>

<span class="k">class</span> <span class="nc">Softmax</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""calculate grads after softmaz operation

        Args:
            x (Tensor): shape=(batch,num_classes)

        Returns:
            Tensor: contains gradients relative to softmax function
        """</span>
    
        <span class="n">prob</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">Z</span><span class="o">=</span><span class="n">prob</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">prob</span><span class="o">=</span><span class="n">prob</span><span class="o">/</span><span class="n">Z</span>

        <span class="k">if</span> <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
            <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">i</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span>
                    <span class="n">l</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">].</span><span class="n">ndim</span>
                    <span class="n">expand</span><span class="o">=</span><span class="nb">tuple</span><span class="p">([</span><span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">l</span><span class="p">)])</span>
                    <span class="n">grad_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="n">expand</span><span class="p">)</span>
                    <span class="n">dp</span><span class="o">=</span><span class="n">grad_func</span><span class="o">*</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="n">dp</span><span class="o">-</span><span class="n">grad_func</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">dp</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>

            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">Tensor</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="s">'NA'</span><span class="p">)</span>
</code></pre></div></div>

<p>With these definitions it is now easy to build a simple feed forward neural network, without the trouble of coding the gradients explicitly.</p>

<p>Here we show an example. First we define a LinearLayer class:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">in_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">in_dim</span><span class="o">=</span><span class="n">in_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span>

        <span class="n">weight_</span><span class="p">,</span><span class="n">bias_</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">init_params</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">weight_</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">bias_</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">trainable</span><span class="o">=</span><span class="p">{</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                        <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">):</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">}</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        x: Tensor [batch,in_dim]
        """</span>
        <span class="n">out</span><span class="o">=</span><span class="n">x</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">out</span>

    <span class="k">def</span> <span class="nf">init_params</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">weight</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="bp">self</span><span class="p">.</span><span class="n">in_dim</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="p">))</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span>
</code></pre></div></div>
<p>and the Feed Forward model is obtained by superimposing linear layers,</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FeedForward</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">n_hid_layers</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">train</span><span class="p">()</span> 
        <span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hid_layers</span><span class="o">=</span><span class="p">[</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_hid_layers</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="o">=</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sig</span><span class="o">=</span><span class="n">Sigmoid</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        assume two class problem
        """</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">hid_layers</span><span class="p">:</span>
            <span class="n">out</span><span class="o">=</span><span class="n">layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sig</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="s">"""
        predict
        """</span>
        <span class="c1">#set model to eval mode so we dont need to calculate the derivatives
</span>        <span class="bp">self</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="n">pred</span><span class="o">=</span><span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">pred</span><span class="o">=</span><span class="n">pred</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="p">(</span><span class="n">pred</span><span class="p">.</span><span class="n">array</span><span class="o">&gt;=</span><span class="mf">0.5</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'int8'</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">y_pred</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">True</span>
    
    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">Tensor</span><span class="p">.</span><span class="n">calc_grad</span><span class="o">=</span><span class="bp">False</span> 
</code></pre></div></div>

<p>For a two-class problem we define the loss function and also the optimizer</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LogLoss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="o">=</span><span class="n">Log</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">prob</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="s">"""
        prob: Tensor
        y: Tensor
        """</span>
        <span class="n">not_y</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">.</span><span class="n">array</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="n">T</span>
        <span class="n">not_y</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">not_y</span><span class="p">)</span>
        <span class="n">y_</span><span class="o">=</span><span class="n">y</span><span class="p">.</span><span class="n">array</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">).</span><span class="n">T</span>
        <span class="n">y_</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>

        <span class="n">not_prob</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="n">prob</span><span class="p">.</span><span class="n">array</span>
        <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">prob</span><span class="p">.</span><span class="n">grad</span><span class="p">:</span>
            <span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">=-</span><span class="n">prob</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">w</span><span class="p">]</span>
        <span class="n">not_prob</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="n">not_prob</span><span class="p">,</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span><span class="p">)</span>

        <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">prob</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">L</span><span class="o">=</span><span class="n">y_</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">)</span><span class="o">+</span><span class="n">not_y</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">not_prob</span><span class="p">)</span>
        <span class="n">L</span><span class="o">=-</span><span class="n">L</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">L</span><span class="o">=</span><span class="n">size</span><span class="o">*</span><span class="n">L</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span><span class="o">=</span><span class="n">L</span><span class="p">.</span><span class="n">grad</span>

        <span class="k">return</span> <span class="n">L</span><span class="p">.</span><span class="n">array</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">back_grads</span>

<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">find_tensor</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">tensor</span><span class="p">.</span><span class="n">requires_grad</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{}</span>
                <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">=</span><span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">=</span><span class="p">{</span><span class="s">'none'</span><span class="p">:</span><span class="mi">0</span><span class="p">}</span>
                <span class="n">tensor</span><span class="p">.</span><span class="n">grad</span><span class="o">=</span><span class="n">grad</span> 

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">tensors</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">:</span>
                    <span class="n">tensor</span><span class="p">.</span><span class="n">array</span><span class="o">-=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'No grads!'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">find_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">tensors</span><span class="o">=</span><span class="p">{}</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">param1</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">__dict__</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param1</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">tensors</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">param1</span><span class="p">)]</span><span class="o">=</span><span class="n">param1</span>
            <span class="k">elif</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">param1</span><span class="p">,</span><span class="s">'__dict__'</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_</span><span class="p">,</span><span class="n">param2</span> <span class="ow">in</span> <span class="n">param1</span><span class="p">.</span><span class="n">__dict__</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param2</span><span class="p">,</span><span class="n">Tensor</span><span class="p">):</span>
                        <span class="n">tensors</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">param2</span><span class="p">)]</span><span class="o">=</span><span class="n">param2</span>
        <span class="k">return</span> <span class="n">tensors</span>

</code></pre></div></div>
<p>For mini-batch gradient descent we use the data loader</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DataSet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">28</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="o">=</span><span class="n">x</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data_y</span><span class="o">=</span><span class="n">y</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bsz</span><span class="o">=</span><span class="n">batch_size</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
    <span class="k">def</span> <span class="nf">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">L</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">bsz</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">bsz</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">bsz</span><span class="p">):</span>
            <span class="n">batch_x</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_x</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">bsz</span><span class="p">])</span>
            <span class="n">batch_y</span><span class="o">=</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data_y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">bsz</span><span class="p">])</span>
            <span class="k">yield</span> <span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span>
</code></pre></div></div>

<p>Now, we are ready to train a one-hidden layer model. As an example, we load the breast_cancer dataset from sklearn api.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span> 

<span class="n">data</span><span class="o">=</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'data'</span><span class="p">]</span>
<span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'target'</span><span class="p">]</span>
<span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="o">/</span><span class="n">x</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>

<span class="n">data_loader</span><span class="o">=</span><span class="n">DataSet</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="mi">128</span><span class="p">)</span>
</code></pre></div></div>

<p>Define model, loss function and optimizer</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">model</span><span class="o">=</span><span class="n">FeedForward</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">loss</span><span class="o">=</span><span class="n">LogLoss</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">opt</span><span class="o">=</span><span class="n">Optimizer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
</code></pre></div></div>

<p>Perform training</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="n">epochs</span><span class="p">):</span>
        
    <span class="n">L</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">total_loss</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
            <span class="n">x_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="o">=</span><span class="n">batch</span>
            <span class="n">bsz</span><span class="o">=</span><span class="n">x_batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">out</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">x_batch</span><span class="p">)</span>
            <span class="n">total_loss</span><span class="o">+=</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span><span class="n">y_batch</span><span class="p">)</span><span class="o">*</span><span class="n">bsz</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">opt</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">epoch</span><span class="o">%</span><span class="mi">10</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Loss: '</span><span class="p">,</span><span class="n">total_loss</span><span class="o">/</span><span class="n">L</span><span class="p">)</span>
    
<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">opt</span><span class="p">,</span><span class="n">data_loader</span><span class="p">,</span><span class="mi">20</span><span class="p">)</span>

</code></pre></div></div>]]></content><author><name>Joao Gomes</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[We describe how to automate the calculation of gradients using tensors in python. This is very useful when building neural networks. We build a class that calculates the gradients without having to code them explicitly. This library is a toy-model that mimics professional libraries like the Pytorch or TensorFlow.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/tensor_python.png" /><media:content medium="image" url="http://localhost:4000/tensor_python.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Recurrent Neural Network</title><link href="http://localhost:4000/machine%20learning/2021/01/02/rnn.html" rel="alternate" type="text/html" title="Recurrent Neural Network" /><published>2021-01-02T00:00:00+01:00</published><updated>2021-01-02T00:00:00+01:00</updated><id>http://localhost:4000/machine%20learning/2021/01/02/rnn</id><content type="html" xml:base="http://localhost:4000/machine%20learning/2021/01/02/rnn.html"><![CDATA[<ul>
  <li><a href="#1-rnn-architecture"><strong>1. RNN architecture</strong></a></li>
  <li><a href="#2-training"><strong>2. Training</strong></a></li>
  <li><a href="#3-python-implementation"><strong>3. Python Implementation</strong></a></li>
</ul>

<p><a name="rnn"></a></p>
<h3 id="1-rnn-architecture"><strong>1. RNN architecture</strong></h3>

<p>A recurrent neural network (RNN) learns sequential data. For example, text is a type of sequential data because a character depends on their previous neighbors in order of appearance. Similarly, for the sequence of words themselves.</p>

<p>Suppose we want to study a sequence of data \(x_{0},x_{1},\ldots x_{t}\). Given this data, we want to model the probability of finding \(x_{t+1}\). That is, we want to calculate</p>

\[P(x_{t+1}|x_{t},\ldots,x_0)\]

<p>Using the chain rule, we can write the probability of finding the full sequence as</p>

\[P(x_{0},\ldots,x_{t},x_{t+1})=P(x_{t+1}|x_{t},\ldots, x_0)P(x_{t}|x_{t-1},\ldots,x_0)\ldots P(x_{1}|x_0)P(x_0)\]

<p>If we write \(h_t\) to denote all the previous states up to time \(t-1\)</p>

\[h_t\equiv \{x_0,x_1\ldots x_{t-1}\}\]

<p>then the probability becomes</p>

\[P(x_{0},\ldots,x_{t},x_{t+1})=P(x_{t+1}|x_t,h_t)P(x_t|x_{t-1},h_{t-1})\ldots P(x_1|x_0,h_0)\]

<p>The hidden variable \(h_t\) lives in a space we have yet to determine. Because the probability is now the product of the probabilities at each time \(t\), the loss function (using maximum likelihood) is,</p>

\[L=-\sum_t \ln P(x_t|x_{t-1},h_{t-1})\]

<p>where the sum runs over all the elements in the sequence. Below is depicted a recurrent neural network unit that models the probability \(P(x_{t+1}|x_t,h_t)\):</p>
<div style="text-align: center"><img src="/images/rnn.png" width="50%" /></div>

<p>Each node \(N_i\) contains an activation unit that takes the input \(x_t\) and the hidden state \(h_t\). That is, for each node \(N_i\) one calculates</p>

\[h_{t+1}^i=g(\sum_jW^x_{ij}x_t^j+\sum_{\alpha}W^h_{i\alpha}h^{\alpha}_t+b_i)\]

<p>where \(W^x, W^h, b\) are the parameters we want to fit. The resulting hidden state \(h_{t+1}\) sequentially passes to the next unit. To determine the probability, we stack a softmax layer on top of the hidden layer, that is,</p>

\[P(x_{t+1}|x_t,h_t)=\frac{e^{\sum_iw_{ai}h_{t+1}^i+\tilde{b}_a}}{\sum_a e^{\sum_i w_{ai}h_{t+1}^i+\tilde{b}_a}}\]

<p>where \(a\) is the class of \(x_{t+1}\).</p>

<p>We can include additional hidden layers, and they can have a different number of nodes. At each step, we have a set of ingoing hidden states \(h^1,h^2,\ldots\) for the layers 1,2, etc, and an outgoing set of hidden states.</p>

<p><a name="train"></a></p>
<h3 id="2-training"><strong>2. Training</strong></h3>

<p>Training a full sequence of data can be problematic. The gradients depend on all the past units, which for a long series makes the problem computationally very expensive. Due to the backpropagation algorithm the gradients contain many products that may lead the gradient to explode or become extremely small. Instead, we can divide the full sequence into shorter sequences. We feed the algorithm using batches of size \(N\) with sequences of length \(L\). We can stack several units horizontally, so we have a single network acting on a series.</p>

<div style="text-align: center"><img src="/images/rnn_2.png" width="50%" /></div>

<p>Here \(Y\) stands for the target.</p>

<p>The backpropagation algorithm acquires a component along the time direction. Say we want to calculate the derivative of the loss function with respect to \(\omega\), the parameter that multiplies \(x_0\). Then the gradient will receive several contributions coming from the later units because of the recurrence relationship.</p>

<div style="text-align: center"><img src="/images/rnn_backprop.png" width="50%" /></div>

<p>In this example we have</p>

\[\begin{equation*}\begin{split}\frac{\partial L}{\partial \omega}&amp;=\frac{\partial L_1}{\partial h_1}\frac{\partial h_1}{\partial \omega}+\frac{\partial L_2}{\partial h_2}\frac{\partial h_2}{\partial \omega}+\frac{\partial L_3}{\partial h_3}\frac{\partial h_3}{\partial \omega}+\ldots\\
&amp;+\frac{\partial L_2}{\partial h_2}\frac{\partial h_2}{\partial h_1}\frac{\partial h_1}{\partial \omega}+\frac{\partial L_3}{\partial h_3}\frac{\partial h_3}{\partial h_2}\frac{\partial h_2}{\partial \omega}+\ldots\\
&amp;+\frac{\partial L_3}{\partial h_3}\frac{\partial h_3}{\partial h_2}\frac{\partial h_2}{\partial h_1}\frac{\partial h_1}{\partial \omega}+\ldots\end{split}\end{equation*}\]

<p>where \(L_i\) is the contribution to the loss function coming from the i-th term in the sequence.
More generally, we calculate</p>

\[\begin{equation*}\begin{split}\frac{\partial L}{\partial \omega}&amp;=\sum_i \frac{\partial L_i}{\partial h_i}\frac{\partial h_i}{\partial \omega}+\sum_i\frac{\partial L_{i+1}}{\partial h_{i+1}}\frac{\partial h_{i+1}}{\partial h_i}\frac{\partial h_i}{\partial \omega} + \sum_i\frac{\partial L_{i+2}}{\partial h_{i+2}}\frac{\partial h_{i+2}}{\partial h_{i+1}}\frac{\partial h_{i+1}}{\partial h_i}\frac{\partial h_i}{\partial \omega}+\ldots\\
&amp;=\sum_n \sum_i \frac{\partial L_{i+n}}{\partial h_{i+n}}\frac{\partial h_i}{\partial \omega}\prod_{j=i}^{n-1+i} \frac{\partial h_{j+1}}{\partial h_{j}}
\end{split}\end{equation*}\]

<p><a name="python"></a></p>
<h3 id="3-python-implementation"><strong>3. Python Implementation</strong></h3>

<p>For this implementation, we consider a one-dimensional input \(x\) and hidden state \(h\) with dimension \(d\). For the activation function we take the \(\tanh\) function. So in each unit we have</p>

\[h_{t+1,i}=\tanh(w^0_ix_t+w^1_{ij}h^j_t+b^0_{t,i})\]

<p>and we calculate the derivatives</p>

\[\begin{equation*}\begin{split}&amp;\frac{\partial h_{t+1,i}}{\partial h_{t,j}}=(1-h_{t+1,i}^2)w^1_{ij}\\
&amp;\frac{\partial h_{t+1,i}}{\partial w^1_{ij}}=(1-h_{t+1,i}^2)h_{t,j}\\
&amp;\frac{\partial h_{t+1,i}}{\partial w^0_i}=(1-h_{t+1,i}^2)x_t\\
&amp;\frac{\partial h_{t+1,i}}{\partial b^0_{t,i}}=(1-h_{t+1,i}^2)\end{split}\end{equation*}\]

<p>We consider a regression problem, and as such the predictor has the form</p>

\[\hat{y}_{t+1,a}= w^2_{ai} h_{t+1,i}+b^1_{t,a}=w^2_{ai} \tanh(w^0_ix_t+w^1_{ij}h_{t,j}+b^0_{t,i}) + b^1_{t,a}\]

<p>where the target has features \(a\).</p>

<p>From the loss function
\(L=\sum_t L_t=\frac{1}{2N}\sum_{t=1}^N (y_t-\hat{y}_t)^2\)</p>

<p>we calculate
\(\frac{\partial L_t}{\partial h_{t,i}}=\frac{1}{N}(\hat{y}_t-y_t)_aw^2_{ai}\)</p>

<p>We define classes for the recurrent neural network, loss function and optimizer, that implements, the gradient descent update.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">RNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">wx</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tanh</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cache</span><span class="o">=</span><span class="bp">None</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="o">=</span><span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                         <span class="s">'dh_dwx'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">wx</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                         <span class="s">'dh_db'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span>
                         <span class="s">'dw2'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                         <span class="s">'db2'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">.</span><span class="n">bias</span><span class="p">}</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">hidden</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">h</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">xi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">wx</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="n">hidden</span><span class="o">=</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        
        
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cache</span><span class="o">=</span><span class="p">{</span><span class="s">'hidden'</span><span class="p">:</span><span class="n">h</span><span class="p">,</span><span class="s">'output'</span><span class="p">:</span><span class="n">out</span><span class="p">}</span>
        
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h_t</span><span class="p">,</span><span class="n">h_tp1</span><span class="p">):</span>
        <span class="n">mat</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">h_tp1</span><span class="o">*</span><span class="n">h_tp1</span><span class="p">)</span>
        
        <span class="n">dh_dwh</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="n">h_t</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#[hid_dim,hid_dim,hid_dim]
</span>        <span class="n">dh_dwx</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#[hid_dim,hid_dim,input_dim]
</span>        <span class="n">dh_dh</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span> <span class="c1">#[hid_dim,hid_dim]
</span>        <span class="n">dh_db</span><span class="o">=</span><span class="n">mat</span> <span class="c1">#[hid_dim,hid_dim]
</span>        
        <span class="k">return</span> <span class="n">dh_dh</span><span class="p">,</span> <span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:</span><span class="n">dh_dwh</span><span class="p">,</span><span class="s">'dh_dwx'</span><span class="p">:</span><span class="n">dh_dwx</span><span class="p">,</span><span class="s">'dh_db'</span><span class="p">:</span><span class="n">dh_db</span><span class="p">}</span>
</code></pre></div></div>

<p>The mean-square loss function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MSE_Loss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">y</span><span class="o">-</span><span class="n">y_pred</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="o">*</span><span class="n">loss</span>
        
        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        
        <span class="n">L</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">cache</span><span class="p">[</span><span class="s">'hidden'</span><span class="p">]</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">cache</span><span class="p">[</span><span class="s">'output'</span><span class="p">]</span>
        <span class="n">z</span><span class="o">=</span><span class="p">(</span><span class="n">out</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">L</span> <span class="c1"># (y_pred-y)/N
</span>        
        <span class="n">w2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">out_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span> <span class="c1">#[out_dim,hidden_dim]
</span>        <span class="n">grads_cache</span><span class="o">=</span><span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:[],</span><span class="s">'dh_dwx'</span><span class="p">:[],</span><span class="s">'dh_db'</span><span class="p">:[]}</span>
        <span class="n">grads_total</span><span class="o">=</span><span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span><span class="s">'dh_dwx'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span><span class="s">'dh_db'</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        
        <span class="n">hidden</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">xi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            
            <span class="n">dh_dh</span><span class="p">,</span><span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="n">hidden</span><span class="p">,</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">hidden</span><span class="o">=</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
            <span class="k">for</span> <span class="n">var</span><span class="p">,</span><span class="n">grad</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">wt</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">grads_cache</span><span class="p">[</span><span class="n">var</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">grads_total</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">+=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">wt</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                
            <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grads_cache</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">temp</span><span class="o">=</span><span class="p">[]</span>
                    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">dh</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grads_cache</span><span class="p">[</span><span class="n">var</span><span class="p">]):</span>
                        <span class="n">wt</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">dh_dh</span><span class="p">,</span><span class="n">dh</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">temp</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">wt</span><span class="p">)</span>
                        <span class="n">wt2</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span><span class="n">wt</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">wt2</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">grads_total</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">+=</span><span class="n">a</span>
                        
                    <span class="n">grads_cache</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">=</span><span class="n">temp</span><span class="p">[:]</span>
        
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'dw2'</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">])).</span><span class="n">T</span>
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'db2'</span><span class="p">]</span><span class="o">=</span><span class="n">z</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwh'</span><span class="p">]</span><span class="o">=</span><span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwh'</span><span class="p">].</span><span class="n">T</span>
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwx'</span><span class="p">]</span><span class="o">=</span><span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwx'</span><span class="p">].</span><span class="n">T</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradients</span><span class="o">=</span><span class="n">grads_total</span>
</code></pre></div></div>
<p>the optimizer</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradients</span>
        
        <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">-=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">var</span><span class="p">]</span>
</code></pre></div></div>
<p>and the training function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">targets</span><span class="p">,</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">epochs</span><span class="p">):</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">total_loss</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">seq</span><span class="p">,</span><span class="n">target</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">targets</span><span class="p">),</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">seqs</span><span class="p">)):</span>
            <span class="n">ypred</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
            <span class="n">total_loss</span><span class="o">+=</span><span class="n">loss</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">print</span><span class="p">(</span><span class="s">'epoch: '</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span><span class="s">' Loss: '</span><span class="p">,</span><span class="n">total_loss</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Data preparation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">seq_len</span><span class="o">=</span><span class="mi">10</span>
<span class="n">seqs</span><span class="o">=</span><span class="p">[]</span>
<span class="n">targets</span><span class="o">=</span><span class="p">[]</span>
<span class="n">xs</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">ts</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span><span class="n">seq_len</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ts</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span>
    <span class="c1"># add noise
</span>    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">noise</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.008</span><span class="p">,</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">data</span><span class="o">+=</span><span class="n">noise</span>
    
    <span class="n">seqs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">targets</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>This is the result of training after only 6 epochs:</p>
<div style="text-align: center"><img src="/images/RNN_6epochs.png" width="70%" /></div>

<p>and after 9 epochs:</p>
<div style="text-align: center"><img src="/images/RNN_9epochs.png" width="70%" /></div>]]></content><author><name>Joao Gomes</name></author><category term="Machine Learning" /><summary type="html"><![CDATA[A recurrent neural network implements recurrency in the data. This is suitable for time-series forecasting, text generation, or text translation.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/rnn.png" /><media:content medium="image" url="http://localhost:4000/rnn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>