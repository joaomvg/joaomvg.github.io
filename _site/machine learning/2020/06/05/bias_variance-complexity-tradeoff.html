<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Bias-Variance/Complexity tradeoff | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Bias-Variance/Complexity tradeoff" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="When fitting a model to the data, there is a tradeoff between bias and complexity. A less biased model can have higher complexity, but this also makes it more prone to overfit. In contrast, with more bias, the model is limited to simpler problems. We explain this phenomenon with python examples in both classification and regression examples." />
<meta property="og:description" content="When fitting a model to the data, there is a tradeoff between bias and complexity. A less biased model can have higher complexity, but this also makes it more prone to overfit. In contrast, with more bias, the model is limited to simpler problems. We explain this phenomenon with python examples in both classification and regression examples." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2020/06/05/bias_variance-complexity-tradeoff.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2020/06/05/bias_variance-complexity-tradeoff.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/bias_vs_complexity.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-06-05T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/bias_vs_complexity.png" />
<meta property="twitter:title" content="Bias-Variance/Complexity tradeoff" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2020/06/05/bias_variance-complexity-tradeoff.html","image":"http://localhost:4000/bias_vs_complexity.png","headline":"Bias-Variance/Complexity tradeoff","dateModified":"2020-06-05T00:00:00+02:00","datePublished":"2020-06-05T00:00:00+02:00","description":"When fitting a model to the data, there is a tradeoff between bias and complexity. A less biased model can have higher complexity, but this also makes it more prone to overfit. In contrast, with more bias, the model is limited to simpler problems. We explain this phenomenon with python examples in both classification and regression examples.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2020/06/05/bias_variance-complexity-tradeoff.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Bias-Variance/Complexity tradeoff</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-06-05T00:00:00+02:00" itemprop="datePublished">
        Jun 5, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul>
  <li><a href="#1-basic-concept"><strong>1. Basic concept</strong></a></li>
  <li><a href="#2-python-implementation-classification"><strong>2. Python implementation: Classification</strong></a></li>
  <li><a href="#3-python-implementation-regression"><strong>3. Python implementation: Regression</strong></a></li>
</ul>

<p><a name="def1"></a></p>
<h3 id="1-basic-concept"><strong>1. Basic concept</strong></h3>

<p>Let $h_S$ be the solution of an ERM algorithm. We decompose the generalization error as</p>

\[L_D(h_S)=\epsilon_{app}+\epsilon_{est}\]

<p>with</p>

\[\epsilon_{app}=\text{min}_{h\in \mathcal{H}}L_D(h),\;\;\epsilon_{est}=L_D(h_S)-\text{min}_{h\in \mathcal{H}}L_D(h)\]

<p>Here $\epsilon_{app}$ is the <strong>approximation error</strong> which is the smallest error one can achieve using the hypothesis class $\mathcal{H}$. This error is independent of the data and depends only on the choice of the hypothesis class. On the other hand, $\epsilon_{est}$ is the <strong>estimation error</strong>, that is, it measures how far the generalization error is from the approximation error. Since $h_S$ depends on the training set, the estimation error depends strongly on the training data.</p>

<p>To reduce the approximation error we need a more complex hypothesis class but this might make the estimation error worse, since a more complex hypothesis may lead to overfitting. On the other hand, a smaller hypothesis class, that is, less complex, reduces the estimation error, because $h_S$ and $\text{argmin}_hL_D(h)$ are now closer, but it increases the approximation error because of underfitting. This tradeoff is known as the <strong>bias-complexity</strong> tradeoff.</p>

<p>Lets see how this works in practice. We create artificial data of around 1 million samples in a 10 dimensional feature space, according to the classification rule:</p>

\[y(x)=\text{sign}(w^0_1\tanh(w^1_ix^i)+w^0_2\tanh(w^2_ix^i))\]

<p>where $w^1,w^2$ are 10 dimensional parameters and $(w^0_1,w^0_2)$ is a two parameter. For the classification we use a decision tree and adjust its max depth and number of features used in order to obtain different levels of complexity. Below we show the behaviour of the estimation (est_error), approximation (app_error) and generalization errors (gen_error):</p>

<div style="text-align: center"><img src="/images/bias_vs_complexity.png" width="60%" /></div>

<p>To determine the approximation error, we train the decision tree on the full data while keeping fixed the number of features used and adjusting the tree’s depth (max_depth in the picture above). On the other hand, to determine the estimation error, we train the decision tree on 10% of the data (around 100k samples) for various depths and number of features. The generalization error is calculated on the remaining 90% of the data.</p>

<p>The generalization error curves show a tradeoff between bias and complexity. When the depth is smaller, so bias is more considerable, the approximation error grows, but the estimation error is smaller. In contrast, if we increase the depth, the approximation error becomes smaller, but the estimation error grows due to overfitting. The “sweet spot” occurs for an intermediate value of the depth, where the generalization error is a minimum.</p>

<p>Similar behaviour is obtained for different number of features (max_features):</p>

<div style="text-align: center"><img src="/images/bias_vs_complex_multiple.png" width="80%" /></div>

<p>In the case of regression, we observe a similar tradeoff. Nevertheless, the analysis is slightly different. In general we want to model $y=f(x)+\epsilon$ where $\epsilon$ is noise with mean zero and standard deviation $\sigma$. So we use an algorithm to approximate $f(x)\simeq \hat{f}(x)$. Here $\hat{f}$ is the output of our algorithm.</p>

<p>The mean square error of a predictor (regression problem) can be decomposed as follows:</p>

\[E_D[(y-\hat{f}(x_0;D))^2]=\text{Bias}^2+\text{Var}^2+\sigma^2\]

<p>where</p>

\[\text{Bias}=E_D[\hat{f}(x_0;D)]-f(x_0)\]

<p>and</p>

\[\text{Var}^2=E_D[E_D[\hat{f}(x_0;D)]-\hat{f}(x_0;D)]^2\]

<p>Note that the expectation $E_D$ is calculated by using different training datasets and $x_0$ is a reference point- the error will depend on this point which is kept fixed while averaging over different training sets.</p>

<p>We use fake data (1million samples) in a 10 dimensional feature space and target function</p>

\[f(x)=(x.w)^4+(x.w)^2+x.h\]

<p>where $w,h$ are 10 dimensional parameter arrays. We use a decision tree regressor and adjust number of features used and max depth.</p>

<div style="text-align: center"><img src="/images/bias_variance_maxfeatures.png" width="60%" /></div>

<p>We sample about 20k data points in each iteration and fit the decision tree,  calculate $\hat{f}(x_0)$ for a determined reference point and store this value. The bias is calculated from the mean of the difference $\hat{f}(x_0)-f(x_0)$ and for the variance, we compute $\hat{f}(x_0)$ after each training sample and then calculate the variance of that array.</p>

<div style="text-align: center"><img src="/images/bias_variance_multiple.png" width="80%" /></div>

<p>We can see that while variance increases with increasing depth, bias decreases. This behavior translates into a trade-off between bias and variance, explaining why the mean square error (mse) reachs its minimum at an intermediate depth.</p>

<p><a name="python"></a></p>
<h3 id="2-python-implementation-classification"><strong>2. Python implementation: Classification</strong></h3>

<p>Classification with Decision Tree:</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_wine</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">import</span> <span class="nn">progressbar</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
</code></pre></div></div>
<p>Create artificial data</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">3</span><span class="p">,(</span><span class="mi">1000000</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">w1</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">w2</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">w0</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="n">y</span><span class="o">=</span><span class="n">w0</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w1</span><span class="p">))</span><span class="o">+</span><span class="n">w0</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">))</span>
<span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sign</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>
<p>Train and test data:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">indices</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

<span class="n">l</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">0.1</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span> <span class="c1">#keep only 10% of the data
</span><span class="n">train_x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="p">][:</span><span class="n">l</span><span class="p">]</span>
<span class="n">test_x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="n">indices</span><span class="p">][</span><span class="n">l</span><span class="p">:]</span>
<span class="n">train_y</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">][:</span><span class="n">l</span><span class="p">]</span>
<span class="n">test_y</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">indices</span><span class="p">][</span><span class="n">l</span><span class="p">:]</span>

<span class="c1">#add some noise to the training data
</span><span class="n">noise</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,(</span><span class="n">l</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">train_x</span><span class="o">=</span><span class="n">train_x</span><span class="o">+</span><span class="n">noise</span>
</code></pre></div></div>
<p>Train the algorithm on the full dataset (1million). We can then determine the approximation error:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Decision Tree parameters
</span><span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s">'ccp_alpha'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s">'class_weight'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'criterion'</span><span class="p">:</span> <span class="s">'gini'</span><span class="p">,</span>
 <span class="s">'max_depth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="s">'max_features'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
 <span class="s">'max_leaf_nodes'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s">'min_impurity_split'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'min_samples_leaf'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="s">'min_samples_split'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="s">'min_weight_fraction_leaf'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s">'presort'</span><span class="p">:</span> <span class="s">'deprecated'</span><span class="p">,</span>
 <span class="s">'random_state'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'splitter'</span><span class="p">:</span> <span class="s">'best'</span><span class="p">}</span>

<span class="n">complexity</span><span class="o">=</span><span class="p">{}</span>
<span class="n">max_features</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span>

<span class="k">for</span> <span class="n">max_f</span> <span class="ow">in</span> <span class="n">progressbar</span><span class="p">.</span><span class="n">progressbar</span><span class="p">(</span><span class="n">max_features</span><span class="p">):</span>
    <span class="n">params</span><span class="p">[</span><span class="s">'max_features'</span><span class="p">]</span><span class="o">=</span><span class="n">max_f</span>
    <span class="n">complexity</span><span class="p">[</span><span class="n">max_f</span><span class="p">]</span><span class="o">=</span><span class="p">[]</span>
    
    <span class="n">DT</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">pred</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
        <span class="n">DT</span><span class="p">.</span><span class="n">max_depth</span><span class="o">=</span><span class="n">d</span>
        <span class="n">DT</span><span class="p">.</span><span class="n">random_state</span><span class="o">=</span><span class="n">d</span>
        <span class="n">DT</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
        <span class="n">acc</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">DT</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">),</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">-</span><span class="n">acc</span>

    <span class="c1">#parallelize the calculation
</span>    <span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">mp</span><span class="p">.</span><span class="n">cpu_count</span><span class="p">())</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span> 
        <span class="n">complexity</span><span class="p">[</span><span class="n">max_f</span><span class="p">]</span><span class="o">=</span><span class="n">pool</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">pred</span><span class="p">,[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">31</span><span class="p">)])</span>
</code></pre></div></div>
<p>Now train on the train set:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">max_features</span><span class="o">=</span><span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">complexity</span><span class="p">]</span>

<span class="n">learning</span><span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">max_features</span><span class="p">:</span>
    <span class="n">params</span><span class="p">[</span><span class="s">'max_features'</span><span class="p">]</span><span class="o">=</span><span class="n">f</span>
    <span class="n">learning</span><span class="p">[</span><span class="n">f</span><span class="p">]</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">progressbar</span><span class="p">.</span><span class="n">progressbar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">31</span><span class="p">)):</span>
        <span class="n">params</span><span class="p">[</span><span class="s">'max_depth'</span><span class="p">]</span><span class="o">=</span><span class="n">depth</span>
        <span class="n">params</span><span class="p">[</span><span class="s">'random_state'</span><span class="p">]</span><span class="o">=</span><span class="n">depth</span><span class="o">+</span><span class="mi">100</span>
        <span class="n">DT</span><span class="o">=</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>
        <span class="n">DT</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span><span class="n">train_y</span><span class="p">)</span>
        <span class="n">acc</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">DT</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">),</span><span class="n">test_y</span><span class="p">)</span> <span class="c1">#calculates the generalization error
</span>        <span class="n">learning</span><span class="p">[</span><span class="n">f</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">acc</span><span class="p">)</span>
</code></pre></div></div>
<p><a name="python2"></a></p>
<h3 id="3-python-implementation-regression"><strong>3. Python implementation: Regression</strong></h3>
<p>Regression with Decision Tree:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">progressbar</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">import</span> <span class="nn">multiprocessing</span> <span class="k">as</span> <span class="n">mp</span>
</code></pre></div></div>

<p>Data preparation</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,(</span><span class="mi">1000000</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
<span class="n">h</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>

<span class="c1">#target function
</span><span class="k">def</span> <span class="nf">fnt</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="o">+</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">h</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">**</span><span class="mi">4</span>

<span class="n">y</span><span class="o">=</span><span class="n">fnt</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p>Decision Tree regressor:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s">'criterion'</span><span class="p">:</span> <span class="s">'mse'</span><span class="p">,</span>
 <span class="s">'max_depth'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'max_features'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'max_leaf_nodes'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'min_impurity_decrease'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s">'min_impurity_split'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'min_samples_leaf'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
 <span class="s">'min_samples_split'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
 <span class="s">'min_weight_fraction_leaf'</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s">'random_state'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
 <span class="s">'splitter'</span><span class="p">:</span> <span class="s">'best'</span><span class="p">}</span>

<span class="n">indices</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="n">i</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">indices</span><span class="p">[:</span><span class="mi">10</span><span class="p">],</span><span class="mi">1</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">x0</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1">#reference point
</span>
<span class="n">max_features</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span>
<span class="n">models</span><span class="o">=</span><span class="p">{}</span>

<span class="k">def</span> <span class="nf">sampling</span><span class="p">(</span><span class="n">t</span><span class="p">):</span> <span class="c1">#takes in a model and fits on a sample
</span>    <span class="n">model</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">t</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">model</span><span class="p">.</span><span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="o">%</span><span class="mi">11</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">indices</span><span class="p">[</span><span class="mi">10</span><span class="p">:],</span><span class="mi">2</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">,</span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">train_x</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">train_y</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span><span class="n">train_y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
        
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">progressbar</span><span class="p">.</span><span class="n">progressbar</span><span class="p">(</span><span class="n">max_features</span><span class="p">):</span>
    <span class="n">params</span><span class="p">[</span><span class="s">'max_features'</span><span class="p">]</span><span class="o">=</span><span class="n">f</span>
    <span class="n">models</span><span class="p">[</span><span class="n">f</span><span class="p">]</span><span class="o">=</span><span class="p">{}</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">31</span><span class="p">):</span>
        <span class="n">params</span><span class="p">[</span><span class="s">'max_depth'</span><span class="p">]</span><span class="o">=</span><span class="n">d</span>
        
        <span class="k">with</span> <span class="n">mp</span><span class="p">.</span><span class="n">Pool</span><span class="p">(</span><span class="n">mp</span><span class="p">.</span><span class="n">cpu_count</span><span class="p">())</span> <span class="k">as</span> <span class="n">pool</span><span class="p">:</span>
            <span class="n">models</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="n">d</span><span class="p">]</span><span class="o">=</span><span class="n">pool</span><span class="p">.</span><span class="nb">map</span><span class="p">(</span><span class="n">sampling</span><span class="p">,[(</span><span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">),</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">)])</span>
</code></pre></div></div>

<p>Calculate Bias and Variance:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">bias</span><span class="o">=</span><span class="p">{}</span>
<span class="n">var</span><span class="o">=</span><span class="p">{}</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">progressbar</span><span class="p">.</span><span class="n">progressbar</span><span class="p">(</span><span class="n">max_features</span><span class="p">):</span>
    <span class="n">bias</span><span class="p">[</span><span class="n">f</span><span class="p">]</span><span class="o">=</span><span class="p">{}</span>
    <span class="n">var</span><span class="p">[</span><span class="n">f</span><span class="p">]</span><span class="o">=</span><span class="p">{}</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">31</span><span class="p">):</span>
        <span class="n">bias</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="n">d</span><span class="p">]</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">var</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="n">d</span><span class="p">]</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="n">models</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="n">d</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="c1">#predictions
</span>        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="n">d</span><span class="p">][</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="n">y_pred</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">y_pred</span><span class="p">,</span><span class="n">m</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">bias</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="n">d</span><span class="p">]</span><span class="o">=</span><span class="n">y_pred</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span><span class="o">-</span><span class="n">fnt</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
        <span class="n">var</span><span class="p">[</span><span class="n">f</span><span class="p">][</span><span class="n">d</span><span class="p">]</span><span class="o">=</span><span class="n">y_pred</span><span class="p">.</span><span class="n">var</span><span class="p">()</span>
</code></pre></div></div>

  </div><a class="u-url" href="/machine%20learning/2020/06/05/bias_variance-complexity-tradeoff.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




