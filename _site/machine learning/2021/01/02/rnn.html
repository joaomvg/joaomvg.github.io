<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Recurrent Neural Network | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Recurrent Neural Network" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A recurrent neural network implements recurrency in the data. This is suitable for time-series forecasting, text generation, or text translation." />
<meta property="og:description" content="A recurrent neural network implements recurrency in the data. This is suitable for time-series forecasting, text generation, or text translation." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2021/01/02/rnn.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2021/01/02/rnn.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/rnn.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-02T00:00:00+01:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/rnn.png" />
<meta property="twitter:title" content="Recurrent Neural Network" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2021/01/02/rnn.html","image":"http://localhost:4000/rnn.png","headline":"Recurrent Neural Network","dateModified":"2021-01-02T00:00:00+01:00","datePublished":"2021-01-02T00:00:00+01:00","description":"A recurrent neural network implements recurrency in the data. This is suitable for time-series forecasting, text generation, or text translation.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2021/01/02/rnn.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Recurrent Neural Network</h1>
    <p class="post-meta"><time class="dt-published" datetime="2021-01-02T00:00:00+01:00" itemprop="datePublished">
        Jan 2, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul>
  <li><a href="#1-rnn-architecture"><strong>1. RNN architecture</strong></a></li>
  <li><a href="#2-training"><strong>2. Training</strong></a></li>
  <li><a href="#3-python-implementation"><strong>3. Python Implementation</strong></a></li>
</ul>

<p><a name="rnn"></a></p>
<h3 id="1-rnn-architecture"><strong>1. RNN architecture</strong></h3>

<p>A recurrent neural network (RNN) learns sequential data. For example, text is a type of sequential data because a character depends on their previous neighbors in order of appearance. Similarly, for the sequence of words themselves.</p>

<p>Suppose we want to study a sequence of data \(x_{0},x_{1},\ldots x_{t}\). Given this data, we want to model the probability of finding \(x_{t+1}\). That is, we want to calculate</p>

\[P(x_{t+1}|x_{t},\ldots,x_0)\]

<p>Using the chain rule, we can write the probability of finding the full sequence as</p>

\[P(x_{0},\ldots,x_{t},x_{t+1})=P(x_{t+1}|x_{t},\ldots, x_0)P(x_{t}|x_{t-1},\ldots,x_0)\ldots P(x_{1}|x_0)P(x_0)\]

<p>If we write \(h_t\) to denote all the previous states up to time \(t-1\)</p>

\[h_t\equiv \{x_0,x_1\ldots x_{t-1}\}\]

<p>then the probability becomes</p>

\[P(x_{0},\ldots,x_{t},x_{t+1})=P(x_{t+1}|x_t,h_t)P(x_t|x_{t-1},h_{t-1})\ldots P(x_1|x_0,h_0)\]

<p>The hidden variable \(h_t\) lives in a space we have yet to determine. Because the probability is now the product of the probabilities at each time \(t\), the loss function (using maximum likelihood) is,</p>

\[L=-\sum_t \ln P(x_t|x_{t-1},h_{t-1})\]

<p>where the sum runs over all the elements in the sequence. Below is depicted a recurrent neural network unit that models the probability \(P(x_{t+1}|x_t,h_t)\):</p>
<div style="text-align: center"><img src="/images/rnn.png" width="50%" /></div>

<p>Each node \(N_i\) contains an activation unit that takes the input \(x_t\) and the hidden state \(h_t\). That is, for each node \(N_i\) one calculates</p>

\[h_{t+1}^i=g(\sum_jW^x_{ij}x_t^j+\sum_{\alpha}W^h_{i\alpha}h^{\alpha}_t+b_i)\]

<p>where \(W^x, W^h, b\) are the parameters we want to fit. The resulting hidden state \(h_{t+1}\) sequentially passes to the next unit. To determine the probability, we stack a softmax layer on top of the hidden layer, that is,</p>

\[P(x_{t+1}|x_t,h_t)=\frac{e^{\sum_iw_{ai}h_{t+1}^i+\tilde{b}_a}}{\sum_a e^{\sum_i w_{ai}h_{t+1}^i+\tilde{b}_a}}\]

<p>where \(a\) is the class of \(x_{t+1}\).</p>

<p>We can include additional hidden layers, and they can have a different number of nodes. At each step, we have a set of ingoing hidden states \(h^1,h^2,\ldots\) for the layers 1,2, etc, and an outgoing set of hidden states.</p>

<p><a name="train"></a></p>
<h3 id="2-training"><strong>2. Training</strong></h3>

<p>Training a full sequence of data can be problematic. The gradients depend on all the past units, which for a long series makes the problem computationally very expensive. Due to the backpropagation algorithm the gradients contain many products that may lead the gradient to explode or become extremely small. Instead, we can divide the full sequence into shorter sequences. We feed the algorithm using batches of size \(N\) with sequences of length \(L\). We can stack several units horizontally, so we have a single network acting on a series.</p>

<div style="text-align: center"><img src="/images/rnn_2.png" width="50%" /></div>

<p>Here \(Y\) stands for the target.</p>

<p>The backpropagation algorithm acquires a component along the time direction. Say we want to calculate the derivative of the loss function with respect to \(\omega\), the parameter that multiplies \(x_0\). Then the gradient will receive several contributions coming from the later units because of the recurrence relationship.</p>

<div style="text-align: center"><img src="/images/rnn_backprop.png" width="50%" /></div>

<p>In this example we have</p>

\[\begin{equation*}\begin{split}\frac{\partial L}{\partial \omega}&amp;=\frac{\partial L_1}{\partial h_1}\frac{\partial h_1}{\partial \omega}+\frac{\partial L_2}{\partial h_2}\frac{\partial h_2}{\partial \omega}+\frac{\partial L_3}{\partial h_3}\frac{\partial h_3}{\partial \omega}+\ldots\\
&amp;+\frac{\partial L_2}{\partial h_2}\frac{\partial h_2}{\partial h_1}\frac{\partial h_1}{\partial \omega}+\frac{\partial L_3}{\partial h_3}\frac{\partial h_3}{\partial h_2}\frac{\partial h_2}{\partial \omega}+\ldots\\
&amp;+\frac{\partial L_3}{\partial h_3}\frac{\partial h_3}{\partial h_2}\frac{\partial h_2}{\partial h_1}\frac{\partial h_1}{\partial \omega}+\ldots\end{split}\end{equation*}\]

<p>where \(L_i\) is the contribution to the loss function coming from the i-th term in the sequence.
More generally, we calculate</p>

\[\begin{equation*}\begin{split}\frac{\partial L}{\partial \omega}&amp;=\sum_i \frac{\partial L_i}{\partial h_i}\frac{\partial h_i}{\partial \omega}+\sum_i\frac{\partial L_{i+1}}{\partial h_{i+1}}\frac{\partial h_{i+1}}{\partial h_i}\frac{\partial h_i}{\partial \omega} + \sum_i\frac{\partial L_{i+2}}{\partial h_{i+2}}\frac{\partial h_{i+2}}{\partial h_{i+1}}\frac{\partial h_{i+1}}{\partial h_i}\frac{\partial h_i}{\partial \omega}+\ldots\\
&amp;=\sum_n \sum_i \frac{\partial L_{i+n}}{\partial h_{i+n}}\frac{\partial h_i}{\partial \omega}\prod_{j=i}^{n-1+i} \frac{\partial h_{j+1}}{\partial h_{j}}
\end{split}\end{equation*}\]

<p><a name="python"></a></p>
<h3 id="3-python-implementation"><strong>3. Python Implementation</strong></h3>

<p>For this implementation, we consider a one-dimensional input \(x\) and hidden state \(h\) with dimension \(d\). For the activation function we take the \(\tanh\) function. So in each unit we have</p>

\[h_{t+1,i}=\tanh(w^0_ix_t+w^1_{ij}h^j_t+b^0_{t,i})\]

<p>and we calculate the derivatives</p>

\[\begin{equation*}\begin{split}&amp;\frac{\partial h_{t+1,i}}{\partial h_{t,j}}=(1-h_{t+1,i}^2)w^1_{ij}\\
&amp;\frac{\partial h_{t+1,i}}{\partial w^1_{ij}}=(1-h_{t+1,i}^2)h_{t,j}\\
&amp;\frac{\partial h_{t+1,i}}{\partial w^0_i}=(1-h_{t+1,i}^2)x_t\\
&amp;\frac{\partial h_{t+1,i}}{\partial b^0_{t,i}}=(1-h_{t+1,i}^2)\end{split}\end{equation*}\]

<p>We consider a regression problem, and as such the predictor has the form</p>

\[\hat{y}_{t+1,a}= w^2_{ai} h_{t+1,i}+b^1_{t,a}=w^2_{ai} \tanh(w^0_ix_t+w^1_{ij}h_{t,j}+b^0_{t,i}) + b^1_{t,a}\]

<p>where the target has features \(a\).</p>

<p>From the loss function
\(L=\sum_t L_t=\frac{1}{2N}\sum_{t=1}^N (y_t-\hat{y}_t)^2\)</p>

<p>we calculate
\(\frac{\partial L_t}{\partial h_{t,i}}=\frac{1}{N}(\hat{y}_t-y_t)_aw^2_{ai}\)</p>

<p>We define classes for the recurrent neural network, loss function and optimizer, that implements, the gradient descent update.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="mi">0</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span> <span class="nc">RNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="o">=</span><span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_dim</span><span class="o">=</span><span class="n">out_dim</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">out_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">wx</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tanh</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">gradients</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cache</span><span class="o">=</span><span class="bp">None</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">parameters</span><span class="o">=</span><span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                         <span class="s">'dh_dwx'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">wx</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                         <span class="s">'dh_db'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">,</span>
                         <span class="s">'dw2'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span>
                         <span class="s">'db2'</span><span class="p">:</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">.</span><span class="n">bias</span><span class="p">}</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">hidden</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">h</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">))</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">xi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">g</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">wx</span><span class="p">(</span><span class="n">xi</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span><span class="o">+</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="n">hidden</span><span class="o">=</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        
        
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cache</span><span class="o">=</span><span class="p">{</span><span class="s">'hidden'</span><span class="p">:</span><span class="n">h</span><span class="p">,</span><span class="s">'output'</span><span class="p">:</span><span class="n">out</span><span class="p">}</span>
        
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">grads</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">h_t</span><span class="p">,</span><span class="n">h_tp1</span><span class="p">):</span>
        <span class="n">mat</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="n">np</span><span class="p">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">h_tp1</span><span class="o">*</span><span class="n">h_tp1</span><span class="p">)</span>
        
        <span class="n">dh_dwh</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="n">h_t</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#[hid_dim,hid_dim,hid_dim]
</span>        <span class="n">dh_dwx</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="c1">#[hid_dim,hid_dim,input_dim]
</span>        <span class="n">dh_dh</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">mat</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">wh</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span> <span class="c1">#[hid_dim,hid_dim]
</span>        <span class="n">dh_db</span><span class="o">=</span><span class="n">mat</span> <span class="c1">#[hid_dim,hid_dim]
</span>        
        <span class="k">return</span> <span class="n">dh_dh</span><span class="p">,</span> <span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:</span><span class="n">dh_dwh</span><span class="p">,</span><span class="s">'dh_dwx'</span><span class="p">:</span><span class="n">dh_dwx</span><span class="p">,</span><span class="s">'dh_db'</span><span class="p">:</span><span class="n">dh_db</span><span class="p">}</span>
</code></pre></div></div>

<p>The mean-square loss function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MSE_Loss</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">y_pred</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">y</span><span class="o">-</span><span class="n">y_pred</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="o">*</span><span class="n">loss</span>
        
        <span class="k">return</span> <span class="n">loss</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        
        <span class="n">L</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">h</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">cache</span><span class="p">[</span><span class="s">'hidden'</span><span class="p">]</span>
        <span class="n">out</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">cache</span><span class="p">[</span><span class="s">'output'</span><span class="p">]</span>
        <span class="n">z</span><span class="o">=</span><span class="p">(</span><span class="n">out</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">L</span> <span class="c1"># (y_pred-y)/N
</span>        
        <span class="n">w2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">out_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">T</span> <span class="c1">#[out_dim,hidden_dim]
</span>        <span class="n">grads_cache</span><span class="o">=</span><span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:[],</span><span class="s">'dh_dwx'</span><span class="p">:[],</span><span class="s">'dh_db'</span><span class="p">:[]}</span>
        <span class="n">grads_total</span><span class="o">=</span><span class="p">{</span><span class="s">'dh_dwh'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span><span class="s">'dh_dwx'</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span><span class="s">'dh_db'</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        
        <span class="n">hidden</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">xi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            
            <span class="n">dh_dh</span><span class="p">,</span><span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">grads</span><span class="p">(</span><span class="n">xi</span><span class="p">,</span><span class="n">hidden</span><span class="p">,</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">hidden</span><span class="o">=</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            
            <span class="k">for</span> <span class="n">var</span><span class="p">,</span><span class="n">grad</span> <span class="ow">in</span> <span class="n">grads</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">wt</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">grads_cache</span><span class="p">[</span><span class="n">var</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="n">grads_total</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">+=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">wt</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                
            <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">grads_cache</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">temp</span><span class="o">=</span><span class="p">[]</span>
                    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">dh</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grads_cache</span><span class="p">[</span><span class="n">var</span><span class="p">]):</span>
                        <span class="n">wt</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">dh_dh</span><span class="p">,</span><span class="n">dh</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">temp</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">wt</span><span class="p">)</span>
                        <span class="n">wt2</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span><span class="n">wt</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">a</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">wt2</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">]))</span>
                        <span class="n">grads_total</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">+=</span><span class="n">a</span>
                        
                    <span class="n">grads_cache</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">=</span><span class="n">temp</span><span class="p">[:]</span>
        
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'dw2'</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">axes</span><span class="o">=</span><span class="p">([</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">])).</span><span class="n">T</span>
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'db2'</span><span class="p">]</span><span class="o">=</span><span class="n">z</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwh'</span><span class="p">]</span><span class="o">=</span><span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwh'</span><span class="p">].</span><span class="n">T</span>
        <span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwx'</span><span class="p">]</span><span class="o">=</span><span class="n">grads_total</span><span class="p">[</span><span class="s">'dh_dwx'</span><span class="p">].</span><span class="n">T</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradients</span><span class="o">=</span><span class="n">grads_total</span>
</code></pre></div></div>
<p>the optimizer</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">grads</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">gradients</span>
        
        <span class="k">for</span> <span class="n">var</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">-=</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">*</span><span class="n">grads</span><span class="p">[</span><span class="n">var</span><span class="p">]</span>
</code></pre></div></div>
<p>and the training function:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">targets</span><span class="p">,</span><span class="n">model</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span><span class="n">optimizer</span><span class="p">,</span><span class="n">epochs</span><span class="p">):</span>
    
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">total_loss</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">seq</span><span class="p">,</span><span class="n">target</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">seqs</span><span class="p">,</span><span class="n">targets</span><span class="p">),</span><span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">seqs</span><span class="p">)):</span>
            <span class="n">ypred</span><span class="o">=</span><span class="n">model</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
            <span class="n">total_loss</span><span class="o">+=</span><span class="n">loss</span><span class="p">(</span><span class="n">ypred</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span><span class="n">target</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">print</span><span class="p">(</span><span class="s">'epoch: '</span><span class="p">,</span><span class="n">epoch</span><span class="p">,</span><span class="s">' Loss: '</span><span class="p">,</span><span class="n">total_loss</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Data preparation:</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">seq_len</span><span class="o">=</span><span class="mi">10</span>
<span class="n">seqs</span><span class="o">=</span><span class="p">[]</span>
<span class="n">targets</span><span class="o">=</span><span class="p">[]</span>
<span class="n">xs</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">ts</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="p">,</span><span class="n">seq_len</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ts</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span>
    <span class="c1"># add noise
</span>    <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">noise</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.008</span><span class="p">,</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">data</span><span class="o">+=</span><span class="n">noise</span>
    
    <span class="n">seqs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">targets</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<p>This is the result of training after only 6 epochs:</p>
<div style="text-align: center"><img src="/images/RNN_6epochs.png" width="70%" /></div>

<p>and after 9 epochs:</p>
<div style="text-align: center"><img src="/images/RNN_9epochs.png" width="70%" /></div>

  </div><a class="u-url" href="/machine%20learning/2021/01/02/rnn.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




