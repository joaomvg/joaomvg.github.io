<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Neural Network | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Neural Network" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A neural network is a graphical representation of a set of linear and non-linear operations acting on an input data-point. In a feed-forward neural network, we stack several layers sequentially. The input data cross multiple layers, changing its feature’s representation along the way. This process allows the creation of very complex predictors." />
<meta property="og:description" content="A neural network is a graphical representation of a set of linear and non-linear operations acting on an input data-point. In a feed-forward neural network, we stack several layers sequentially. The input data cross multiple layers, changing its feature’s representation along the way. This process allows the creation of very complex predictors." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2020/11/12/nn.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2020/11/12/nn.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/nn.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-12T00:00:00+01:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/nn.png" />
<meta property="twitter:title" content="Neural Network" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2020/11/12/nn.html","image":"http://localhost:4000/nn.png","headline":"Neural Network","dateModified":"2020-11-12T00:00:00+01:00","datePublished":"2020-11-12T00:00:00+01:00","description":"A neural network is a graphical representation of a set of linear and non-linear operations acting on an input data-point. In a feed-forward neural network, we stack several layers sequentially. The input data cross multiple layers, changing its feature’s representation along the way. This process allows the creation of very complex predictors.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2020/11/12/nn.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Neural Network</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-11-12T00:00:00+01:00" itemprop="datePublished">
        Nov 12, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul>
  <li><a href="#1-neural-network"><strong>1. Neural Network</strong></a></li>
  <li><a href="#2-backpropagation"><strong>2. Backpropagation</strong></a></li>
  <li><a href="#3-vc-dimension"><strong>3. VC-dimension</strong></a></li>
  <li><a href="#4-decision-boundary"><strong>4. Decision Boundary</strong></a></li>
  <li><a href="#5-python-implementation"><strong>5. Python implementation</strong></a></li>
  <li><a href="#references"><strong>References</strong></a></li>
</ul>

<p><a name="def1"></a></p>
<h3 id="1-neural-network"><strong>1. Neural Network</strong></h3>
<div style="text-align: center"><img src="/images/nn.png" width="50%" /></div>

<p>A neural network is a graph composed of nodes and edges. The edges implement linear operations while the nodes aggregate each edge’s contribution before composing by an activation function $g$. This process is replicated to the next layer. Note that the nodes do not interact with each other within each layer; that is, there is no edge between nodes. Mathematically we have the following series of operations.</p>

\[\begin{equation}\begin{split}&amp;z^{(l)}_i=g(\omega^{(l)}_{ij}z^{(l-1)}_j+b^{(l)}_j)\\
&amp;z^{(l-1)}_j=g(\omega^{(l-1)}_{jk}z^{(l-2)}_k+b^{(l-1)}_j)\\
&amp;\ldots\\
&amp;z^{(1)}_p = g(\omega^{(1)}_{pr}x_r+b^{(0)}_l)
\end{split}\end{equation}\]

<p>The activation function $g$ is a non-linear function with support on the real line. A common choice is the sigmoid function but the sign function also works. This sequence of compositions is known as forward pass.</p>

<p>A neural network is a type of universal approximator. Cybenko (1989) has shown that any continuous function $f$ in $I_n$, the n-dimensional unit cube, can be approximated with arbitrary accuracy by a sum of the form
\(C=\sum_{i=1}^N \alpha_i\sigma(\beta_i^T\cdot x+b_i)\)
That is,
\(|C(x)-f(x)|&lt;\epsilon,\;\forall x\in I_n\)
for any $\epsilon&gt;0$.</p>

<p>The network architecture has two major parameters that we need to tune: the number of neurons per layer and the depth or number of layers. Increasing the number of neurons in a layer adds complexity to the neural network because we add more parameters to fit. And the depth does it too. However, adding depth to the neural network increases the number of parameters more rapidly than adding neurons in the same layer. Suppose we have one hidden layer with $n$ neurons. The number of edges flowing to this layer is $n(d+1)$ where $d$ is the input dimension. Instead, if we consider two hidden layers with $n/2$ neurons each, we have in total $n(n/2+1)/2+n(d+1)/2$ edges flowing to the hidden layers. This number scales quadratically with $n$, while for a single hidden layer, it scales linearly.</p>

<p>But adding depth has an additional effect. We can see the output of a layer as a different feature representation of the data. Adding layers also allows the neural network to learn other representations of the data, which may help performance. The neural network can be trained beforehand on large datasets and learn very complex features. We can take the last hidden layer as a new input feature and train only the last layer weights. Training the last layer allows the neural network to learn datasets that may differ, in population, from the training set.</p>

<p>Instead, if we were to train a neural network with only one hidden layer, we would need to add an increasing number of neurons to capture more complex functions. However, the effect of having a large number of neurons may be prejudicial as we are increasing the dimension of the hidden feature space, leading to dimensionality issues. In contrast, adding depth increases complexity while keeping the dimensionality of the hidden space under control.</p>

<p>Although depth helps to learn, it brings other shortcomings in terms of training. With more depth, the loss function derivatives can be challenging to calculate. While the last layers’ parameters are easier to learn, the first layers’ parameters can be hard to learn. Having many products will eventually make the derivative approach zero or become quite large, which hinders training.</p>

<p><a name="training"></a></p>
<h3 id="2-backpropagation"><strong>2. Backpropagation</strong></h3>

<p>Lets consider a binary classification problem with classes $y={0,1}$. In this case we want to model the probability 
$p(x)\equiv p(y=1|x)$. 
The loss function is the log-loss function given by</p>

\[L=-\sum_iy_i\ln p(x_i)+(1-y_i)\ln(1-p(x_i))\]

<p>and we model $p(x)$ with a neural network with one hidden layer, that is,</p>

<div style="text-align: center"><img src="/images/nn2.png" width="50%" /></div>

<p>So we have</p>

\[\begin{aligned}&amp; p(x)=\sigma(\omega^{(2)}_{i}z^{(1)}_i+b^{(2)})\\
&amp;z^{(1)}_j = \sigma( \omega^{(1)}_{jk}x_k+b^{(1)}_j)
\end{aligned}\]

<p>where implicit contractions are in place.</p>

<p>The loss function depends on the weights 
\(\omega^{(2)}_i\) 
and 
\(\omega^{(1)}_{ij}\)
in a very complicated way. To calculate the minimum of the loss function we use the gradient descent algorithm. So we calculate the derivatives of the loss function with respect to the weights,</p>

\[\begin{equation*}\begin{split}&amp;\frac{\partial L}{\partial \omega^{(2)}_i}=\sum_x \frac{\partial L}{\partial p(x)}\dot{\sigma}(\omega^{(2)}_{k}z^{(1)}_k+b^{(2)})z^{(1)}_i\\
&amp;\frac{\partial L}{\partial b^{(2)}}=\sum_x \frac{\partial L}{\partial p(x)}\dot{\sigma}(\omega^{(2)}_{k}z^{(1)}_k+b^{(2)})\\
&amp;\frac{\partial L}{\partial \omega^{(1)}_{ij}}=\sum_x \frac{\partial L}{\partial p(x)}\dot{\sigma}(\omega^{(2)}_{k}z^{(1)}_k+b^{(2)})\omega^{(2)}_i \dot{\sigma}(\omega^{(1)}_{ik} x_k+b^{(1)}_i)x_j\\
&amp;\frac{\partial L}{\partial b^{(1)}_{i}}=\sum_x \frac{\partial L}{\partial p(x)}\dot{\sigma}(\omega^{(2)}_{k}z^{(1)}_k+b^{(2)})\omega^{(2)}_i \dot{\sigma}(\omega^{(1)}_{ik} x_k+b^{(1)}_i)
\end{split}\end{equation*}\]

<p>We see that as we go down in the layer level, we need to propagate back the composition of the forward pass in order to calculate the derivatives. That is, first we calculate the derivatives of the weights in the higher levels, and progress downwards towards lower levels. This process can be done in an iterative way, and is known as backpropagation algorithm. More generally we have</p>

\[\begin{equation*}\begin{split}&amp;\frac{\partial p_k(x)}{\partial \omega^{(n)}_{ij}}=\dot{g}^{(l)}_k\omega^{(l)}_{kk_1} \dot{g}^{(l-1)}_{k_1}\omega^{(l-1)}_{k_1k_2}\ldots \dot{g}^{(n)}_i z^{(n-1)}_j\\
&amp;\frac{\partial p_k(x)}{\partial b^{(n)}_{i}}=\dot{g}^{(l)}_k\omega^{(l)}_{kk_1} \dot{g}^{(l-1)}_{k_1}\omega^{(l-1)}_{k_1k_2}\ldots \dot{g}^{(n)}_i
\end{split}\end{equation*}\]

<p>where again sums over indices are implicit.</p>

<p><a name="gen"></a></p>
<h3 id="3-vc-dimension"><strong>3. VC-dimension</strong></h3>

<p>We can estimate the VC-dimension of a neural network with activation the sign function. Recall the growth function definition: $\text{max}_{C\in \chi: |C|=m}|\mathcal{H}_C|$ where $\mathcal{H}_C$ is the restriction of the neural network hypotheses from the set $C$ to ${0,1}$. Between adjacent layers $L^{t-1}$ and $L^{t}$ one can define a mapping between $\mathcal{H}^t:\,\mathbb{R}^{|L^{t-1}|}\rightarrow \mathbb{R}^{|L^{t}|}$, where $|L^{t-1}|$ and $|L^t|$ are the number of neurons in the layers, respectively. Then the hypothesis class can be written as the composition of each of these maps, that is, $\mathcal{H}=\mathcal{H}^T\circ \ldots \circ \mathcal{H}^1$. The growth function can thus be bounded by
\(\Pi_{\mathcal{H}}(m)\leq \prod_{t=1}^T\Pi_{\mathcal{H^t}}(m)\)</p>

<p>In turn, for each layer the class $\mathcal{H}^t$ can be written as the product of each neuron class, that is, $\mathcal{H}^t=\mathcal{H}^{t,1}\times \ldots \times \mathcal{H}^{t,i}$ where $i$ is the number of neurons in that layer. Similarly, we can bound the growth function of each layer class
\(\Pi_{\mathcal{H}^t}(m)\leq \prod_{i=1}^{|L^t|}\Pi_{\mathcal{H^{t,i}}}(m)\)</p>

<p>Each neuron is a homogeneous halfspace class, and we have seen that the VC-dimension of this class is the dimension of their input plus one (VC-dimension of a separating hyperplane). If we count the bias constant as a single edge, then this dimension is just the number of edges flowing into the node $i$, which we denote as $d_{t,i}$. Using Sauer’s Lemma, we have
\(\Pi_{\mathcal{H}^{t,i}}\leq \Big(\frac{em}{d_{t,i}}\Big)^{d_{t,i}}&lt;(em)^{d_{t,i}}\)</p>

<p>Putting all these factors together we obtain the bound
\(\Pi_{\mathcal{H}}(m)\leq \prod_{t,i} (em)^{d_{t,i}}=(em)^{|E|}\)
where $|E|$ is the total number of edges, which is also the total number of parameters in the network. For 
\(m^*=\text{VC-dim}\) we have \(\Pi_{\mathcal{H}}=2^{m^*}\)
, therefore
\(2^{m^*}\leq (em^*)^{|E|}\)
It follows that $m^*$ must be of the order $\mathcal{O}(|E|\log_2|E|)$.</p>

<p>If the activation function is the sigmoid, the proof is out of scope. One can though give a rough estimate. The VC-dimension should be of the order of the number of tunable parameters. This is the number of edges 
\(|E|\),
 counting the bias parameters.</p>

<p><a name="decision"></a></p>

<h3 id="4-decision-boundary"><strong>4. Decision Boundary</strong></h3>

<p>Below we plot the decision boundary for a neural network with one hidden layer after several iterations, that is, the number of gradient descent steps:</p>
<div style="text-align: center"><img src="/images/nn_decision.png" width="70%" /></div>

<p>The neural network has enough capacity to draw complicated decision boundaries. Below we show the decision boundary at different stages of learning.</p>

<p float="left">
  <img src="/images/nn_decision_200.png" width="230" />
  <img src="/images/nn_decision_1000.png" width="230" />
  <img src="/images/nn_decision_2000.png" width="230" />
</p>

<p float="center">
  <img src="/images/nn_decision_3000.png" width="230" />
  <img src="/images/nn_decision_4000.png" width="230" />
  <img src="/images/nn_decision_30k.png" width="230" />
</p>

<p>Waiting long enough, allows the neural network to overfit the data as we see in the last picture.</p>

<p><a name="python"></a></p>
<h3 id="5-python-implementation"><strong>5. Python implementation</strong></h3>

<p>Define classes for linear layer and sigmoid activation function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">class</span> <span class="nc">LinearLayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dim_in</span><span class="p">,</span><span class="n">dim_out</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_in</span><span class="o">=</span><span class="n">dim_in</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_out</span><span class="o">=</span><span class="n">dim_out</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,(</span><span class="n">dim_in</span><span class="p">,</span><span class="n">dim_out</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.01</span><span class="p">,</span><span class="n">dim_out</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span>
        <span class="n">out</span><span class="o">+=</span><span class="bp">self</span><span class="p">.</span><span class="n">bias</span>
        
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">dL</span><span class="o">=</span><span class="n">x</span>
        
        <span class="k">return</span> <span class="n">dL</span>

<span class="k">class</span> <span class="nc">Sigmoid</span><span class="p">:</span>
    <span class="s">"sigmoid function"</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">out</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">out</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">return</span> <span class="n">out</span>

</code></pre></div></div>
<p>Class NN: implements neural network</p>

<p>Class logloss: returns loss function object which contains backward derivates for gradient descent</p>

<p>Class optimizer: implements gradiend descent step with specified learning rate</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NN</span><span class="p">:</span>
    <span class="s">""" Neural Network with one hidden layer"""</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">dim_in</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">dim_out</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">dim_in</span><span class="p">,</span><span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="o">=</span><span class="n">LinearLayer</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span><span class="n">dim_out</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sig</span><span class="o">=</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">delta</span><span class="o">=</span><span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_l1</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_s1</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sig</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">out_l1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_l2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">layer2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">out_s1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out_s2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">sig</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">out_l2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">out_s2</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">pred</span><span class="o">=</span><span class="p">(</span><span class="n">p</span><span class="o">&gt;=</span><span class="mf">0.5</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="s">'int'</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">pred</span>

<span class="k">class</span> <span class="nc">logloss</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">L</span><span class="o">=</span><span class="n">y</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="o">-</span><span class="n">L</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span><span class="o">/</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">dL</span><span class="o">=-</span><span class="n">y</span><span class="o">/</span><span class="n">p</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>
        <span class="n">dL</span><span class="o">=</span><span class="n">dL</span><span class="o">/</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">dL</span><span class="o">=</span><span class="n">dL</span><span class="o">*</span><span class="n">model</span><span class="p">.</span><span class="n">sig</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">out_l2</span><span class="p">)</span>
        <span class="n">dw2</span><span class="p">,</span><span class="n">db2</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">out_s1</span><span class="p">.</span><span class="n">T</span><span class="p">,</span><span class="n">dL</span><span class="p">),</span><span class="n">dL</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
        
        <span class="n">dw1</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">layer2</span><span class="p">.</span><span class="n">weights</span><span class="p">.</span><span class="n">T</span><span class="o">*</span><span class="n">model</span><span class="p">.</span><span class="n">sig</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">out_l1</span><span class="p">)</span>
        <span class="n">db1</span><span class="o">=</span><span class="p">(</span><span class="n">dL</span><span class="o">*</span><span class="n">dw1</span><span class="p">).</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">dw1</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">dL</span><span class="o">*</span><span class="n">x</span><span class="p">).</span><span class="n">T</span><span class="p">,</span><span class="n">dw1</span><span class="p">)</span>
        
        <span class="n">model</span><span class="p">.</span><span class="n">delta</span><span class="o">=</span><span class="n">db1</span><span class="p">,</span><span class="n">dw1</span><span class="p">,</span><span class="n">db2</span><span class="p">,</span><span class="n">dw2</span>
    
<span class="k">class</span> <span class="nc">optimizer</span><span class="p">:</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">model</span><span class="p">,</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="o">=</span><span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span>
        
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        
        <span class="n">db1</span><span class="p">,</span><span class="n">dw1</span><span class="p">,</span><span class="n">db2</span><span class="p">,</span><span class="n">dw2</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">delta</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">layer1</span><span class="p">.</span><span class="n">bias</span><span class="o">-=</span><span class="n">db1</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">layer1</span><span class="p">.</span><span class="n">weights</span><span class="o">-=</span><span class="n">dw1</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">layer2</span><span class="p">.</span><span class="n">bias</span><span class="o">-=</span><span class="n">db2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">layer2</span><span class="p">.</span><span class="n">weights</span><span class="o">-=</span><span class="n">dw2</span><span class="o">*</span><span class="bp">self</span><span class="p">.</span><span class="n">lr</span>
</code></pre></div></div>
<p>Training for loop:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span><span class="n">loss</span><span class="p">,</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">,</span><span class="n">num_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
        
        <span class="n">L</span><span class="o">=</span><span class="n">loss</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="k">if</span> <span class="n">i</span><span class="o">%</span><span class="mi">10</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Iteration '</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s">', loss: '</span><span class="p">,</span><span class="n">L</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="references"><strong>References</strong></h3>
<p><br /></p>

<p>[1] <em>Understanding Machine Learning: from Theory to Algorithms</em>, Shai Ben-David and Shai Shalev-Shwartz</p>

<p>[2] <em>The elements of statistical learning</em>, T. Hastie, R. Tibshirani, J. Friedman</p>

<p>[3] <em>Approximation by superpositions of a sigmoidal function</em>, Cybenko, G. (1989)</p>

  </div><a class="u-url" href="/machine%20learning/2020/11/12/nn.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




