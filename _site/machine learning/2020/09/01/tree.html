<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Decision Tree | Data Science and Machine Learning</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Decision Tree" />
<meta name="author" content="Joao Gomes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The decision tree is one of the most robust algorithms in machine learning. We explain how the algorithm works, the type of decision boundary, and a Python implementation." />
<meta property="og:description" content="The decision tree is one of the most robust algorithms in machine learning. We explain how the algorithm works, the type of decision boundary, and a Python implementation." />
<link rel="canonical" href="http://localhost:4000/machine%20learning/2020/09/01/tree.html" />
<meta property="og:url" content="http://localhost:4000/machine%20learning/2020/09/01/tree.html" />
<meta property="og:site_name" content="Data Science and Machine Learning" />
<meta property="og:image" content="http://localhost:4000/tree2.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-01T00:00:00+02:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="http://localhost:4000/tree2.png" />
<meta property="twitter:title" content="Decision Tree" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Joao Gomes"},"url":"http://localhost:4000/machine%20learning/2020/09/01/tree.html","image":"http://localhost:4000/tree2.png","headline":"Decision Tree","dateModified":"2020-09-01T00:00:00+02:00","datePublished":"2020-09-01T00:00:00+02:00","description":"The decision tree is one of the most robust algorithms in machine learning. We explain how the algorithm works, the type of decision boundary, and a Python implementation.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine%20learning/2020/09/01/tree.html"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
  <link rel="shortcut icon" type="image/png" href="/blog-data-science/favicon.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Data Science and Machine Learning" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Data Science and Machine Learning</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Decision Tree</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-09-01T00:00:00+02:00" itemprop="datePublished">
        Sep 1, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul>
  <li><a href="#1-the-algorithm"><strong>1. The algorithm</strong></a></li>
  <li><a href="#2-decision-boundary"><strong>2. Decision Boundary</strong></a></li>
  <li><a href="#3-python-implementation"><strong>3. Python implementation</strong></a></li>
</ul>

<p><a name="def1"></a></p>
<h3 id="1-the-algorithm"><strong>1. The algorithm</strong></h3>

<p>The decision tree algorithm consists of a sequence of splits, or decisions, which take the form of a tree. This tree organizes the data in a way that there is a gain of information at each node.</p>

<p>A measure of information is the Shannon entropy, defined as</p>

\[S=-\sum_i p_i\ln(p_i)\]

<p>with 
\(p_i\) 
the probability of the element $i$ in a set $\Omega$. The Shannon entropy is always smaller than the most entropic configuration which happens for \(p_i=1/|\Omega|\)
, with $|\Omega|$ the number of elements in the set. To see this, write $p_i=n_i/|\Omega|$ where $n_i$ is the number of elements in class $i$. Therefore</p>

\[-\sum_ip_i\ln(p_i)=-\sum_ip_i\ln\Big(\frac{n_i}{|\Omega|}\Big)&lt;\ln|\Omega|\]

<p>This means that more diverse the set is, larger the entropy.</p>

<p>At each node in the decision tree, a feature $f_i$ and a threshold $t_i$ is chosen so that the entropy of the left split $f_i&lt; t_i$ plus the entropy of the right split $f_i\geq t_i$ is smaller than the entropy of the initial configuration $A$. That is,</p>

\[S(A)&gt; S(A_{f_i&lt; t_i})+S(A_{f_i\geq t_i})\]

<p>In the example below we have 7 balls: 3 of color red, 2 green, 1 pink and 1 blue.</p>
<div style="text-align: center"><img src="/images/tree1.png" width="40%" /></div>

<p>We want to determine a rule that predicts the ballâ€™s color as a function of $x$. For that purpose, we build a decision tree containing splits with thresholds as a function of $x$.</p>

<div style="text-align: center"><img src="/images/tree2.png" width="50%" /></div>

<p>Following, we show that this particular tree provides information gains at each step of the splits. The entropy of the initial configuration is</p>

\[S=-\frac{3}{7}\ln\big(\frac{3}{7}\big)-\frac{2}{7}\ln\big(\frac{2}{7}\big)-\frac{2}{7}\ln\big(\frac{1}{7}\big)\simeq 1.277\]

<p>After the first split the entropy on the left and right sides of the node is reduced to $0$ and $1.034$ respectively. Their sum is smaller than the initial entropy $1.277$. The next split at $x=b$ results in two configurations with entropies $0$ on the left and $0.69$ on the right side of the split. The last split classifies unequivocally the configuration resulting in splits with zero entropy.</p>

<p>The decision tree algorithm consists of the following steps:</p>

\[\begin{equation*}\begin{split}
\text{for i}&amp;=1\ldots \text{Depth}:\\
&amp; \text{for j}=1\ldots \text{Leaves}:\\
&amp; \;\;\text{Choose feature and threshold }(f,t)=\text{argmin}_{f,t} S(A_{f&lt; t})+S(A_{f\geq t})
\end{split}\end{equation*}\]

<p>That is, at each depth level we loop through each of the leaves and split if there is gain in information. Then we continue one step further in depth. The predictor consists in attributing the majority class at each ending node.</p>

<p>In the case of regression, each split consists in chosing a feature $f$ and threshold $t$ so that the sum of the mean squared errors of the left and right split configurations is minimized. That is</p>

\[\text{Split }(f,t)=\text{argmin}_{f,t} \sum_{i\in A_l}(y_i-\bar{y}_l)^2+\sum_{i\in A_r}(y_i-\bar{y}_r)^2\]

<p>where $A_l,A_r$ are the left and right split configurations, and $\bar{y}_l,\bar{y}_r$ are the average values in each of the configurations.</p>

<p><strong>Other measures of information gain</strong></p>

<p>The Gini index, defined as</p>

\[G=1-\sum_ip_i^2\]

<p>provides another measure of information. The gini index is a bounded quantity that is $0\leq G&lt;1$, unlike the Shannon entropy. Noting that $\sum_i p_i=1$ we can write $G=\sum_i p_i(1-p_i)$. For $p_i=1-\epsilon$ with $\epsilon\ll 1$ we can approximate $-p_i\ln p_i\simeq \epsilon(1-\epsilon)=p_i(1-p_i)$. Analogously for $p_i\simeq \epsilon$ we can approximate</p>

\[p_i(1-p_i)\simeq -(1-p_i)\ln(1-p_i)\]

<p>Therefore for distributions which are concentrated in a particular class the gini index is an approximation to the Shannon entropy. 
One of the advantages of using the Gini index is its simpler computational complexity due to its polynomial form compared to the logarithm in the Shannon entropy.</p>

<p><strong>Time complexity</strong></p>

<p>Take a note as an example. A practical algorithm for the split consists of sorting the data for each of the features and then choosing the threshold that produces higher information gains. Calculating the frequencies for each threshold is of order $\mathcal{O}(N)$, with $N$ the number of samples. Therefore, finding the right split is of order $\mathcal{O}(dN\log N)$, with $d$ the number of features. The number of splits increases exponentially with the depth, but the number of samples per node decreases exponentially, which gives a net effect of $\mathcal{O}(dN\log N)$ at each depth. Since the maximum depth attainable is of order $\log N$, the total time complexity of training a decision tree should be at most $\mathcal{O}(dN\log^2 N)$</p>

<p><strong>Sample complexity</strong></p>

<p>Consider a binary classification problem where the feature space is of the form $\chi={0,1}^d$, with $d$ the dimension. It is easy to see that a tree with depth $d$ has $2^d$ leaves and can represent all data points in $\chi$. The $\text{VC}$-dimension is therefore $\text{VC}_{dim}=2^d$.</p>

<p><a name="decision"></a></p>
<h3 id="2-decision-boundary"><strong>2. Decision Boundary</strong></h3>

<p>Below we depict the sucession of splits for a 2-dimensional feature space. As we can see the tree divides the feature space into successive rectangles.</p>
<div style="text-align: center"><img src="/images/tree_decision.png" width="100%" /></div>

<p>For large depth the tree can lead to highly non-linear decision bondaries.</p>

<p><a name="python"></a></p>
<h3 id="3-python-implementation"><strong>3. Python implementation</strong></h3>

<p>First we create a Binary-Search-Tree data structure. This tree is composed of a main node, and pointers to a left and right nodes, which are generated during the split. The main node contains a subset of the data, the corresponding Shannon-entropy, the depth of the node, the selected feature and threshold, and the predicted class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Node</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">cl_dic</span><span class="p">,</span><span class="n">entropy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">feature</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">idx</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">depth</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">feature</span><span class="o">=</span><span class="n">feature</span> <span class="c1"># (feature_num, threshold)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">idx_subset</span><span class="o">=</span><span class="n">idx</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">depth</span><span class="o">=</span><span class="n">depth</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">entropy</span><span class="o">=</span><span class="n">entropy</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cl_dic</span><span class="o">=</span><span class="n">cl_dic</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">right</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">left</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">prediction</span><span class="o">=</span><span class="bp">None</span>
    
    <span class="c1">#method cross_entropy determines the Shannon-entropy of the subset at the main node
</span>    <span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">idx</span><span class="p">):</span>
        <span class="n">subset</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cl_dic</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">subset</span><span class="p">:</span>
            <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cl_dic</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">z</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">+=</span><span class="mi">1</span>
            
        <span class="n">pred</span><span class="o">=</span><span class="n">z</span><span class="p">.</span><span class="n">argmax</span><span class="p">()</span>
        <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="o">/</span><span class="n">z</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
        <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">z</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
        <span class="n">ent</span><span class="o">=-</span><span class="n">z</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">ent</span><span class="o">=</span><span class="n">ent</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">pred</span><span class="p">,</span><span class="n">ent</span>
    
    <span class="c1">#the entropy_split method takes in an array of frequencies and returns the best split that leads to the highest gains in information
</span>    <span class="k">def</span> <span class="nf">entropy_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">freq</span><span class="p">):</span>
        
        <span class="n">z</span><span class="o">=</span><span class="n">freq</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">w</span><span class="o">=</span><span class="n">freq</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        
        <span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="o">/</span><span class="n">w</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">w</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
        <span class="n">entropy1</span><span class="o">=-</span><span class="n">w</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="n">entropy1</span><span class="o">=</span><span class="n">entropy1</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">z</span>
        <span class="n">s</span><span class="o">=</span><span class="n">z</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
        <span class="n">z</span><span class="o">=</span><span class="n">z</span><span class="o">/</span><span class="n">s</span>
        <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">z</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">z</span><span class="p">)</span>
        
        <span class="n">entropy2</span><span class="o">=-</span><span class="n">z</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">log2</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">entropy2</span><span class="o">=</span><span class="n">entropy2</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">total</span><span class="o">=</span><span class="n">entropy1</span><span class="o">+</span><span class="n">entropy2</span>
        <span class="n">j</span><span class="o">=</span><span class="n">total</span><span class="p">.</span><span class="n">argmin</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">j</span><span class="p">,</span><span class="n">entropy1</span><span class="p">[</span><span class="n">j</span><span class="p">],</span><span class="n">entropy2</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    
    <span class="c1">#the method best_feature loops through all the features and chooses the best split
</span>    <span class="k">def</span> <span class="nf">best_feature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">idx_subset</span><span class="p">):</span>
        
        <span class="n">w</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">indices</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">pos</span><span class="o">=</span><span class="p">[]</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">idx</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">f</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">indices</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
            <span class="n">y_sort</span><span class="o">=</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
            <span class="n">z</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">cl_dic</span><span class="p">)))</span>
            <span class="n">cum</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y_sort</span><span class="p">):</span>
                <span class="n">k</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cl_dic</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
                <span class="n">cum</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">+=</span><span class="mi">1</span>
                <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">cum</span><span class="p">[:]</span>
            <span class="n">j</span><span class="p">,</span><span class="n">e1</span><span class="p">,</span><span class="n">e2</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">entropy_split</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">pos</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
            <span class="n">w</span><span class="p">[</span><span class="n">f</span><span class="p">]</span><span class="o">=</span><span class="n">e1</span><span class="o">+</span><span class="n">e2</span>
            
            
        <span class="n">num</span><span class="o">=</span><span class="n">w</span><span class="p">.</span><span class="n">argmin</span><span class="p">()</span> <span class="c1">#best feature index
</span>        <span class="n">idx</span><span class="o">=</span><span class="n">indices</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>
        <span class="n">j</span><span class="o">=</span><span class="n">pos</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>
        <span class="n">value</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">j</span><span class="p">,</span><span class="n">num</span><span class="p">]</span>
        <span class="c1">#multiple values for the same feature
</span>        <span class="n">i</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">while</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">j</span><span class="o">+</span><span class="n">i</span><span class="p">,</span><span class="n">num</span><span class="p">]</span><span class="o">==</span><span class="n">value</span><span class="p">:</span>
            <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>
        <span class="n">j</span><span class="o">=</span><span class="n">j</span><span class="o">+</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span>
        
        <span class="n">value</span><span class="o">=</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">j</span><span class="p">:</span><span class="n">j</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span><span class="n">num</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">idx_left</span><span class="o">=</span><span class="n">idx_subset</span><span class="p">[</span><span class="n">idx</span><span class="p">[:</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">idx_right</span><span class="o">=</span><span class="n">idx_subset</span><span class="p">[</span><span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">:]]</span>
        
        <span class="n">pred_l</span><span class="p">,</span><span class="n">entropy_left</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">idx</span><span class="p">[:</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">pred_r</span><span class="p">,</span><span class="n">entropy_right</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">idx</span><span class="p">[</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
        <span class="n">j</span><span class="o">=</span><span class="n">pos</span><span class="p">[</span><span class="n">num</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">num</span><span class="p">,</span><span class="n">value</span><span class="p">,</span><span class="n">pred_l</span><span class="p">,</span><span class="n">entropy_left</span><span class="p">,</span><span class="n">pred_r</span><span class="p">,</span><span class="n">entropy_right</span><span class="p">,</span><span class="n">idx_left</span><span class="p">,</span><span class="n">idx_right</span>
    
    <span class="c1"># the split method runs through all the nodes in the tree and splits them
</span>    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">node</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">node</span><span class="p">.</span><span class="n">entropy</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">node</span><span class="p">.</span><span class="n">feature</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1">#split
</span>            <span class="n">idx</span><span class="o">=</span><span class="n">node</span><span class="p">.</span><span class="n">idx_subset</span>
            <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">pl</span><span class="p">,</span><span class="n">el</span><span class="p">,</span><span class="n">pr</span><span class="p">,</span><span class="n">er</span><span class="p">,</span><span class="n">idl</span><span class="p">,</span><span class="n">idr</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">best_feature</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span><span class="n">idx</span><span class="p">)</span>
            <span class="n">node</span><span class="p">.</span><span class="n">feature</span><span class="o">=</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
            <span class="n">node</span><span class="p">.</span><span class="n">right</span><span class="o">=</span><span class="n">Node</span><span class="p">(</span><span class="n">entropy</span><span class="o">=</span><span class="n">er</span><span class="p">,</span><span class="n">idx</span><span class="o">=</span><span class="n">idr</span><span class="p">,</span><span class="n">depth</span><span class="o">=</span><span class="n">node</span><span class="p">.</span><span class="n">depth</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">cl_dic</span><span class="o">=</span><span class="n">node</span><span class="p">.</span><span class="n">cl_dic</span><span class="p">)</span>
            <span class="n">node</span><span class="p">.</span><span class="n">right</span><span class="p">.</span><span class="n">prediction</span><span class="o">=</span><span class="n">pr</span> <span class="c1">#prediction
</span>            <span class="n">node</span><span class="p">.</span><span class="n">left</span><span class="o">=</span><span class="n">Node</span><span class="p">(</span><span class="n">entropy</span><span class="o">=</span><span class="n">el</span><span class="p">,</span><span class="n">idx</span><span class="o">=</span><span class="n">idl</span><span class="p">,</span><span class="n">depth</span><span class="o">=</span><span class="n">node</span><span class="p">.</span><span class="n">depth</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">cl_dic</span><span class="o">=</span><span class="n">node</span><span class="p">.</span><span class="n">cl_dic</span><span class="p">)</span>
            <span class="n">node</span><span class="p">.</span><span class="n">left</span><span class="p">.</span><span class="n">prediction</span><span class="o">=</span><span class="n">pl</span> <span class="c1">#prediction
</span>        <span class="k">elif</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">node</span><span class="p">.</span><span class="n">feature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="c1">#go down on tree
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">right</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">left</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<p>The Decision Tree classifier is build upon the Node class with a fit and a predict methods.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DTreeClassifier</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">criterion</span><span class="o">=</span><span class="s">'entropy'</span><span class="p">,</span><span class="n">depth</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">criterion</span><span class="o">=</span><span class="s">'entropy'</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'Using entropy criterion'</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_features</span><span class="o">=</span><span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="o">=</span><span class="bp">None</span>
        
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="o">=</span><span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_features</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">cl_dic</span><span class="o">=</span><span class="p">{</span><span class="n">c</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">classes</span><span class="p">)}</span>
        <span class="n">idx</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="o">=</span><span class="n">Node</span><span class="p">(</span><span class="n">cl_dic</span><span class="o">=</span><span class="n">cl_dic</span><span class="p">,</span><span class="n">idx</span><span class="o">=</span><span class="n">idx</span><span class="p">,</span><span class="n">depth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">_</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">.</span><span class="n">entropy</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">idx</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
    
    <span class="c1">#private method
</span>    <span class="k">def</span> <span class="nf">__recurrence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">node</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">node</span><span class="p">.</span><span class="n">feature</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">f</span><span class="p">,</span><span class="n">threshold</span><span class="o">=</span><span class="n">node</span><span class="p">.</span><span class="n">feature</span>
            <span class="k">if</span> <span class="n">x</span><span class="p">[</span><span class="n">f</span><span class="p">]</span><span class="o">&gt;</span><span class="n">threshold</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">__recurrence</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">right</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">__recurrence</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">left</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">node</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">node</span><span class="p">.</span><span class="n">feature</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">node</span><span class="p">.</span><span class="n">prediction</span>
            
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">x</span><span class="p">):</span>
        <span class="n">pred</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">xi</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
            <span class="n">pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">__recurrence</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">head</span><span class="p">,</span><span class="n">xi</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">pred</span>
</code></pre></div></div>

  </div><a class="u-url" href="/machine%20learning/2020/09/01/tree.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Data Science and Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Joao Gomes</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">joaomvg</span></a></li><li><a href="https://www.linkedin.com/in/joaomvg"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">joaomvg</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Machine Learning algorithms in Python, statistics and cloud computing.</p>
      </div>
    </div>

  </div>

</footer>
</body>
  

</html>

 

<!-- CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"/>

<!-- JavaScript -->
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body,{
    delimiters: [
      { left: '$$',  right: '$$',  display: true  },
      { left: '$',   right: '$',   display: false },
      { left: '\\[', right: '\\]', display: true  },
      { left: '\\(', right: '\\)', display: false }
  ]
  });">
</script>




